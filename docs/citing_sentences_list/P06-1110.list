Similar recent algorithms such a decision-tree-based parsing [55] can also be seen as running a (slightly modiﬁed) ﬁrst iteration of Searn.
The work of Turian and Melamed [144, 145] present a discriminative classifier-based parser, which is one of the first discriminative phrase-based parsers to outperform a generative baseline without the explicit use of the baseline parser (e.g.,to re-rank or for parameter initialization).
In constituent parsing of natural language even the best fully discriminative approaches (Turian & Melamed, 2006; Taskar et al., 2004) struggle to achieve state-of-the-art accuracy and are generally inferior to their generative counterparts.
First, globally trained conditional models require an expensive decoding stage during training, which is usually implemented by using aggressive approximation techniques (Taskar et al., 2004) and locally trained models (Ratnaparkhi, 1999) either suffer from the label-bias problem (Bottou, 1991) or do not have clear motivation for combining scores of entire trees from local scores of decisions (Turian et al., 2006).
Global discriminative training was also attempted in (Turian & Melamed, 2006). They achieved statistically significant improvement over their baseline, Collins’ parser (Collins, 1999), without the use of probability estimates from a generative model.
They used a nonlinear model, ensembles of decision trees, to perform induction of complex features from elementary ones.
Unlike these approaches the fully discriminative constituent parser proposed in (Turian & Melamed, 2006) uses decision trees to build compound features from atomic features during training.
Even though research into non-linear models in natural language parsing is still in its infancy, the state-of-the-art results that have been demonstrated (Turian & Melamed, 2006) suggest that non-linear models represent a very promising direction.
I consider the problem of dependency parsing in a bottom-up, left-to-right framework (Nivre, 2003; Sagae and Lavie, 2005; Turian and Melamed, 2006).
We represent the hypothesis function for each inference type as an ensemble of gradient boosted decision trees (GBDTs), trained using the discriminative approach described in [Turian et al.2006].
Turian and Melamed (2006) proposes a uniform-cost search approach.
The second kind was generated by the parser of Turian and Melamed (2006), which was trained in a purely discriminative manner using the method.
The discriminatively-trained transducer far outperformed the generatively trained transducer on both precision and recall.
As in (Henderson, 2003; Turian and Melamed, 2006) we used a publicly available tagger (Ratna-parkhi, 1996) to provide the part-of-speech tag for each word in the sentence.
Turian and Melamed (2006) combine uniformly their models according to general syntactic labels and so do He et al. (2008) when integrating the rule selection models with respect to rules.
Turian and Melamed (2006) reports almost 5 days of training for their own parser, using parallelization, on this setup.
They also report several months of training for Taskar et al.'s parser.
In order to overcome this problem, discriminative models with global inference have been proposed, using approximate search (Turian and Melamed 2006), but eﬃciency remains a problem for these methods, which do not seem to scale up to sentences of arbitrary length.
One approach for compactly increasing capacity is to automatically induce intermediate features through the composition of non-linearities, for example, boosting decision trees (Turian and Melamed, 2006).
More recent is the work of Turian and Melamed (2006) and Turian et al. (2007), who also only reported results on WSJ15, and which improved both the accuracy of Taskar et al. (2004).
They deﬁne a simple linear model, use boosted decision trees to select feature conjunctions, and a line search to optimiza the parameters.
