<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>Automatic Acquisition of Hyponyms from Large Text Corpora.</title>
<date>1992</date>
<journal>COLING</journal>
<volume>92</volume>
<pages>539--545</pages>
<location>Nantes, France,</location>
<contexts>
<context position="5700" citStr="Hearst (1992)" startWordPosition="918" endWordPosition="919"> results obtained using the nearestneighbor, neural network (i.e. multi-layer perceptron) and SVM. The mechanisms of these different learning approaches will be discussed briefly in Section 4. 3 Related Work 3.1 Web Mining Much of the recent work conducted on the problem of assigning semantic relations to noun phrases has used the web as a corpus. The use of hit counts from web search engines to obtain lexical information was introduced by Turney (2001). The idea of searching a large corpus for specific lexicosyntactic phrases to indicate a semantic relation of interest was first described by Hearst (1992). A lexical pattern specific enough to indicate a particular semantic relation is usually not very frequent, and using the web as a corpus alleviates the data sparseness problem. However, it also introduces some problems. • The query language permitted by the large search engines is somewhat limited. • Two of the major search engines (Google and Yahoo) do not provide exact frequencies, but give rounded estimates instead. • The number of results returned is unstable as new pages are created and deleted all the time. Nakov and Hearst (2005) examined the use of web-based n-gram frequencies for an</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>Marti A. Hearst. 1992. Automatic Acquisition of Hyponyms from Large Text Corpora. COLING 92: (2) pp. 539-545, Nantes, France,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Keller</author>
<author>Mirella Lapata</author>
</authors>
<title>Using the Web to Obtain Frequencies for Unseen Bigrams.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<pages>459--484</pages>
<contexts>
<context position="6423" citStr="Keller and Lapata (2003)" startWordPosition="1034" endWordPosition="1037">frequent, and using the web as a corpus alleviates the data sparseness problem. However, it also introduces some problems. • The query language permitted by the large search engines is somewhat limited. • Two of the major search engines (Google and Yahoo) do not provide exact frequencies, but give rounded estimates instead. • The number of results returned is unstable as new pages are created and deleted all the time. Nakov and Hearst (2005) examined the use of web-based n-gram frequencies for an NLP task and concluded that these issues do not greatly impact the interpretation of the results. Keller and Lapata (2003) showed that web frequencies correlate reliably with standard corpus frequencies. Lauer (1995) tackles the problem of semantically disambiguating noun phrases by trying to find the preposition which best describes the relation between the modifier and head noun. His method involves searching a corpus for occurrences paraphrases of the form &amp;quot;noun preposition modifier&amp;quot;. Whichever preposition is most frequent in this context is chosen. Lapata and Keller (2005) improved on Lauer&apos;s results at the same task by using the web as a corpus. Nakov and Hearst (2006) use queries of the form &amp;quot;noun that * mo</context>
<context position="15272" citStr="Keller and Lapata, 2003" startWordPosition="2511" endWordPosition="2514">ext condition used a vector of web hits obtained using the joining terms that occurred 1-14 15-28 of against in within to during for through on over with towards at without is across from because as behind by after between before about while has under Table 2: joining terms ordered by the frequency with which they occurred between two nouns in the BNC. from position 14 to 28 in the list of the most frequent terms found in the BNC. The third condition used all 28 joining terms. The joining terms are listed in Table 2. We used the log of the web counts returned, as recommended in previous work (Keller and Lapata, 2003). The first learning algorithm we experimented with was the nearest neighbor algorithm `IB1&apos;, as 82 implemented in Weka. This algorithm considers the vector of n-gram frequencies as a multidimensional space, and chooses the label of the nearest example in this space as the label for each new example. Testing for this algorithm was done using leave-one-out cross validation. The next learning algorithm we used was the multi-layer perceptron, or neural network. The network was trained using the backpropagation of error technique implemented in Weka. For the first two sets of data we used a networ</context>
</contexts>
<marker>Keller, Lapata, 2003</marker>
<rawString>Frank Keller and Mirella Lapata. 2003. Using the Web to Obtain Frequencies for Unseen Bigrams. Computational Linguistics, 29: pp 459-484.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mirella Lapata</author>
<author>Frank Keller</author>
</authors>
<title>Web-Based Models for Natural Language Processing.</title>
<date>2005</date>
<journal>ACM Transactions on Speech and Language Processing</journal>
<volume>2</volume>
<pages>1--31</pages>
<contexts>
<context position="6884" citStr="Lapata and Keller (2005)" startWordPosition="1100" endWordPosition="1103">e of web-based n-gram frequencies for an NLP task and concluded that these issues do not greatly impact the interpretation of the results. Keller and Lapata (2003) showed that web frequencies correlate reliably with standard corpus frequencies. Lauer (1995) tackles the problem of semantically disambiguating noun phrases by trying to find the preposition which best describes the relation between the modifier and head noun. His method involves searching a corpus for occurrences paraphrases of the form &amp;quot;noun preposition modifier&amp;quot;. Whichever preposition is most frequent in this context is chosen. Lapata and Keller (2005) improved on Lauer&apos;s results at the same task by using the web as a corpus. Nakov and Hearst (2006) use queries of the form &amp;quot;noun that * modifier&amp;quot; where &apos;*&apos; is a wildcard operator. By retrieving the words that most commonly occurred in the place of the wildcard they were able to identify very specific predicates that are likely to represent the relation between noun and modifier. 80 3.2 Machine Learning Approaches There have been two main approaches used when applying machine learning algorithms to the semantic disambiguation of modifier-noun phrases. The first approach is to use semantic prop</context>
</contexts>
<marker>Lapata, Keller, 2005</marker>
<rawString>Mirella Lapata and Frank Keller. 2005. Web-Based Models for Natural Language Processing. ACM Transactions on Speech and Language Processing 2:1, pp 1-31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Lauer</author>
</authors>
<title>Designing Statistical Language Learners: Experiments on Noun Compounds.</title>
<date>1995</date>
<tech>PhD thesis,</tech>
<institution>Macquarie University,</institution>
<location>NSW 2109, Australia.</location>
<contexts>
<context position="6517" citStr="Lauer (1995)" startWordPosition="1048" endWordPosition="1049">s some problems. • The query language permitted by the large search engines is somewhat limited. • Two of the major search engines (Google and Yahoo) do not provide exact frequencies, but give rounded estimates instead. • The number of results returned is unstable as new pages are created and deleted all the time. Nakov and Hearst (2005) examined the use of web-based n-gram frequencies for an NLP task and concluded that these issues do not greatly impact the interpretation of the results. Keller and Lapata (2003) showed that web frequencies correlate reliably with standard corpus frequencies. Lauer (1995) tackles the problem of semantically disambiguating noun phrases by trying to find the preposition which best describes the relation between the modifier and head noun. His method involves searching a corpus for occurrences paraphrases of the form &amp;quot;noun preposition modifier&amp;quot;. Whichever preposition is most frequent in this context is chosen. Lapata and Keller (2005) improved on Lauer&apos;s results at the same task by using the web as a corpus. Nakov and Hearst (2006) use queries of the form &amp;quot;noun that * modifier&amp;quot; where &apos;*&apos; is a wildcard operator. By retrieving the words that most commonly occurred </context>
</contexts>
<marker>Lauer, 1995</marker>
<rawString>Mark Lauer. 1995. Designing Statistical Language Learners: Experiments on Noun Compounds. PhD thesis, Macquarie University, NSW 2109, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>judith Levi</author>
</authors>
<title>The Syntax and Semantics of Complex Nominals,</title>
<date>1978</date>
<publisher>Academic Press,</publisher>
<location>New York, NY.</location>
<contexts>
<context position="1801" citStr="Levi 1978" startWordPosition="281" endWordPosition="282">action between the modifier and the head noun, and then attempt to assign one of these semantic relations to each noun-modifier pair. For example, the phrase &amp;quot;flu virus&amp;quot; could be assigned the semantic relation &amp;quot;causal&amp;quot; (the virus causes the flu); the relation for &amp;quot;desert storm&amp;quot; could be &amp;quot;location&amp;quot; (the storm is located in the desert). There is no consensus as to which set of semantic relations best captures the differences in meaning of various noun phrases. Work in theoretical linguistics has suggested that noun-noun compounds may be formed by the deletion of a predicate verb or preposition (Levi 1978). However, whether the set of possible predicates numbers 5 or 50, there are likely to be some examples of noun phrases that fit into none of the categories and some that fit in multiple categories. Modifier-noun phrases are often used interchangeably with paraphrases which contain the modifier and the noun joined by a preposition or simple verb. For example, the query &amp;quot;morning exercise&amp;quot; returns 133,000 results from the Yahoo search engine, and a query for the phrase &amp;quot;exercise in the morning&amp;quot; returns 47,500 results. Sometimes people choose to use a modifier-noun compound phrase to describe a c</context>
</contexts>
<marker>Levi, 1978</marker>
<rawString>judith Levi. 1978. The Syntax and Semantics of Complex Nominals, Academic Press, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Moldovan</author>
<author>Adriana Badulescu</author>
<author>Marta Tatu</author>
<author>Daniel Antohe</author>
<author>Roxana Girju</author>
</authors>
<title>Models for the Semantic Classification of Noun Phrases.</title>
<date>2004</date>
<booktitle>In Proceedings of the HLTINAACL Workshop on Computational Lexical Semantics.</booktitle>
<pages>60--67</pages>
<location>Boston , MA.</location>
<contexts>
<context position="8186" citStr="Moldovan et al (2004)" startWordPosition="1310" endWordPosition="1313"> extract these properties. This approach was used by Rosario and Hearst (2001) within a specific domain — medical texts. Using an ontology of medical terms they train a neural network to semantically classify nominal phrases, achieving 60% accuracy over 16 classes. Nastase and Szpakowicz (2003) use the position of the noun and modifier words within general semantic hierarchies (Roget&apos;s Thesaurus and WordNet) as attributes for their learning algorithms. They experiment with various algorithms and conclude that a rule induction system is capable of generalizing to characterize the noun phrases. Moldovan et al (2004) also use WordNet. They experiment with a Bayesian algorithm, decision trees, and their own algorithm; semantic scattering. There are some drawbacks to the technique of using semantic properties extracted from a lexical hierarchy. Firstly, it has been noted that the distinctions between word senses in WordNet are very fine-grained, making the task of word-sense disambiguation tricky. Secondly, it is usual to use a rule-based learning algorithm when the attributes are properties of the words rather than n-gram frequency counts. As Nastase and Szpakowicz (2003) point out, a large amount of label</context>
</contexts>
<marker>Moldovan, Badulescu, Tatu, Antohe, Girju, 2004</marker>
<rawString>Dan Moldovan, Adriana Badulescu, Marta Tatu, Daniel Antohe and Roxana Girju. 2004. Models for the Semantic Classification of Noun Phrases. In Proceedings of the HLTINAACL Workshop on Computational Lexical Semantics. pp 60-67 Boston , MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Marti Hearst</author>
</authors>
<title>Using Verbs to Characterize Noun-Noun Relations.</title>
<date>2006</date>
<booktitle>In Proceedings of AIMSA</booktitle>
<pages>233--244</pages>
<location>Varne, Bulgaria.</location>
<contexts>
<context position="6983" citStr="Nakov and Hearst (2006)" startWordPosition="1119" endWordPosition="1122">act the interpretation of the results. Keller and Lapata (2003) showed that web frequencies correlate reliably with standard corpus frequencies. Lauer (1995) tackles the problem of semantically disambiguating noun phrases by trying to find the preposition which best describes the relation between the modifier and head noun. His method involves searching a corpus for occurrences paraphrases of the form &amp;quot;noun preposition modifier&amp;quot;. Whichever preposition is most frequent in this context is chosen. Lapata and Keller (2005) improved on Lauer&apos;s results at the same task by using the web as a corpus. Nakov and Hearst (2006) use queries of the form &amp;quot;noun that * modifier&amp;quot; where &apos;*&apos; is a wildcard operator. By retrieving the words that most commonly occurred in the place of the wildcard they were able to identify very specific predicates that are likely to represent the relation between noun and modifier. 80 3.2 Machine Learning Approaches There have been two main approaches used when applying machine learning algorithms to the semantic disambiguation of modifier-noun phrases. The first approach is to use semantic properties of the noun and modifier words as attributes, using a lexical hierarchy to extract these pro</context>
</contexts>
<marker>Nakov, Hearst, 2006</marker>
<rawString>Preslav Nakov and Marti Hearst. 2006. Using Verbs to Characterize Noun-Noun Relations. In Proceedings of AIMSA 2006, pp 233-244, Varne, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Preslav Nakov</author>
<author>Marti Hearst</author>
</authors>
<title>Using the Web as an Implicit Training Set: Application to Structural Ambiguity Resolution.</title>
<date>2005</date>
<booktitle>In Proceedings of HLTIEMNLP&apos;0S.</booktitle>
<pages>835--842</pages>
<location>Vancouver, Canada.</location>
<contexts>
<context position="6244" citStr="Nakov and Hearst (2005)" startWordPosition="1005" endWordPosition="1008">o indicate a semantic relation of interest was first described by Hearst (1992). A lexical pattern specific enough to indicate a particular semantic relation is usually not very frequent, and using the web as a corpus alleviates the data sparseness problem. However, it also introduces some problems. • The query language permitted by the large search engines is somewhat limited. • Two of the major search engines (Google and Yahoo) do not provide exact frequencies, but give rounded estimates instead. • The number of results returned is unstable as new pages are created and deleted all the time. Nakov and Hearst (2005) examined the use of web-based n-gram frequencies for an NLP task and concluded that these issues do not greatly impact the interpretation of the results. Keller and Lapata (2003) showed that web frequencies correlate reliably with standard corpus frequencies. Lauer (1995) tackles the problem of semantically disambiguating noun phrases by trying to find the preposition which best describes the relation between the modifier and head noun. His method involves searching a corpus for occurrences paraphrases of the form &amp;quot;noun preposition modifier&amp;quot;. Whichever preposition is most frequent in this con</context>
</contexts>
<marker>Nakov, Hearst, 2005</marker>
<rawString>Preslav Nakov and Marti Hearst. 2005. Using the Web as an Implicit Training Set: Application to Structural Ambiguity Resolution. In Proceedings of HLTIEMNLP&apos;0S. pp 835-842, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vivi Nastase</author>
<author>Stan Szpakowicz</author>
</authors>
<title>Exploring Noun-Modifier Semantic Relations.</title>
<date>2003</date>
<booktitle>In Fifth International Workshop on Computational Semantics,</booktitle>
<pages>285--301</pages>
<location>Tillburg, Netherlands.</location>
<contexts>
<context position="7860" citStr="Nastase and Szpakowicz (2003)" startWordPosition="1259" endWordPosition="1262">relation between noun and modifier. 80 3.2 Machine Learning Approaches There have been two main approaches used when applying machine learning algorithms to the semantic disambiguation of modifier-noun phrases. The first approach is to use semantic properties of the noun and modifier words as attributes, using a lexical hierarchy to extract these properties. This approach was used by Rosario and Hearst (2001) within a specific domain — medical texts. Using an ontology of medical terms they train a neural network to semantically classify nominal phrases, achieving 60% accuracy over 16 classes. Nastase and Szpakowicz (2003) use the position of the noun and modifier words within general semantic hierarchies (Roget&apos;s Thesaurus and WordNet) as attributes for their learning algorithms. They experiment with various algorithms and conclude that a rule induction system is capable of generalizing to characterize the noun phrases. Moldovan et al (2004) also use WordNet. They experiment with a Bayesian algorithm, decision trees, and their own algorithm; semantic scattering. There are some drawbacks to the technique of using semantic properties extracted from a lexical hierarchy. Firstly, it has been noted that the distinc</context>
<context position="9809" citStr="Nastase and Szpakowicz (2003)" startWordPosition="1572" endWordPosition="1575">second approach is to use statistical information about the occurrence of the noun and modifier in a corpus to generate attributes for a machine learning algorithm. This is the method we will describe in this paper. Turney and Littman (2005) use a set of 64 short prepositional and conjunctive phrases they call &amp;quot;joining terms&amp;quot; to generate exact queries for AltaVista of the form &amp;quot;noun joining term modifier&amp;quot;, and &amp;quot;modifier joining term noun&amp;quot;. These hit counts were used with a nearest neighbor algorithm to assign the noun phrases semantic relations. Over the set of 5 semantic relations defined by Nastase and Szpakowicz (2003), they achieve an accuracy of 45.7% for the task of assigning one of 5 semantic relations to each of the 600 modifier-noun phrases. 4 Method The method described in this paper is similar to the work presented in Turney and Littman (2005). We collect web frequencies for queries of the form &amp;quot;head joining term modifier&amp;quot;. We did not collect queries of the form &amp;quot;modifier joining term head&amp;quot;; in the majority of paraphrases of noun phrases the head noun occurs before the modifying word. As well as trying to achieve reasonable accuracy, we were interested in discovering what kinds of joining phrases ar</context>
</contexts>
<marker>Nastase, Szpakowicz, 2003</marker>
<rawString>Vivi Nastase and Stan Szpakowicz. 2003. Exploring Noun-Modifier Semantic Relations. In Fifth International Workshop on Computational Semantics, pp 285-301. Tillburg, Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Rosario</author>
<author>Marti A Hearst</author>
</authors>
<title>Classifying the semantic relations in noun compounds via a domain-specific lexical hierarchy.</title>
<date>2001</date>
<booktitle>In Proceedings of EMNLP</booktitle>
<pages>82--90</pages>
<location>Pittsburgh, PA, USA.</location>
<contexts>
<context position="7643" citStr="Rosario and Hearst (2001)" startWordPosition="1225" endWordPosition="1228"> * modifier&amp;quot; where &apos;*&apos; is a wildcard operator. By retrieving the words that most commonly occurred in the place of the wildcard they were able to identify very specific predicates that are likely to represent the relation between noun and modifier. 80 3.2 Machine Learning Approaches There have been two main approaches used when applying machine learning algorithms to the semantic disambiguation of modifier-noun phrases. The first approach is to use semantic properties of the noun and modifier words as attributes, using a lexical hierarchy to extract these properties. This approach was used by Rosario and Hearst (2001) within a specific domain — medical texts. Using an ontology of medical terms they train a neural network to semantically classify nominal phrases, achieving 60% accuracy over 16 classes. Nastase and Szpakowicz (2003) use the position of the noun and modifier words within general semantic hierarchies (Roget&apos;s Thesaurus and WordNet) as attributes for their learning algorithms. They experiment with various algorithms and conclude that a rule induction system is capable of generalizing to characterize the noun phrases. Moldovan et al (2004) also use WordNet. They experiment with a Bayesian algori</context>
</contexts>
<marker>Rosario, Hearst, 2001</marker>
<rawString>Barbara Rosario and Marti A. Hearst. 2001. Classifying the semantic relations in noun compounds via a domain-specific lexical hierarchy. In Proceedings of EMNLP 2001, pp 82-90, Pittsburgh, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Mining the web for synonyms: PM-IR vs LSA on TOEFL.</title>
<date>2001</date>
<booktitle>Proceedings of ECML&apos;01.</booktitle>
<pages>491--502</pages>
<location>Freiburg, Germany.</location>
<contexts>
<context position="5544" citStr="Turney (2001)" startWordPosition="893" endWordPosition="894">Witten and Frank, 1999) a machine learning toolkit which allows for fast experimentation with many standard learning algorithms. In Section 5 we present the results obtained using the nearestneighbor, neural network (i.e. multi-layer perceptron) and SVM. The mechanisms of these different learning approaches will be discussed briefly in Section 4. 3 Related Work 3.1 Web Mining Much of the recent work conducted on the problem of assigning semantic relations to noun phrases has used the web as a corpus. The use of hit counts from web search engines to obtain lexical information was introduced by Turney (2001). The idea of searching a large corpus for specific lexicosyntactic phrases to indicate a semantic relation of interest was first described by Hearst (1992). A lexical pattern specific enough to indicate a particular semantic relation is usually not very frequent, and using the web as a corpus alleviates the data sparseness problem. However, it also introduces some problems. • The query language permitted by the large search engines is somewhat limited. • Two of the major search engines (Google and Yahoo) do not provide exact frequencies, but give rounded estimates instead. • The number of res</context>
</contexts>
<marker>Turney, 2001</marker>
<rawString>Peter D. Turney. 2001. Mining the web for synonyms: PM-IR vs LSA on TOEFL. Proceedings of ECML&apos;01. pp 491-502. Freiburg, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Michael L Littman</author>
</authors>
<title>Corpusbased learning of analogies and semantic relations.</title>
<date>2005</date>
<booktitle>Machine Learning,</booktitle>
<pages>60--1</pages>
<contexts>
<context position="9421" citStr="Turney and Littman (2005)" startWordPosition="1508" endWordPosition="1511"> required to allow these rule-based learners to effectively generalize, and manually labeling thousands of modifier-noun compounds would be a timeconsuming task. causal flu virus, onion tear temporal summer travel, morning class spatial west coast, home remedy participant mail sorter, blood donor quality rice paper, picture book Table 1: Examples for each of the five relations The second approach is to use statistical information about the occurrence of the noun and modifier in a corpus to generate attributes for a machine learning algorithm. This is the method we will describe in this paper. Turney and Littman (2005) use a set of 64 short prepositional and conjunctive phrases they call &amp;quot;joining terms&amp;quot; to generate exact queries for AltaVista of the form &amp;quot;noun joining term modifier&amp;quot;, and &amp;quot;modifier joining term noun&amp;quot;. These hit counts were used with a nearest neighbor algorithm to assign the noun phrases semantic relations. Over the set of 5 semantic relations defined by Nastase and Szpakowicz (2003), they achieve an accuracy of 45.7% for the task of assigning one of 5 semantic relations to each of the 600 modifier-noun phrases. 4 Method The method described in this paper is similar to the work presented in </context>
<context position="10772" citStr="Turney and Littman (2005)" startWordPosition="1735" endWordPosition="1738"> queries of the form &amp;quot;modifier joining term head&amp;quot;; in the majority of paraphrases of noun phrases the head noun occurs before the modifying word. As well as trying to achieve reasonable accuracy, we were interested in discovering what kinds of joining phrases are most useful when trying to predict the semantic relation, and which machine learning algorithms perform best at the task of using vectors of web-based n-gram frequencies to predict the semantic relation. For our experiments we used the set of 600 labeled noun-modifier pairs of Nastase and Szpakowicz (2003). This data was also used by Turney and Littman (2005). Of the 600 modifier-noun phrases, three contained hyphenated or two-word modifier terms, for example &amp;quot;test-tube baby&amp;quot;. We omitted these three examples from our experiments, leaving a dataset of 597 examples. The data is labeled with two different sets of semantic relations: one set of 30 relations with fairly specific meanings, and another set of 5 relations with more abstract meanings. For our experiments we focused on the set of 5 relations. One reason for this is that dividing a set of 600 instances into 30 classes results in a fairly sparse and uneven dataset. Table 1 is a list of the re</context>
<context position="13169" citStr="Turney and Littman (2005)" startWordPosition="2149" endWordPosition="2153">dent&amp;quot; OR &amp;quot;invention has the student&amp;quot; It would be possible to construct a set of handcoded rules to map from joining terms to semantic relations; for example &amp;quot;during&amp;quot; maps to temporal, &amp;quot;by&amp;quot; maps to causal and so on. However, we hope that the classifiers will be able to identify combinations of prepositions that indicate a relation. 4.2 Choosing a Set of Joining Terms Possibly the most difficult problem with this method is deciding on a set of joining terms which is likely to provide enough information about the noun-modifier pairs to allow a learning algorithm to predict the semantic relation. Turney and Littman (2005) use a large and varied set of joining terms. They include the most common prepositions, conjunctions and simple verbs like &amp;quot;has&amp;quot;, &amp;quot;goes &amp;quot; and &amp;quot;is&amp;quot;. Also, they include the wildcard operator &apos;*&apos; in many of their queries; for example &amp;quot;not&amp;quot;, &amp;quot;* not&amp;quot; and &amp;quot;but not&amp;quot; are all separate queries. In addition, they include prepositions both with and without the definite article as separate queries, for example &amp;quot;for&amp;quot; and &amp;quot;for the&amp;quot;. The joining terms used for the experiments in this paper were chosen by examining which phrases most commonly occurred between two nouns in the BNC. We counted the frequencies w</context>
<context position="18353" citStr="Turney and Littman (2005)" startWordPosition="3016" endWordPosition="3019">ther of the other algorithms did so. 6. Discussion and Future Work Our motivation in this paper was twofold. Firstly, we wanted to compare the performance of different machine learning algorithms on the task of mapping from a vector of web frequencies of paraphrases containing joining terms to semantic relations. Secondly, we wanted to discover whether the frequency of joining terms was related to their effectiveness at predicting a semantic relation. 6.1 Learning Algorithms The results suggest that the nearest neighbor approach is not the most effective algorithm for the classification task. Turney and Littman (2005) achieve an accuracy of 45.7%, where we achieve a maximum accuracy of 38.1% on this dataset using a nearest neighbor algorithm. However, their technique uses the cosine of the angle between the vectors of web counts as the similarity metric, while the nearest neighbor implementation in Weka uses the Euclidean distance. Also, they use 64 joining terms and gather counts for both the forms &amp;quot;noun joining term modifier&amp;quot; and &amp;quot;modifier joining term noun&amp;quot; (128 frequencies in total); while we use only the former construction with 28 joining terms. By using the SVM classifier, we were able to achieve a </context>
</contexts>
<marker>Turney, Littman, 2005</marker>
<rawString>Peter D. Turney and Michael L. Littman. 2005. Corpusbased learning of analogies and semantic relations. Machine Learning, 60(1-3):251-278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian H Witten</author>
<author>Eibe Frank</author>
</authors>
<date>1999</date>
<booktitle>Data Mining: Practical Machine Learning Tools and Techniques with Java Implementations.</booktitle>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="4954" citStr="Witten and Frank, 1999" startWordPosition="795" endWordPosition="798">gs or senses it is likely to have (Zipf 1929). If this is true, we would expect that very frequent prepositions, such as &amp;quot;of&amp;quot;, would have many possible meanings and therefore not reliably predict a semantic relation. However, less frequent prepositions, such as &amp;quot;while&amp;quot; would have a more limited set of senses and therefore accurately predict a semantic relation. 2.2 Machine Learning Algorithms We are also interested in comparing the performance of machine learning algorithms on the task of mapping from n-gram frequencies of joining terms to semantic relations. For the experiments we use Weka, (Witten and Frank, 1999) a machine learning toolkit which allows for fast experimentation with many standard learning algorithms. In Section 5 we present the results obtained using the nearestneighbor, neural network (i.e. multi-layer perceptron) and SVM. The mechanisms of these different learning approaches will be discussed briefly in Section 4. 3 Related Work 3.1 Web Mining Much of the recent work conducted on the problem of assigning semantic relations to noun phrases has used the web as a corpus. The use of hit counts from web search engines to obtain lexical information was introduced by Turney (2001). The idea</context>
</contexts>
<marker>Witten, Frank, 1999</marker>
<rawString>Ian H. Witten and Eibe Frank. 1999. Data Mining: Practical Machine Learning Tools and Techniques with Java Implementations. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George K Zipf</author>
</authors>
<title>Selected Studies of the Principle of Relative Frequency in Language.</title>
<date>1932</date>
<location>Cambridge, MA.</location>
<marker>Zipf, 1932</marker>
<rawString>George K. Zipf. 1932. Selected Studies of the Principle of Relative Frequency in Language. Cambridge, MA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>