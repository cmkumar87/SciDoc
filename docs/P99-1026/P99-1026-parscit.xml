<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven Abney</author>
</authors>
<title>Partial parsing via finite-state cascades.</title>
<date>1996</date>
<booktitle>In Proceedings of the ESSLLI &apos;96 Robust Parsing Workshop,</booktitle>
<pages>8--15</pages>
<contexts>
<context position="11827" citStr="Abney, 1996" startWordPosition="1836" endWordPosition="1837">on Wednesday. The most likely significant-utterance sequence consists of &amp;quot;I need to, uh, book Room 2&amp;quot; and &amp;quot;it&apos;s on Wednesday&amp;quot;. From the speech act representation of these utterances, the system can infer the user wants to book Room 2 on Wednesday. 4.3 Finding Significant-Utterance Sequences SUs are identified in the process of understanding. Unlike ordinary parsers, the understanding module does not try to determine whether the whole input forms an SU or not, but instead determines where SUs are. Although this can be considered a kind of partial parsing technique (McDonald, 1992; Lavie, 1996; Abney, 1996), the SUs obtained by ISSS are not always subsentential phrases; they are sometimes full sentences. For one discourse, multiple significant-utterance sequences can be considered. &amp;quot;Wednesday next week&amp;quot; above illustrates this well. Let us assume that the parser finds two SUs, &amp;quot;Wednesday&amp;quot; and &amp;quot;Wednesday next week&amp;quot;. Then three significantutterance sequences are possible: one consisting of &amp;quot;Wednesday&amp;quot;, one consisting of &amp;quot;Wednesday next 202 week&amp;quot;, and one consisting of no SUs. The second sequence is obviously the most likely at this point, but it is not possible to choose only one sequence and disca</context>
</contexts>
<marker>Abney, 1996</marker>
<rawString>Steven Abney. 1996. Partial parsing via finite-state cascades. In Proceedings of the ESSLLI &apos;96 Robust Parsing Workshop, pages 8-15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James F Allen</author>
<author>Bradford W Miller</author>
<author>Eric K Ringger</author>
<author>Teresa Sikorski</author>
</authors>
<title>A robust system for natural spoken dialogue.</title>
<date>1996</date>
<booktitle>In Proceedings of ACL-96,</booktitle>
<pages>62--70</pages>
<contexts>
<context position="2025" citStr="Allen et al. (1996)" startWordPosition="275" endWordPosition="278">mputers to understand unrestricted human utterances and respond appropriately to them. Considering the current level of speech recognition technology, system-initiative dialogue systems, which prohibit users from speaking unrestrictedly, are preferred (Walker et al., 1998). Nevertheless, we are still pursuing techniques for understanding unrestricted user utterances because, if the accuracy of understanding can be improved, systems that allow users to speak freely could be developed and these would be more useful than systems that do not. Most previous spoken dialogue systems (e.g. systems by Allen et al. (1996), Zue et al. (1994) and Peckham (1993)) assume that the user makes one utterance unit in each speech interval, unless the push-to-talk method is used. Here, by utterance unit we mean a phrase from which a speech act representation is derived, and it corresponds to a sentence in written language. We also use speech act in this paper to mean a command that updates the hearer&apos;s belief state about the speaker&apos;s intention and the context of the dialogue. In this paper, a system using this assumption is called an interval-based system. The above assumption no longer holds when no restrictions are pl</context>
</contexts>
<marker>Allen, Miller, Ringger, Sikorski, 1996</marker>
<rawString>James F. Allen, Bradford W. Miller, Eric K. Ringger, and Teresa Sikorski. 1996. A robust system for natural spoken dialogue. In Proceedings of ACL-96, pages 62-70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harald Aust</author>
<author>Martin Oerder</author>
<author>Frank Seide</author>
<author>Volker Steinbiss</author>
</authors>
<title>The Philips automatic train timetable information system.</title>
<date>1995</date>
<journal>Speech Communication,</journal>
<pages>17--249</pages>
<contexts>
<context position="7196" citStr="Aust et al., 1995" startWordPosition="1095" endWordPosition="1098">eman and Allen, 1997) and probabilistic language models (Stolcke et al., 1998; Ramaswamy and Kleindienst, 1998; Cettolo and Falavigna, 1998). Since these methods are not perfect, the resulting segments do not always correspond to utterances and might not be parsable because of speech recognition errors. In addition, since the algorithms of the probabilistic methods are not designed to work in an incremental way, they cannot be used in real-time analysis in a straightforward way. Some methods use keyword detection (Rose, 1995; Hatazaki et al., 1994; Seto et al., 1994) and key-phrase detection (Aust et al., 1995; Kawahara et al., 1996) to understand speech mainly because the speech recognition score is not high enough. The lack of the full use of syntax in these approaches, however, means user utterances might be misunderstood even if the speech recognition gave the correct answer. Zechner and Waibel (1998) and Worm (1998) proposed understanding utterances by combining partial parses. Their methods, however, cannot syntactically analyze phrases across pauses since they use speech intervals as input units. Although Lavie et al. (1997) proposed a segmentation method that combines segmentation prior to </context>
</contexts>
<marker>Aust, Oerder, Seide, Steinbiss, 1995</marker>
<rawString>Harald Aust, Martin Oerder, Frank Seide, and Volker Steinbiss. 1995. The Philips automatic train timetable information system. Speech Communication, 17:249-262.</rawString>
</citation>
<citation valid="false">
<title>Note that 91% of user speech intervals were well-formed substrings (not necessary SUs).</title>
<marker></marker>
<rawString>Note that 91% of user speech intervals were well-formed substrings (not necessary SUs).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mauro Cettolo</author>
<author>Daniele Falavigna</author>
</authors>
<title>Automatic detection of semantic boundaries based on acoustic and lexical knowledge.</title>
<date>1998</date>
<booktitle>In Proceedings of ICSLP-98,</booktitle>
<pages>1551--1554</pages>
<contexts>
<context position="6719" citStr="Cettolo and Falavigna, 1998" startWordPosition="1017" endWordPosition="1020">erance boundary in unrestricted user utterances, the system cannot make an interpretation and thus cannot respond appropriately. Waiting for a long pause enables an interpretation, but prevents response in real time. We therefore need a way to reconcile real-time understanding and analysis without boundary clues. 3 Previous Work Several techniques have been proposed to segment user utterances prior to parsing. They use intonation (Wang and Hirschberg, 1992; Traum and Heeman, 1997; Heeman and Allen, 1997) and probabilistic language models (Stolcke et al., 1998; Ramaswamy and Kleindienst, 1998; Cettolo and Falavigna, 1998). Since these methods are not perfect, the resulting segments do not always correspond to utterances and might not be parsable because of speech recognition errors. In addition, since the algorithms of the probabilistic methods are not designed to work in an incremental way, they cannot be used in real-time analysis in a straightforward way. Some methods use keyword detection (Rose, 1995; Hatazaki et al., 1994; Seto et al., 1994) and key-phrase detection (Aust et al., 1995; Kawahara et al., 1996) to understand speech mainly because the speech recognition score is not high enough. The lack of t</context>
</contexts>
<marker>Cettolo, Falavigna, 1998</marker>
<rawString>Mauro Cettolo and Daniele Falavigna. 1998. Automatic detection of semantic boundaries based on acoustic and lexical knowledge. In Proceedings of ICSLP-98, pages 1551-1554.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul R Cohen</author>
</authors>
<title>Empirical Methods for Artificial Intelligence.</title>
<date>1995</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="25167" citStr="Cohen, 1995" startWordPosition="4029" endWordPosition="4030">overed by the recognition grammar due to the small vocabulary size of the recognition grammar. For the remaining 50% of the intervals, the word error rate of recognition was about 20%. The word error rate is defined as 100 * ( substitutions + deletions + insertions ) I ( correct + substitutions + deletions ) (Zechner and Waibel, 1998). 4In this test, we used a kind of censored mean which is computed by taking the mean of the logarithms of the ratios of the times only for the subjects that completed the tasks with both systems. The population distribution was estimated by the bootstrap method (Cohen, 1995). with system B is that, compared to system A, the probability that it understood user utterances was much lower. This is because the recognition results of speech intervals do not always form one SU. About 67% of all recognition results of user speech intervals were SUs or fillers.5 Needless to say, these results depend on the recognition grammar, the grammar for understanding, the response strategy and other factors. It has been suggested, however, that assuming each speech interval to be an utterance unit could reduce system performance and that ISSS is effective. 6 Concluding Remarks This </context>
</contexts>
<marker>Cohen, 1995</marker>
<rawString>Paul R. Cohen. 1995. Empirical Methods for Artificial Intelligence. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark G Core</author>
<author>Lenhart K Schubert</author>
</authors>
<title>Handling speech repairs and other disruptions through parser metarules.</title>
<date>1997</date>
<booktitle>In Working Notes of AAAI Spring Symposium on Computational Models for Mixed Initiative Interaction,</booktitle>
<pages>23--29</pages>
<contexts>
<context position="7925" citStr="Core and Schubert (1997)" startWordPosition="1207" endWordPosition="1210">ugh. The lack of the full use of syntax in these approaches, however, means user utterances might be misunderstood even if the speech recognition gave the correct answer. Zechner and Waibel (1998) and Worm (1998) proposed understanding utterances by combining partial parses. Their methods, however, cannot syntactically analyze phrases across pauses since they use speech intervals as input units. Although Lavie et al. (1997) proposed a segmentation method that combines segmentation prior to parsing and segmentation during parsing, but it suffers from the same problem. In the parser proposed by Core and Schubert (1997), utterances interrupted by the other dialogue participant are analyzed based on meta-rules. It is unclear, however, how this parser can be incorpo201 rated into a real-time dialogue system; it seems that it cannot output analysis results without boundary clues. 4 Incremental Significant-UtteranceSequence Search Method 4.1 Overview The above problem can be solved by incremental understanding, which means obtaining the most plausible interpretation of user utterances every time a word hypothesis is inputted from the speech recognizer. For incremental understanding, we propose incremental signif</context>
</contexts>
<marker>Core, Schubert, 1997</marker>
<rawString>Mark G. Core and Lenhart K. Schubert. 1997. Handling speech repairs and other disruptions through parser metarules. In Working Notes of AAAI Spring Symposium on Computational Models for Mixed Initiative Interaction, pages 23-29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giinther Gorz</author>
<author>Marcus Kesseler</author>
<author>Thrg Spilker</author>
<author>Hans Weber</author>
</authors>
<title>Research on architectures for integrated speech/language systems in Verbmobil.</title>
<date>1996</date>
<booktitle>In Proceedings of COLING-96,</booktitle>
<pages>484--489</pages>
<contexts>
<context position="18930" citStr="Gorz et al., 1996" startWordPosition="3012" endWordPosition="3015">gnition User utterance Pause detection 204 grammar (Noda et al., 1998). This grammar is weak enough to capture spontaneously spoken utterances, which sometimes include fillers and self-repairs, and allows each speech interval to be an arbitrary number of arbitrary bunsetsu phrases.&apos; The grammar contains less than one hundred words for each task; we reduced the vocabulary size so that the speech recognizer could output results in real time. The speech recognizer incrementally outputs word hypotheses as soon as they are found in the best-scored path in the forward search (Hirasawa et al., 1998; Gorz et al., 1996). Since each word hypothesis is accompanied by the pointer to its preceding word, the understanding module can reconstruct word sequences. The newest word hypothesis determines the word sequence that is acoustically most likely at a point in time.2 The utterance understanding module works based on ISSS and uses a domain-dependent unification grammar with a context-free backbone that is based on bunsetsu phrases. This grammar is more restrictive than the grammar for speech recognition, but covers phenomena peculiar to spoken language such as particle omission and self-repairs. A belief state is</context>
</contexts>
<marker>Gorz, Kesseler, Spilker, Weber, 1996</marker>
<rawString>Giinther Gorz, Marcus Kesseler, Thrg Spilker, and Hans Weber. 1996. Research on architectures for integrated speech/language systems in Verbmobil. In Proceedings of COLING-96, pages 484-489.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kaichiro Hatazaki</author>
<author>Farzad Ehsani</author>
<author>Jun Noguchi</author>
<author>Takao Watanabe</author>
</authors>
<title>Speech dialogue system based on simultaneous understanding.</title>
<date>1994</date>
<journal>Speech Communication,</journal>
<pages>15--323</pages>
<contexts>
<context position="7132" citStr="Hatazaki et al., 1994" startWordPosition="1084" endWordPosition="1087">se intonation (Wang and Hirschberg, 1992; Traum and Heeman, 1997; Heeman and Allen, 1997) and probabilistic language models (Stolcke et al., 1998; Ramaswamy and Kleindienst, 1998; Cettolo and Falavigna, 1998). Since these methods are not perfect, the resulting segments do not always correspond to utterances and might not be parsable because of speech recognition errors. In addition, since the algorithms of the probabilistic methods are not designed to work in an incremental way, they cannot be used in real-time analysis in a straightforward way. Some methods use keyword detection (Rose, 1995; Hatazaki et al., 1994; Seto et al., 1994) and key-phrase detection (Aust et al., 1995; Kawahara et al., 1996) to understand speech mainly because the speech recognition score is not high enough. The lack of the full use of syntax in these approaches, however, means user utterances might be misunderstood even if the speech recognition gave the correct answer. Zechner and Waibel (1998) and Worm (1998) proposed understanding utterances by combining partial parses. Their methods, however, cannot syntactically analyze phrases across pauses since they use speech intervals as input units. Although Lavie et al. (1997) pro</context>
</contexts>
<marker>Hatazaki, Ehsani, Noguchi, Watanabe, 1994</marker>
<rawString>Kaichiro Hatazaki, Farzad Ehsani, Jun Noguchi, and Takao Watanabe. 1994. Speech dialogue system based on simultaneous understanding. Speech Communication, 15:323-330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter A Heeman</author>
<author>James F Allen</author>
</authors>
<title>Intonational boundaries, speech repairs, and discourse markers: Modeling spoken dialog.</title>
<date>1997</date>
<booktitle>In Proceedings of ACL/EACL-97.</booktitle>
<contexts>
<context position="6600" citStr="Heeman and Allen, 1997" startWordPosition="1000" endWordPosition="1003">function words that represent negation come after content words. Since there is no explicit clue indicating an utterance boundary in unrestricted user utterances, the system cannot make an interpretation and thus cannot respond appropriately. Waiting for a long pause enables an interpretation, but prevents response in real time. We therefore need a way to reconcile real-time understanding and analysis without boundary clues. 3 Previous Work Several techniques have been proposed to segment user utterances prior to parsing. They use intonation (Wang and Hirschberg, 1992; Traum and Heeman, 1997; Heeman and Allen, 1997) and probabilistic language models (Stolcke et al., 1998; Ramaswamy and Kleindienst, 1998; Cettolo and Falavigna, 1998). Since these methods are not perfect, the resulting segments do not always correspond to utterances and might not be parsable because of speech recognition errors. In addition, since the algorithms of the probabilistic methods are not designed to work in an incremental way, they cannot be used in real-time analysis in a straightforward way. Some methods use keyword detection (Rose, 1995; Hatazaki et al., 1994; Seto et al., 1994) and key-phrase detection (Aust et al., 1995; Ka</context>
</contexts>
<marker>Heeman, Allen, 1997</marker>
<rawString>Peter A. Heeman and James F. Allen. 1997. Intonational boundaries, speech repairs, and discourse markers: Modeling spoken dialog. In Proceedings of ACL/EACL-97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun-ichi Hirasawa</author>
<author>Noboru Miyazaki</author>
<author>Mikio Nakano</author>
<author>Takeshi Kawabata</author>
</authors>
<title>Implementation of coordinative nodding behavior on spoken dialogue systems.</title>
<date>1998</date>
<booktitle>In Proceedings of ICSLP-98,</booktitle>
<pages>2347--2350</pages>
<contexts>
<context position="18910" citStr="Hirasawa et al., 1998" startWordPosition="3008" endWordPosition="3011">SSS method) Speech Recognition User utterance Pause detection 204 grammar (Noda et al., 1998). This grammar is weak enough to capture spontaneously spoken utterances, which sometimes include fillers and self-repairs, and allows each speech interval to be an arbitrary number of arbitrary bunsetsu phrases.&apos; The grammar contains less than one hundred words for each task; we reduced the vocabulary size so that the speech recognizer could output results in real time. The speech recognizer incrementally outputs word hypotheses as soon as they are found in the best-scored path in the forward search (Hirasawa et al., 1998; Gorz et al., 1996). Since each word hypothesis is accompanied by the pointer to its preceding word, the understanding module can reconstruct word sequences. The newest word hypothesis determines the word sequence that is acoustically most likely at a point in time.2 The utterance understanding module works based on ISSS and uses a domain-dependent unification grammar with a context-free backbone that is based on bunsetsu phrases. This grammar is more restrictive than the grammar for speech recognition, but covers phenomena peculiar to spoken language such as particle omission and self-repair</context>
</contexts>
<marker>Hirasawa, Miyazaki, Nakano, Kawabata, 1998</marker>
<rawString>Jun-ichi Hirasawa, Noboru Miyazaki, Mikio Nakano, and Takeshi Kawabata. 1998. Implementation of coordinative nodding behavior on spoken dialogue systems. In Proceedings of ICSLP-98, pages 2347-2350.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tatsuya Kawahara</author>
<author>Chin-Hui Lee</author>
<author>Biing-Hwang Juang</author>
</authors>
<title>Key-phrase detection and verification for flexible speech understanding.</title>
<date>1996</date>
<booktitle>In Proceedings of ICSLP-96,</booktitle>
<pages>861--864</pages>
<contexts>
<context position="7220" citStr="Kawahara et al., 1996" startWordPosition="1099" endWordPosition="1102">7) and probabilistic language models (Stolcke et al., 1998; Ramaswamy and Kleindienst, 1998; Cettolo and Falavigna, 1998). Since these methods are not perfect, the resulting segments do not always correspond to utterances and might not be parsable because of speech recognition errors. In addition, since the algorithms of the probabilistic methods are not designed to work in an incremental way, they cannot be used in real-time analysis in a straightforward way. Some methods use keyword detection (Rose, 1995; Hatazaki et al., 1994; Seto et al., 1994) and key-phrase detection (Aust et al., 1995; Kawahara et al., 1996) to understand speech mainly because the speech recognition score is not high enough. The lack of the full use of syntax in these approaches, however, means user utterances might be misunderstood even if the speech recognition gave the correct answer. Zechner and Waibel (1998) and Worm (1998) proposed understanding utterances by combining partial parses. Their methods, however, cannot syntactically analyze phrases across pauses since they use speech intervals as input units. Although Lavie et al. (1997) proposed a segmentation method that combines segmentation prior to parsing and segmentation</context>
</contexts>
<marker>Kawahara, Lee, Juang, 1996</marker>
<rawString>Tatsuya Kawahara, Chin-Hui Lee, and Biing-Hwang Juang. 1996. Key-phrase detection and verification for flexible speech understanding. In Proceedings of ICSLP-96, pages 861-864.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alon Lavie</author>
<author>Donna Gates</author>
<author>Noah Coccaro</author>
<author>Lori Levin</author>
</authors>
<title>Input segmentation of spontaneous speech in JANUS: A speech-to-speech translation system.</title>
<date>1997</date>
<booktitle>Dialogue Processing in Spoken Language Systems,</booktitle>
<pages>86--99</pages>
<editor>In Elisabeth Maier, Marion Mast, and Susann LuperFoy, editors,</editor>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="7728" citStr="Lavie et al. (1997)" startWordPosition="1177" endWordPosition="1180">; Hatazaki et al., 1994; Seto et al., 1994) and key-phrase detection (Aust et al., 1995; Kawahara et al., 1996) to understand speech mainly because the speech recognition score is not high enough. The lack of the full use of syntax in these approaches, however, means user utterances might be misunderstood even if the speech recognition gave the correct answer. Zechner and Waibel (1998) and Worm (1998) proposed understanding utterances by combining partial parses. Their methods, however, cannot syntactically analyze phrases across pauses since they use speech intervals as input units. Although Lavie et al. (1997) proposed a segmentation method that combines segmentation prior to parsing and segmentation during parsing, but it suffers from the same problem. In the parser proposed by Core and Schubert (1997), utterances interrupted by the other dialogue participant are analyzed based on meta-rules. It is unclear, however, how this parser can be incorpo201 rated into a real-time dialogue system; it seems that it cannot output analysis results without boundary clues. 4 Incremental Significant-UtteranceSequence Search Method 4.1 Overview The above problem can be solved by incremental understanding, which m</context>
</contexts>
<marker>Lavie, Gates, Coccaro, Levin, 1997</marker>
<rawString>Alon Lavie, Donna Gates, Noah Coccaro, and Lori Levin. 1997. Input segmentation of spontaneous speech in JANUS: A speech-to-speech translation system. In Elisabeth Maier, Marion Mast, and Susann LuperFoy, editors, Dialogue Processing in Spoken Language Systems, pages 86-99. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alon Lavie</author>
</authors>
<title>GLR* : A Robust Grammar-Focused Parser for Spontaneously Spoken Language.</title>
<date>1996</date>
<tech>Ph.D. thesis,</tech>
<institution>School of Computer Science, Carnegie Mellon University.</institution>
<contexts>
<context position="11813" citStr="Lavie, 1996" startWordPosition="1834" endWordPosition="1835"> 2, and it&apos;s on Wednesday. The most likely significant-utterance sequence consists of &amp;quot;I need to, uh, book Room 2&amp;quot; and &amp;quot;it&apos;s on Wednesday&amp;quot;. From the speech act representation of these utterances, the system can infer the user wants to book Room 2 on Wednesday. 4.3 Finding Significant-Utterance Sequences SUs are identified in the process of understanding. Unlike ordinary parsers, the understanding module does not try to determine whether the whole input forms an SU or not, but instead determines where SUs are. Although this can be considered a kind of partial parsing technique (McDonald, 1992; Lavie, 1996; Abney, 1996), the SUs obtained by ISSS are not always subsentential phrases; they are sometimes full sentences. For one discourse, multiple significant-utterance sequences can be considered. &amp;quot;Wednesday next week&amp;quot; above illustrates this well. Let us assume that the parser finds two SUs, &amp;quot;Wednesday&amp;quot; and &amp;quot;Wednesday next week&amp;quot;. Then three significantutterance sequences are possible: one consisting of &amp;quot;Wednesday&amp;quot;, one consisting of &amp;quot;Wednesday next 202 week&amp;quot;, and one consisting of no SUs. The second sequence is obviously the most likely at this point, but it is not possible to choose only one sequ</context>
</contexts>
<marker>Lavie, 1996</marker>
<rawString>Alon Lavie. 1996. GLR* : A Robust Grammar-Focused Parser for Spontaneously Spoken Language. Ph.D. thesis, School of Computer Science, Carnegie Mellon University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D McDonald</author>
</authors>
<title>An efficient chart-based algorithm for partial-parsing of unrestricted texts.</title>
<date>1992</date>
<booktitle>In Proceedings of the Third Conference on Applied Natural Language Processing,</booktitle>
<pages>193--200</pages>
<contexts>
<context position="11800" citStr="McDonald, 1992" startWordPosition="1832" endWordPosition="1833">o, uh, book Room 2, and it&apos;s on Wednesday. The most likely significant-utterance sequence consists of &amp;quot;I need to, uh, book Room 2&amp;quot; and &amp;quot;it&apos;s on Wednesday&amp;quot;. From the speech act representation of these utterances, the system can infer the user wants to book Room 2 on Wednesday. 4.3 Finding Significant-Utterance Sequences SUs are identified in the process of understanding. Unlike ordinary parsers, the understanding module does not try to determine whether the whole input forms an SU or not, but instead determines where SUs are. Although this can be considered a kind of partial parsing technique (McDonald, 1992; Lavie, 1996; Abney, 1996), the SUs obtained by ISSS are not always subsentential phrases; they are sometimes full sentences. For one discourse, multiple significant-utterance sequences can be considered. &amp;quot;Wednesday next week&amp;quot; above illustrates this well. Let us assume that the parser finds two SUs, &amp;quot;Wednesday&amp;quot; and &amp;quot;Wednesday next week&amp;quot;. Then three significantutterance sequences are possible: one consisting of &amp;quot;Wednesday&amp;quot;, one consisting of &amp;quot;Wednesday next 202 week&amp;quot;, and one consisting of no SUs. The second sequence is obviously the most likely at this point, but it is not possible to choose </context>
</contexts>
<marker>McDonald, 1992</marker>
<rawString>David D. McDonald. 1992. An efficient chart-based algorithm for partial-parsing of unrestricted texts. In Proceedings of the Third Conference on Applied Natural Language Processing, pages 193-200.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshiaki Noda</author>
<author>Yoshikazu Yamaguchi</author>
</authors>
<title>Tomokazu Yamada, Akihiro Imamura, Satoshi Takahashi, Tomoko Matsui, and Kiyoaki Aikawa.</title>
<date>1998</date>
<booktitle>In Proceedings of the 1998 IEICE General Conference</booktitle>
<pages>14--9</pages>
<note>(in Japanese).</note>
<marker>Noda, Yamaguchi, 1998</marker>
<rawString>Yoshiaki Noda, Yoshikazu Yamaguchi, Tomokazu Yamada, Akihiro Imamura, Satoshi Takahashi, Tomoko Matsui, and Kiyoaki Aikawa. 1998. The development of speech recognition engine REX. In Proceedings of the 1998 IEICE General Conference D-14-9, page 220. (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeremy Peckham</author>
</authors>
<title>A new generation of spoken language systems: Results and lessons from the SUNDIAL project.</title>
<date>1993</date>
<booktitle>In Proceedings of Eurospeech93,</booktitle>
<pages>33--40</pages>
<contexts>
<context position="2063" citStr="Peckham (1993)" startWordPosition="284" endWordPosition="285">terances and respond appropriately to them. Considering the current level of speech recognition technology, system-initiative dialogue systems, which prohibit users from speaking unrestrictedly, are preferred (Walker et al., 1998). Nevertheless, we are still pursuing techniques for understanding unrestricted user utterances because, if the accuracy of understanding can be improved, systems that allow users to speak freely could be developed and these would be more useful than systems that do not. Most previous spoken dialogue systems (e.g. systems by Allen et al. (1996), Zue et al. (1994) and Peckham (1993)) assume that the user makes one utterance unit in each speech interval, unless the push-to-talk method is used. Here, by utterance unit we mean a phrase from which a speech act representation is derived, and it corresponds to a sentence in written language. We also use speech act in this paper to mean a command that updates the hearer&apos;s belief state about the speaker&apos;s intention and the context of the dialogue. In this paper, a system using this assumption is called an interval-based system. The above assumption no longer holds when no restrictions are placed on the way the user speaks. This </context>
</contexts>
<marker>Peckham, 1993</marker>
<rawString>Jeremy Peckham. 1993. A new generation of spoken language systems: Results and lessons from the SUNDIAL project. In Proceedings of Eurospeech93, pages 33-40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ganesh N Ramaswamy</author>
<author>Jan Kleindienst</author>
</authors>
<title>Automatic identification of command boundaries in a conversational natural language user interface.</title>
<date>1998</date>
<booktitle>In Proceedings of ICSLP-98,</booktitle>
<pages>401--404</pages>
<contexts>
<context position="6689" citStr="Ramaswamy and Kleindienst, 1998" startWordPosition="1013" endWordPosition="1016">o explicit clue indicating an utterance boundary in unrestricted user utterances, the system cannot make an interpretation and thus cannot respond appropriately. Waiting for a long pause enables an interpretation, but prevents response in real time. We therefore need a way to reconcile real-time understanding and analysis without boundary clues. 3 Previous Work Several techniques have been proposed to segment user utterances prior to parsing. They use intonation (Wang and Hirschberg, 1992; Traum and Heeman, 1997; Heeman and Allen, 1997) and probabilistic language models (Stolcke et al., 1998; Ramaswamy and Kleindienst, 1998; Cettolo and Falavigna, 1998). Since these methods are not perfect, the resulting segments do not always correspond to utterances and might not be parsable because of speech recognition errors. In addition, since the algorithms of the probabilistic methods are not designed to work in an incremental way, they cannot be used in real-time analysis in a straightforward way. Some methods use keyword detection (Rose, 1995; Hatazaki et al., 1994; Seto et al., 1994) and key-phrase detection (Aust et al., 1995; Kawahara et al., 1996) to understand speech mainly because the speech recognition score is </context>
</contexts>
<marker>Ramaswamy, Kleindienst, 1998</marker>
<rawString>Ganesh N. Ramaswamy and Jan Kleindienst. 1998. Automatic identification of command boundaries in a conversational natural language user interface. In Proceedings of ICSLP-98, pages 401-404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R C Rose</author>
</authors>
<title>Keyword detection in conversational speech utterances using hidden Markov model based continuous speech recognition.</title>
<date>1995</date>
<journal>Computer Speech and Language,</journal>
<pages>9--309</pages>
<contexts>
<context position="7109" citStr="Rose, 1995" startWordPosition="1082" endWordPosition="1083">sing. They use intonation (Wang and Hirschberg, 1992; Traum and Heeman, 1997; Heeman and Allen, 1997) and probabilistic language models (Stolcke et al., 1998; Ramaswamy and Kleindienst, 1998; Cettolo and Falavigna, 1998). Since these methods are not perfect, the resulting segments do not always correspond to utterances and might not be parsable because of speech recognition errors. In addition, since the algorithms of the probabilistic methods are not designed to work in an incremental way, they cannot be used in real-time analysis in a straightforward way. Some methods use keyword detection (Rose, 1995; Hatazaki et al., 1994; Seto et al., 1994) and key-phrase detection (Aust et al., 1995; Kawahara et al., 1996) to understand speech mainly because the speech recognition score is not high enough. The lack of the full use of syntax in these approaches, however, means user utterances might be misunderstood even if the speech recognition gave the correct answer. Zechner and Waibel (1998) and Worm (1998) proposed understanding utterances by combining partial parses. Their methods, however, cannot syntactically analyze phrases across pauses since they use speech intervals as input units. Although </context>
</contexts>
<marker>Rose, 1995</marker>
<rawString>R. C. Rose. 1995. Keyword detection in conversational speech utterances using hidden Markov model based continuous speech recognition. Computer Speech and Language, 9:309-333.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Seligman</author>
<author>Junko Hosaka</author>
<author>Harald Singer</author>
</authors>
<title>Pause units&amp;quot; and analysis of spontaneous Japanese dialogues: Preliminary studies.</title>
<date>1997</date>
<booktitle>Dialogue Processing in Spoken Language Systems,</booktitle>
<pages>100--112</pages>
<editor>In Elisabeth Maier, Marion Mast, and Susann LuperFoy, editors,</editor>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="5094" citStr="Seligman et al., 1997" startWordPosition="763" endWordPosition="766">ndaries in spontaneous speech in real time using only pauses. Observation of human-human dialogues reveals that humans often put pauses in utterances and sometimes do not put pauses at utterance boundaries. The following human utterance shows where pauses might appear in an utterance. I&apos;d like to make a reservation for a conference room (pause) for, uh (pause) this afternoon (pause) at about (pause) say (pause) 2 or 3 o&apos;clock (pause) for (pause) 15 people As far as Japanese is concerned, several studies have pointed out that speech intervals in dialogues are not always well-formed substrings (Seligman et al., 1997; Takezawa and Morimoto, 1997). On the other hand, since parsing results cannot be obtained unless the end of the utterance is identified, making real-time responses is impossible without boundary information. For example, consider the utterance &amp;quot;I&apos;d like to book Meeting Room 1 on Wednesday&amp;quot;. It is expected that the system should infer the user wants to reserve the room on &apos;Wednesday this week&apos; if this utterance was made on Monday. In real conversations, however, there is no guarantee that &apos;Wednesday&apos; is the final word of the utterance. It might be followed by the phrase &apos;next week&apos;, in which </context>
</contexts>
<marker>Seligman, Hosaka, Singer, 1997</marker>
<rawString>Marc Seligman, Junko Hosaka, and Harald Singer. 1997. &amp;quot;Pause units&amp;quot; and analysis of spontaneous Japanese dialogues: Preliminary studies. In Elisabeth Maier, Marion Mast, and Susann LuperFoy, editors, Dialogue Processing in Spoken Language Systems, pages 100-112. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shigenobu Seto</author>
<author>Hiroshi Kanazawa</author>
<author>Hideaki Shinchi</author>
<author>Yoichi Takebayashi</author>
</authors>
<title>Spontaneous speech dialogue system TOSBURG-II and its evaluation.</title>
<date>1994</date>
<journal>Speech Communication,</journal>
<pages>15--341</pages>
<contexts>
<context position="7152" citStr="Seto et al., 1994" startWordPosition="1088" endWordPosition="1091"> Hirschberg, 1992; Traum and Heeman, 1997; Heeman and Allen, 1997) and probabilistic language models (Stolcke et al., 1998; Ramaswamy and Kleindienst, 1998; Cettolo and Falavigna, 1998). Since these methods are not perfect, the resulting segments do not always correspond to utterances and might not be parsable because of speech recognition errors. In addition, since the algorithms of the probabilistic methods are not designed to work in an incremental way, they cannot be used in real-time analysis in a straightforward way. Some methods use keyword detection (Rose, 1995; Hatazaki et al., 1994; Seto et al., 1994) and key-phrase detection (Aust et al., 1995; Kawahara et al., 1996) to understand speech mainly because the speech recognition score is not high enough. The lack of the full use of syntax in these approaches, however, means user utterances might be misunderstood even if the speech recognition gave the correct answer. Zechner and Waibel (1998) and Worm (1998) proposed understanding utterances by combining partial parses. Their methods, however, cannot syntactically analyze phrases across pauses since they use speech intervals as input units. Although Lavie et al. (1997) proposed a segmentation</context>
</contexts>
<marker>Seto, Kanazawa, Shinchi, Takebayashi, 1994</marker>
<rawString>Shigenobu Seto, Hiroshi Kanazawa, Hideaki Shinchi, and Yoichi Takebayashi. 1994. Spontaneous speech dialogue system TOSBURG-II and its evaluation. Speech Communication, 15:341-353.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Andreas Stolcke</author>
</authors>
<institution>Elizabeth Shriberg, Rebecca Bates, Mari Ostendorf, Dilek Haklcani, Madelaine Plauche,</institution>
<marker>Stolcke, </marker>
<rawString>Andreas Stolcke, Elizabeth Shriberg, Rebecca Bates, Mari Ostendorf, Dilek Haklcani, Madelaine Plauche,</rawString>
</citation>
<citation valid="true">
<authors>
<author>Golchan Tur</author>
<author>Yu Lu</author>
</authors>
<title>Automatic detection of sentence boundaries and disfluencies based on recognized words.</title>
<date>1998</date>
<booktitle>In Proceedings of ICSLP-98,</booktitle>
<pages>2247--2250</pages>
<marker>Tur, Lu, 1998</marker>
<rawString>Golchan Tur, and Yu Lu. 1998. Automatic detection of sentence boundaries and disfluencies based on recognized words. In Proceedings of ICSLP-98, pages 2247-2250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Toshiyuki Takezawa</author>
<author>Tsuyoshi Morimoto</author>
</authors>
<title>Dialogue speech recognition method using syntactic rules based on subtrees and preterminal bigrams.</title>
<date>1997</date>
<booktitle>Systems and Computers in Japan,</booktitle>
<pages>28--5</pages>
<contexts>
<context position="5124" citStr="Takezawa and Morimoto, 1997" startWordPosition="767" endWordPosition="770">speech in real time using only pauses. Observation of human-human dialogues reveals that humans often put pauses in utterances and sometimes do not put pauses at utterance boundaries. The following human utterance shows where pauses might appear in an utterance. I&apos;d like to make a reservation for a conference room (pause) for, uh (pause) this afternoon (pause) at about (pause) say (pause) 2 or 3 o&apos;clock (pause) for (pause) 15 people As far as Japanese is concerned, several studies have pointed out that speech intervals in dialogues are not always well-formed substrings (Seligman et al., 1997; Takezawa and Morimoto, 1997). On the other hand, since parsing results cannot be obtained unless the end of the utterance is identified, making real-time responses is impossible without boundary information. For example, consider the utterance &amp;quot;I&apos;d like to book Meeting Room 1 on Wednesday&amp;quot;. It is expected that the system should infer the user wants to reserve the room on &apos;Wednesday this week&apos; if this utterance was made on Monday. In real conversations, however, there is no guarantee that &apos;Wednesday&apos; is the final word of the utterance. It might be followed by the phrase &apos;next week&apos;, in which case the system made a mistake</context>
</contexts>
<marker>Takezawa, Morimoto, 1997</marker>
<rawString>Toshiyuki Takezawa and Tsuyoshi Morimoto. 1997. Dialogue speech recognition method using syntactic rules based on subtrees and preterminal bigrams. Systems and Computers in Japan, 28(5):22-32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David R Traum</author>
<author>Peter A Heeman</author>
</authors>
<title>Utterance units in spoken dialogue.</title>
<date>1997</date>
<booktitle>Dialogue Processing in Spoken Language Systems,</booktitle>
<pages>125--140</pages>
<editor>In Elisabeth Maier, Marion Mast, and Susann LuperFoy, editor&apos;s,</editor>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="6575" citStr="Traum and Heeman, 1997" startWordPosition="996" endWordPosition="999">uch as Japanese because function words that represent negation come after content words. Since there is no explicit clue indicating an utterance boundary in unrestricted user utterances, the system cannot make an interpretation and thus cannot respond appropriately. Waiting for a long pause enables an interpretation, but prevents response in real time. We therefore need a way to reconcile real-time understanding and analysis without boundary clues. 3 Previous Work Several techniques have been proposed to segment user utterances prior to parsing. They use intonation (Wang and Hirschberg, 1992; Traum and Heeman, 1997; Heeman and Allen, 1997) and probabilistic language models (Stolcke et al., 1998; Ramaswamy and Kleindienst, 1998; Cettolo and Falavigna, 1998). Since these methods are not perfect, the resulting segments do not always correspond to utterances and might not be parsable because of speech recognition errors. In addition, since the algorithms of the probabilistic methods are not designed to work in an incremental way, they cannot be used in real-time analysis in a straightforward way. Some methods use keyword detection (Rose, 1995; Hatazaki et al., 1994; Seto et al., 1994) and key-phrase detecti</context>
</contexts>
<marker>Traum, Heeman, 1997</marker>
<rawString>David R. Traum and Peter A. Heeman. 1997. Utterance units in spoken dialogue. In Elisabeth Maier, Marion Mast, and Susann LuperFoy, editor&apos;s, Dialogue Processing in Spoken Language Systems, pages 125-140. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn A Walker</author>
<author>Jeanne C Fromer</author>
<author>Shrikanth Narayanan</author>
</authors>
<title>Learning optimal dialogue strategies: A case study of a spoken dialogue agent for email.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL&apos;98.</booktitle>
<contexts>
<context position="1679" citStr="Walker et al., 1998" startWordPosition="220" endWordPosition="223"> than an understanding method that assumes pauses to be semantic boundaries. 1 Introduction Building a real-time, interactive spoken dialogue system has long been a dream of researchers, and the recent progress in hardware technology and speech and language processing technologies is making this dream a reality. It is still hard, however, for computers to understand unrestricted human utterances and respond appropriately to them. Considering the current level of speech recognition technology, system-initiative dialogue systems, which prohibit users from speaking unrestrictedly, are preferred (Walker et al., 1998). Nevertheless, we are still pursuing techniques for understanding unrestricted user utterances because, if the accuracy of understanding can be improved, systems that allow users to speak freely could be developed and these would be more useful than systems that do not. Most previous spoken dialogue systems (e.g. systems by Allen et al. (1996), Zue et al. (1994) and Peckham (1993)) assume that the user makes one utterance unit in each speech interval, unless the push-to-talk method is used. Here, by utterance unit we mean a phrase from which a speech act representation is derived, and it corr</context>
</contexts>
<marker>Walker, Fromer, Narayanan, 1998</marker>
<rawString>Marilyn A. Walker, Jeanne C. Fromer, and Shrikanth Narayanan. 1998. Learning optimal dialogue strategies: A case study of a spoken dialogue agent for email. In Proceedings of COLING-ACL&apos;98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michelle Q Wang</author>
<author>Julia Hirschberg</author>
</authors>
<title>Automatic classification of intonational phrase boundaries.</title>
<date>1992</date>
<journal>Computer Speech and Language,</journal>
<pages>6--175</pages>
<contexts>
<context position="6551" citStr="Wang and Hirschberg, 1992" startWordPosition="992" endWordPosition="995">s in head-final languages such as Japanese because function words that represent negation come after content words. Since there is no explicit clue indicating an utterance boundary in unrestricted user utterances, the system cannot make an interpretation and thus cannot respond appropriately. Waiting for a long pause enables an interpretation, but prevents response in real time. We therefore need a way to reconcile real-time understanding and analysis without boundary clues. 3 Previous Work Several techniques have been proposed to segment user utterances prior to parsing. They use intonation (Wang and Hirschberg, 1992; Traum and Heeman, 1997; Heeman and Allen, 1997) and probabilistic language models (Stolcke et al., 1998; Ramaswamy and Kleindienst, 1998; Cettolo and Falavigna, 1998). Since these methods are not perfect, the resulting segments do not always correspond to utterances and might not be parsable because of speech recognition errors. In addition, since the algorithms of the probabilistic methods are not designed to work in an incremental way, they cannot be used in real-time analysis in a straightforward way. Some methods use keyword detection (Rose, 1995; Hatazaki et al., 1994; Seto et al., 1994</context>
</contexts>
<marker>Wang, Hirschberg, 1992</marker>
<rawString>Michelle Q. Wang and Julia Hirschberg. 1992. Automatic classification of intonational phrase boundaries. Computer Speech and Language, 6:175-196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karsten L Worm</author>
</authors>
<title>A model for robust processing of spontaneous speech by integrating viable fragments.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL&apos;98,</booktitle>
<pages>1403--1407</pages>
<contexts>
<context position="7513" citStr="Worm (1998)" startWordPosition="1149" endWordPosition="1150"> the algorithms of the probabilistic methods are not designed to work in an incremental way, they cannot be used in real-time analysis in a straightforward way. Some methods use keyword detection (Rose, 1995; Hatazaki et al., 1994; Seto et al., 1994) and key-phrase detection (Aust et al., 1995; Kawahara et al., 1996) to understand speech mainly because the speech recognition score is not high enough. The lack of the full use of syntax in these approaches, however, means user utterances might be misunderstood even if the speech recognition gave the correct answer. Zechner and Waibel (1998) and Worm (1998) proposed understanding utterances by combining partial parses. Their methods, however, cannot syntactically analyze phrases across pauses since they use speech intervals as input units. Although Lavie et al. (1997) proposed a segmentation method that combines segmentation prior to parsing and segmentation during parsing, but it suffers from the same problem. In the parser proposed by Core and Schubert (1997), utterances interrupted by the other dialogue participant are analyzed based on meta-rules. It is unclear, however, how this parser can be incorpo201 rated into a real-time dialogue syste</context>
<context position="10795" citStr="Worm, 1998" startWordPosition="1665" endWordPosition="1666">the dialogue such as confirmation and denial. Considering a meeting room reservation system, examples of domain-related SUs are &amp;quot;I need to book Room 2 on Wednesday&amp;quot;, &amp;quot;I need to book Room 2&amp;quot;, and &amp;quot;Room 2&amp;quot; and dialogue-related ones are &amp;quot;yes&amp;quot;, &amp;quot;no&amp;quot;, and &amp;quot;Okay&amp;quot;. User utterances are understood by finding a sequence of SUs and updating the belief state based on the sequence. The utterances in the sequence do not overlap. In addition, they do not have to be adjacent to each other, which leads to robustness against speech recognition errors as in fragmentbased understanding (Zechner and Waibel, 1998; Worm, 1998). The belief state can be computed at any point in time if a significant-utterance sequence for user utterances up to that point in time is given. The belief state holds not only the user&apos;s intention but also the history of system utterances, so that all discourse information is stored in it. Consider, for example, the following user speech in a meeting room reservation dialogue. I need to, uh, book Room 2, and it&apos;s on Wednesday. The most likely significant-utterance sequence consists of &amp;quot;I need to, uh, book Room 2&amp;quot; and &amp;quot;it&apos;s on Wednesday&amp;quot;. From the speech act representation of these utterance</context>
</contexts>
<marker>Worm, 1998</marker>
<rawString>Karsten L. Worm. 1998. A model for robust processing of spontaneous speech by integrating viable fragments. In Proceedings of COLING-ACL&apos;98, pages 1403-1407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Zechner</author>
<author>Alex Waibel</author>
</authors>
<title>Using chunk based partial parsing of spontaneous speech in unrestricted domains for reducing word error rate in speech recognition.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL&apos;98,</booktitle>
<pages>1453--1459</pages>
<contexts>
<context position="7497" citStr="Zechner and Waibel (1998)" startWordPosition="1144" endWordPosition="1147">ion errors. In addition, since the algorithms of the probabilistic methods are not designed to work in an incremental way, they cannot be used in real-time analysis in a straightforward way. Some methods use keyword detection (Rose, 1995; Hatazaki et al., 1994; Seto et al., 1994) and key-phrase detection (Aust et al., 1995; Kawahara et al., 1996) to understand speech mainly because the speech recognition score is not high enough. The lack of the full use of syntax in these approaches, however, means user utterances might be misunderstood even if the speech recognition gave the correct answer. Zechner and Waibel (1998) and Worm (1998) proposed understanding utterances by combining partial parses. Their methods, however, cannot syntactically analyze phrases across pauses since they use speech intervals as input units. Although Lavie et al. (1997) proposed a segmentation method that combines segmentation prior to parsing and segmentation during parsing, but it suffers from the same problem. In the parser proposed by Core and Schubert (1997), utterances interrupted by the other dialogue participant are analyzed based on meta-rules. It is unclear, however, how this parser can be incorpo201 rated into a real-tim</context>
<context position="10782" citStr="Zechner and Waibel, 1998" startWordPosition="1661" endWordPosition="1664">espect to the progress of the dialogue such as confirmation and denial. Considering a meeting room reservation system, examples of domain-related SUs are &amp;quot;I need to book Room 2 on Wednesday&amp;quot;, &amp;quot;I need to book Room 2&amp;quot;, and &amp;quot;Room 2&amp;quot; and dialogue-related ones are &amp;quot;yes&amp;quot;, &amp;quot;no&amp;quot;, and &amp;quot;Okay&amp;quot;. User utterances are understood by finding a sequence of SUs and updating the belief state based on the sequence. The utterances in the sequence do not overlap. In addition, they do not have to be adjacent to each other, which leads to robustness against speech recognition errors as in fragmentbased understanding (Zechner and Waibel, 1998; Worm, 1998). The belief state can be computed at any point in time if a significant-utterance sequence for user utterances up to that point in time is given. The belief state holds not only the user&apos;s intention but also the history of system utterances, so that all discourse information is stored in it. Consider, for example, the following user speech in a meeting room reservation dialogue. I need to, uh, book Room 2, and it&apos;s on Wednesday. The most likely significant-utterance sequence consists of &amp;quot;I need to, uh, book Room 2&amp;quot; and &amp;quot;it&apos;s on Wednesday&amp;quot;. From the speech act representation of th</context>
<context position="24891" citStr="Zechner and Waibel, 1998" startWordPosition="3979" endWordPosition="3982">ere some utterances that the system misunderstood because of grammar limitations, excluding the data for the three subjects who had made those utterances did not change the statistical results. The reason it took longer to carry out the tasks 3About 50% of user speech intervals were not covered by the recognition grammar due to the small vocabulary size of the recognition grammar. For the remaining 50% of the intervals, the word error rate of recognition was about 20%. The word error rate is defined as 100 * ( substitutions + deletions + insertions ) I ( correct + substitutions + deletions ) (Zechner and Waibel, 1998). 4In this test, we used a kind of censored mean which is computed by taking the mean of the logarithms of the ratios of the times only for the subjects that completed the tasks with both systems. The population distribution was estimated by the bootstrap method (Cohen, 1995). with system B is that, compared to system A, the probability that it understood user utterances was much lower. This is because the recognition results of speech intervals do not always form one SU. About 67% of all recognition results of user speech intervals were SUs or fillers.5 Needless to say, these results depend o</context>
</contexts>
<marker>Zechner, Waibel, 1998</marker>
<rawString>Klaus Zechner and Alex Waibel. 1998. Using chunk based partial parsing of spontaneous speech in unrestricted domains for reducing word error rate in speech recognition. In Proceedings of COLING-ACL&apos;98, pages 1453-1459.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Victor Zue</author>
<author>Stephanie Seneff</author>
<author>Joseph Polifroni</author>
<author>Michael Phillips</author>
<author>Christine Pao</author>
<author>David Goodine</author>
<author>David Goddeau</author>
<author>James Glass</author>
</authors>
<title>PEGASUS: A spoken dialogue interface for on-line air travel planning.</title>
<date>1994</date>
<journal>Speech Communication,</journal>
<pages>15--331</pages>
<contexts>
<context position="2044" citStr="Zue et al. (1994)" startWordPosition="279" endWordPosition="282"> unrestricted human utterances and respond appropriately to them. Considering the current level of speech recognition technology, system-initiative dialogue systems, which prohibit users from speaking unrestrictedly, are preferred (Walker et al., 1998). Nevertheless, we are still pursuing techniques for understanding unrestricted user utterances because, if the accuracy of understanding can be improved, systems that allow users to speak freely could be developed and these would be more useful than systems that do not. Most previous spoken dialogue systems (e.g. systems by Allen et al. (1996), Zue et al. (1994) and Peckham (1993)) assume that the user makes one utterance unit in each speech interval, unless the push-to-talk method is used. Here, by utterance unit we mean a phrase from which a speech act representation is derived, and it corresponds to a sentence in written language. We also use speech act in this paper to mean a command that updates the hearer&apos;s belief state about the speaker&apos;s intention and the context of the dialogue. In this paper, a system using this assumption is called an interval-based system. The above assumption no longer holds when no restrictions are placed on the way the</context>
</contexts>
<marker>Zue, Seneff, Polifroni, Phillips, Pao, Goodine, Goddeau, Glass, 1994</marker>
<rawString>Victor Zue, Stephanie Seneff, Joseph Polifroni, Michael Phillips, Christine Pao, David Goodine, David Goddeau, and James Glass. 1994. PEGASUS: A spoken dialogue interface for on-line air travel planning. Speech Communication, 15:331-340.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>