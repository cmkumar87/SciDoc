                 ADCS 2009

        Proceedings of the Fourteenth
Australasian Document Computing Symposium




                          4 December 2009

                              Edited by
           Judy Kay, Paul Thomas, and Andrew Trotman




                       Technical report TR 645
       School of Information Technologies, University of Sydney


Proceedings of the Fourteenth Australasian Document Computing Symposium

                  University of New South Wales, Sydney, NSW
                                4 December 2009

                                   Published by
            School of Information Technologies, University of Sydney




                                      Editors

                                     Judy Kay
                                  Paul Thomas
                                 Andrew Trotman




                            ISBN: 978-1-74210-171-2
                           http://es.csiro.au/adcs2009


Proceedings of the Fourteenth Australasian Document Computing Symposium

                   University of New South Wales, Sydney, NSW
                                  4 December 2009

Chairs' preface


These proceedings contain the papers of the Fourteenth Australasian Document
Computing Symposium hosted by HCSNet at the University of New South Wales,
Sydney.

The varied long and short papers, as well as David Traum's and Mark Sanderson's
plenaries, are indicative of the wide breadth of research in the Australasian
document computing community and the wide scope for application.

The quality of submissions was once again high this year. Of the 32 papers
submitted (26 full and 6 short), 10 were accepted for presentation at the symposium
(28%) and 11 were accepted as posters (31%). All submissions received at least
two anonymous reviews by experts in the area, and several received three reviews.
Dual submissions were explicitly prohibited.

The members of the program committee and the extra reviewers deserve special
thanks for their effort, especially given the very tight turnaround needed for this
year's symposium. We would also like to thank HCSNet for its support of ADCS,
which freed us from worrying about most of the logisitics.

The ADCS community has contributed many good papers this year, but as before
the symposium's greatest benefit may be the opportunity it provides for researchers
and practitioners to meet and share ideas. We hope you enjoy it.


Symposium chair
Judy Kay             University of Sydney    Australia

Programme co-chairs
Andrew Trotman       University of Otago     New Zealand
Paul Thomas          CSIRO                   Australia

Programme committee
Alexander Krumpholz  CSIRO/ANU               Australia
Alistair Moffat      University of Melbourne Australia
Andrew Turpin        RMIT University         Australia
Christopher Lueg     University of Tasmania  Australia
David Hawking        Funnelback              Australia
Falk Scholer         RMIT University         Australia
Gitesh Raikundalia   Victoria University     Australia
James Thom           RMIT University         Australia
Judy Kay             University of Sydney    Australia
Nathan Rountree      University of Otago     New Zealand
Peter Bruza          QUT                     Australia
Ross Wilkinson       Australian National Data Service, Australia
Sally Jo Cunningham  University of Waikato   New Zealand
Shlomo Geva          QUT                     Australia
Timothy Jones        ANU                     Australia
Tom Rowlands         CSIRO/ANU               Australia
Vo Anh               University of Melbourne Australia
William Webber       University of Melbourne Australia
Yun Sing Koh         AUT                     New Zealand

Additional reviewers
Cécile Paris         CSIRO                   Australia
Richard O'Keefe      University of Otago     New Zealand
Stephen Wan          CSIRO                   Australia

ADCS steering committee
Alistair Moffat      University of Melbourne Australia
Andrew Trotman       University of Otago     New Zealand
Andrew Turpin        RMIT University         Australia
David Hawking        Funnelback              Australia
James Thom           RMIT University         Australia
Judy Kay             University of Sydney    Australia
Justin Zobel         University of Melbourne Australia
Paul Thomas          CSIRO                   Australia
Peter Bruza          QUT                     Australia
Ross Wilkinson       Australian National Data Service, Australia
Shlomo Geva          QUT                     Australia


Contents

Chairs' preface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . iii



    Plenary


Is this document relevant? Errr it'll do . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
   Mark Sanderson


    Session 1


Collaborative Filtering Recommender Systems based on Popular Tags . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
   Huizhi Liang, Yue Xu, Yuefeng Li and Richi Nayak

External Evaluation of Topic Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .11
   David Newman, Sarvnaz Karimi and Lawrence Cavedon

Id - Dynamic Views on Static and Dynamic Disassembly Listings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
   Nicholas Sherlock and Andrew Trotman

Interestingness Measures for Multi-Level Association Rules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
   Gavin Shaw, Yue Xu and Shlomo Geva

Do Users Find Looking at Text More Useful than Visual Representations? A Comparison of Three Search Result
Interfaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
   Hilal Al Maqbali, Falk Scholer, James A. Thom and Mingfang Wu


    Session 2


Random Indexing K-tree . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .43
   Chris De Vries, Lance De Vine and Shlomo Geva

Modelling Disagreement Between Judges for Information Retrieval System Evaluation . . . . . . . . . . . . . . . . . . .51
   Andrew Turpin and Falk Scholer

University Student Use of the Wikipedia . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
   Andrew Trotman and David Alexander

Feature Selection and Weighting in Sentiment Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67
   Tim O'Keefe and Irena Koprinska

The Use of Topic Representative Words in Text Categorization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75
   Su Nam Kim, Timothy Baldwin and Min-Yen Kan


    Poster presentations


N-Gram Word Segmentation for Chinese Wikipedia Using Phrase Mutual Information . . . . . . . . . . . . . . . . . . . 82
   Ling-Xiang Tang, Shlomo Geva, Andrew Trotman and Yue Xu

An Automatic Question Generation Tool for support Sourcing and Integration in Students' Essays . . . . . . . . 90
   Ming Liu and Rafael A. Calvo

You Are What You Post: User-level Features in Threaded Discourse . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .98
   Marco Lui and Timothy Baldwin

Investigating the use of Association Rules in Improving Recommender Systems . . . . . . . . . . . . . . . . . . . . . . . . 106
   Gavin Shaw, Yue Xu and Shlomo Geva

The Methodology of Manual Assessment in the Evaluation of Link Discovery . . . . . . . . . . . . . . . . . . . . . . . . . . 110
   Wei Che (Darren) Huang, Andrew Trotman and Shlomo Geva

Web Indexing on a Diet: Template Removal with the Sandwich Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115
   Tom Rowlands, Paul Thomas and Stephen Wan

Analyzing Web Multimedia Query Reformulation Behavior . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118
   Liang-Chun Jack Tseng, Dian Tjondronegoro and Amanda Spink

Term Clustering based on Lengths and Co-occurrences of Terms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126
   Michiko Yasukawa and Hidetoshi Yokoo

WriteProc: A Framework for Exploring Collaborative Writing Processes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .129
   Vilaythong Southavilay, Kalina Yacef and Rafael A. Calvo

An Analysis of Lyrics Questions on Yahoo! Answers: Implications for Lyric / Music Retrieval Systems . . . 137
   Sally Jo Cunningham and Simon Laing

Positive, Negative, or Mixed? Mining Blogs for Opinions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141
   Xiuzhen Zhang, Zhixin Zhou and Mingfang Wu

                             Is this document relevant? Errr it'll do

                                                 Mark Sanderson

                                             University of Sheffield

                                            m.sanderson@shef.ac.uk


Abstract     Evaluation of search engines is a critical
topic in the field of information retrieval.       Doing
evaluation well allows researchers to quickly and
efficiently understand if their new algorithms are a
valuable contribution or if they need to go back to
the drawing board.      The modern methods used for
evaluation developed by organizations such as TREC
in the US have their origins in research that started in
the early 1950s. Almost all of the core components of
modern testing environments, known as test collections,
were present in that early work. Potential problems
with the design of these collections were described in a
series of publications in the 1960s, but the criticisms
were largely ignored. However, in the past decade a
series of results were published showing potentially
catastrophic problems with a test collection's "ability"
to predict the way that users will work with searching
systems. A number of research teams showed that users
given a good system (as measured on a test collection)
searched no more effectively than users given one that
was bad.
     In this talk, I will briefly outline the history of
search evaluation, before detailing the work finding
problems with test collections.    I will then describe
some pioneering but relatively overlooked research
that pointed out that the key problem for researchers
isn't the question of how to measure searching systems
accurately, the problem is how to accurately measure
people.




Proceedings of the 14th Australasian Document Comput-
ing Symposium, Sydney, Australia, 4 December 2009.
Copyright for this article remains with the authors.




                                                        1

2

      Collaborative Filtering Recommender Systems based on Popular Tags

               Huizhi Liang                Yue Xu                 Yuefeng Li            Richi Nayak

                                        School of Information Technology
                                      Queensland University of Technology
                                         Queensland, QLD 4001, Australia

  oklianghuizi@gmail.com            yue.xu@qut.edu.au           y2.li@qut.edu.au        r.nayak@qut.edu.au


          1
Abstract The social tags in web 2.0 are becoming          CiteULike,     and    electronic   commerce       website
another important information source to profile users'    Amazon.com.
interests and preferences for making personalized            A social tag is a piece of brief textural information
recommendations.        However,     the     uncontrolled given by users explicitly and proactively to describe
vocabulary causes a lot of problems to profile users      and group items, thus it implies user's interests or
accurately, such as ambiguity, synonyms, misspelling,     preferences information. Therefore, the social tag
low information sharing etc. To solve these problems,     information can be used to profile user's interested
this paper proposes to use popular tags to represent      and preferred topics to          improve personalized
the actual topics of tags, the content of items, and also searching [1], generate user and item clusters [2], and
the topic interests of users. A novel user profiling      make     personalized    recommendations        [3]   etc.
approach is proposed in this paper that first identifies  However, as the tag terms are chosen by users freely
popular tags, then represents users' original tags        (i.e., uncontrolled vocabularies), social tags suffer
using the popular tags, finally generates users' topic    from many difficulties such as ambiguity in the
interests based on the popular tags. A collaborative      meaning of and differences between terms, a
filtering  based    recommender     system     has  been  proliferation   of   synonyms,     varying     levels    of
developed that builds the user profile using the          specificity, meaningless symbols, and lack of
proposed approach. The user profile generated using       guidance on syntax and slight variations of spelling
the proposed approach can represent user interests        and phrasing [4]. These problems cause inaccurate
more accurately and the information sharing among         user profiling and low information sharing among
users in the profile is also increased. Consequently the  users, and also bring challenges to generate proper
neighborhood of a user, which plays a crucial role in     neighborhood for making item recommendations and
collaborative filtering based recommenders, can be        consequently     result   in    low    recommendation
much more accurately determined. The experimental         performances. Therefore, a crucial problem in
results based on real world data obtained from            applying user tagging information to user profiling is
Amazon.com show that the proposed approach                to represent the semantic meanings of the tags.
outperforms other approaches.                                Popular tags refer to the tags that are used by many
                                                          users to collect items. Those popular tags are factual
Keywords Information          Retrieval,    recommender   tags [5] that often capture the tagged items' content
systems, social tags, web 2.0                             related information or topics while those tags that
                                                          have low popularity are often irrelevant to the content
                                                          of the tagged items or meaningless to other users, or
1 Introduction
                                                          even misspelled [5]. For one item, the popularity of
Collaborative tagging is a new means to organize and      using a tag to classify the item reflects the degree of
share information resources or items on the web such      common understanding to the tag and the item. High
as web pages, books, music tracks, people and             popularity means that the majority of the users think
academic papers etc. Due to the simplicity,               this item can be described by the tag. Thus, the
effectiveness and being independent of the contents of    popular tags reflect the common viewpoint of users or
items, social tags have been used in various web          the "wisdom of crowds" [6] in the classification or
applications including social web page bookmarking        descriptions of this item. Therefore, we argue that the
site del.icio.us, academic paper sharing website          popular tags can be used to describe the topics of the
                                                          tagged items. For each user, the original tags and the
                                                          collected items represent the user's personal viewpoint
                                                          of item classifications and collections. In a tag, a set of

Proceedings of the 14th Australasian Document             items are grouped together according to the user's
Computing                                                 viewpoint. The actual topics of the tag can be
Symposium, Sydney, Australia, 4 December 2009.            described by the frequent topics of the collected items.
Copyright for this article remains with the authors.




                                                         3

As we just mentioned above, the major topics of each       work      which    discusses  about   integrating     tag
item can be represented by its popular tags, thus the      information with content based recommender systems
popular tags of the collected items in a tag can be used   [11], extending the user-item matrix to user-item-tag
to represent that tag's actual topics. Since the user's    matrix     to   make    collaborative   filtering   item
personal viewpoint of the classifications of the           recommendation [12], combining uses' explicit rating
collected items are still kept while the original tag      with the predicted users' preferences for items based
terms are converted to popular tags that shared by         on their inferred preferences for tags [16] etc, more
many users, the user information sharing will be           advanced approaches of how to exploit tags to
improved.                                                  improve the performances of item recommendations
    In this paper, we propose to use popular tags to       are still in demand.
represent the topics of items, tags, and users' interests     More recently, the semantic meaning of social tags
to solve the problems of inaccurate user profiling and     has become one important research question. The
low information sharing caused by the free-style           research of Sen etc. [5] suggests that the factual tags
vocabularies of social tags. In Section 2, the related     are more likely to be reused by different users. The
work will be briefly reviewed. Then, the proposed          work of Suchanek etc. [15] shows that popular tags
collaborative filtering recommendation approach            are more semantically meaningful than unpopular
based on popular social tags will be discussed in          tags. And, the research of Bischoff etc. [4] shows that
details in Section 3. In this section, the definitions and not all tags are useful for searching and those tags
the selection of popular social tags will be discussed     related to the content information of items are more
firstly. Then, the approaches of representing items and    useful. These findings support this research. To solve
tags with popular social tags will be presented.           the    difficulties   caused   by   the     uncontrolled
Followed by the user profiling, neighborhood               vocabularies of social tags, some approaches have
formation,      and     recommendation         generation  been discussed to get the actual semantics of tags such
approaches, the experimental results and evaluations       as combining the content keywords with tags [10],
will be discussed in Section 4. Finally, the conclusions   using dictionaries to annotate tags [6], and
will be given in Section 5.                                contextualizing tags [17] etc. Different from these
                                                           approaches, this paper proposes to use popular tags
                                                           generated from the collected items to represent the
2 Related Work
                                                           semantic meanings of tags.
Recommender systems have been an active research
area for more than a decade, and many different
                                                           3 The Proposed Approach
techniques and systems with distinct strength have
been developed. Recommender systems can be                 3.1 Definitions
broadly classified into three categories: content-based
                                                           To describe the proposed approach, we define some
recommender systems, collaborative filtering or social
                                                           key concepts and entities used in this paper as below.
filtering based recommender systems and hybrid
                                                           In this paper, tags and social tags are interchangeably
recommender systems [7]. Because of the advantages
                                                           used.
of using similar users' recommendation and
independent with the contents of items, the                x  Users:  = {1,2,...,} contains all users in an
collaborative filtering based recommender systems             online community who have used tags to organize

have been widely used. Typically, users' explicit             items.

numeric ratings towards items are used to represent        x  Items      or   (Products,    Resources):         =
users' interests and preferences to find similar users or     {1,2,...,} contains all items tagged by users
similar content items to make recommendations.                in U. Items could be any type of online
However, because users' explicit rating information is        information resources or products in an online
not always available, the recommendation techniques           community such as web pages, videos, music
based on user's implicit ratings have drawn more and          tracks, photos, academic papers, books etc. Each
more attention recently.                                      item p can be described by a set of tags contributed
    Besides the web log analysis of users' usage              by different users.
information such as click stream, browse history and       x  Topics: contain items' content related information
purchase record etc., users' textural information such        such as content topics, genres, locations, attributes.
as tags, blogs, reviews in web 2.0 becomes an                 For example, "globalization" is a topic that
important implicit rating information source to profile       describes items' content information, "comedy" is
users'    interests   and     preferences      to   make      a topic that describes items' genre information, and
recommendations [10]. Currently, the researches about         "Shakespeare" is a topic that describes the attribute
tags in recommender systems are mainly focused on             of author information.
how to recommend tags to users such as using the co-       x  Social Tags:  = {1,2,...,
} contains all tags
occurrence of tags [2] and association rules [10] etc.        used by the users in U.
Not so much work has been done on the item                 x  Popular social tags:  = 1,2,..., contains a
recommendation. Although there are some recent                set of popular social tags. Popular social tags are




                                                          4

   tags that are used by at least  users, where  is a     to describe it. Therefore, the popular social tag set 
   threshold. The selection of popular social tags is     also can be denoted as:
   discussed in the followed Section.                           = {|()  ,  ,   > 0},  .
3.2 The Selection of Popular Social Tags                  3.3 Item and Tag Representations

Through tagging, the users, items and tags form a         The selected popular tags are used to represent items'
three dimensional relationship [12]. Based on tags,       major topics and the actual topics of each user's tags.
items are aggregated together if they are collected
                                                          Item Representation
under the same tag by different users and also users
are grouped together if they have used the same tag.      Traditionally, the item classifications or descriptions
Usually, the global popularity of a tag can be            are given by experts using a set of standard and
measured by the number of users that have used this       controlled vocabulary as well as a hierarchical
tag.                                                      structure representing the semantic relationships
   Let () be the set of users who have used the tag       among the topics to describe the topics of the items
, , be the set of users who have used  for                such as item taxonomy and ontology. In web 2.0,
the item  , () = {(,)|()} , where                         harnessing the collaborative work of thousands or
() is the set of items collected under tag  and           millions of web users, the aggregated tags contributed
()   .         The global popularity of  can be           by different users form the item classifications or
measured by |()| which is the number of users that        descriptions from the viewpoint of users or
have used tag , and the local popularity of  for the      folksonomy [13]. For each item , the set of tags used
item  can be measured by |,|. If we choose                by users to tag , denoted as (), and the number
popular tags only based on the global popularity, some    of users for each tag in () form the item
important tags that have high local popularities but      description of item , which is defined as below.
relatively low global popularities (i.e., the tags that   Definition 1 (Item Description): Let  be an
only have one kind of meaning and are used by a           item, the item description of  is defined as the set of
small number of users for tagging some particular
items) will be missed out. Moreover, because a tag        social tags for  and their numbers of being used to
                                                          tag the item
can have multiple meanings and users may have                                 , which is denoted as () =
different understandings to the tags, some tags will      !",,#|  (),, > 0$ ,                                where
have high global popularities but low local               , is the number of users that use the tag  to
popularities such as subjective tags (i.e., "funny"). But
because of the high global popularity, those tags will    tag the item  and , = |,|.
be incorrectly selected.                                     An example of item description is shown in Figure
  To select those popular tags that can well represent    1. The book "The World is Flat" is described by 10
the item topics, we define the global popularity of a     tags such as "globalization", "economics", "business"
tag based on its maximum local popularity. Let ()         etc. and their user numbers.
be the global popularity of the tag  , () =
max(){|,|}. Thus, let  be a threshold,                                            globalization (57) economics

any tag  with () >  will be selected as a popular         The World is Flat       (34) business (22) technology
                                                                                  (22) history (20) 0312 (1)
social tag.
                                                                                  naive analysis (1) ltp(1)
   Theoretically, the threshold  can be any positive
                                                                                  statistics(1) trade(1)...
numbers. However, since () is the maximum local
popularity of  for its collected items, if  is too large,   Figure 1: An example of item description formed by
the number of popular tags will be small, and there
                                                                                  social tags.
might be some items which are not tagged by any of
those selected popular tags. On the other hand, each         Different     from    the    item    descriptions  or
item collects a set of tags that have been used by        classifications    provided   by    experts,   the  item
different users to tag this item. Let () be the           descriptions formed by social tags contain a lot of
collected tag set of  , max {,} is the                    noise, which brings challenges for the organizing,
                             ()                           sharing and retrieval of items. However, an advantage
maximum local popularity of the tags in () for            provided by the item descriptions formed by social
item  . Apparently, if  > max {,}, then                   tags is that the item description () records the user
                              ()
all the tags of item  will be excluded which will         number of each tag for  or the local popularity of
                                                          each tag for . This feature can be used to find the
result in no popular tags to describe the topics of
 .To avoid this situation, we define an upper             major topics of items and filter out the noise. For
                                                          example, in Figure 1, we can see that 57 users use the
boundary       for     the     threshold       .      Let
                                                          tag "globalization" to classify the book "The World is
 =  { {,}}. If   , then each
                ()                                        Flat", which is the most frequently used tag to tag this

item can be guaranteed to have at least one popular tag   book, and the term "globalization" is indeed the actual




                                                         5

major topic of this book. Moreover, the tag "0312"         items collected in the tag t by the user u. Since the
only has one user, and it doesn't reveal any               number of items in different tags may be different, we
information in terms of the topics of the book.            normalize 5(,,) with the number of items in the
Removing the unpopular tags such as "0312" won't           tag t of u. Let (,) denote the set of items that are
reduce the coverage of the remaining tags to represent     collected or classified to the tag t by user u, then the
the topics of the book but the noise. Therefore, we        weight of cx can be calculated as below:
propose to use the selected popular tags to represent      5(,,) =         1   (,)' ,, where
the items.                                                               |(,)|

Definition 2 (Item Representation) Let  be an              ', is the frequency of  for the item  in the

item,  = 1,2,..., be the set of popular tags, the      tag t, as shown in Definition 2, ', =

representation of  is defined as a set of popular          (,)/+ +,.
                                                           Apparently, the tag representation &(,) is
social tags along with their frequencies as described
                                                           generated based on the items collected in the tag t by
below:
%& = , '(,   , '(, > 0,                               the user u. That means, &(,) still reflects the
                                                           personal viewpoint of the user u about the item
'(, ) = (,)/+ +,, where                                classifications or collections. Thus, each user's
'(,) is the frequency of  for , '(, )                   viewpoint of classifying his/her items is still kept
[0,1] and  ', = 1.                                       while a set of popular tags are obtained to represent
                                                           each tag term's semantic meaning. For different users,
   The frequency '(,) represents the degree of            the representations for the same tag can be different.
item  belonging to  . For a given set of popular          On the other hand, for different users, the
tags  with size q, i.e.,|| = ,the topics of each item      representations for different tags can be the same or
   can be represented by a vector                    333
                                                     - =   similar. Even though the tag terms are freely chosen
-,1,-,2,...,-,,...,-,|| , where -, = '(,) .               by individual users, by representing each tag using a
Thus, for each item  , its topic representation            set of popular tags, all tags become comparable since
                                                           all of them are represented using the same set of terms
becomes:
                                                           (i.e.,  popular    tags).   With     the   popular  tag
- = -,1,-,2,...,-,,...,-,||
333
                                                           representation, those unpopular tags that often cause
                                                           confusions and noises become understandable by
Tag Representation                                         other users according to the understanding to their
                                                           corresponding popular tag representation. For those
As mentioned in Introduction, since the unrestricted
nature of tagging, social tags contain a lot of noise and  popular tags, their tag representations reveal other

suffer some problems such as semantic ambiguity and        related popular tags, very often, these popular tags
                                                           themselves    have    high    weight     in  their  tag
a lot of synonyms etc., which brings challenges to
                                                           representation. Since each tag is represented by a set
make use of social tags to profile users' interests
                                                           of popular tags which provides the ground for
accurately.
   Although not all tags are meaningful to other users     comparison, this approach can help to solve the

or can be used to represent the topics, for each user,     problems caused by the free style vocabulary of tags

his/her own tags and items collected with those tags       such as tag synonyms which means some different
                                                           tags have the same meaning, semantic ambiguity of
reflect that user's personal viewpoint of classification
                                                           tags which means one tag has different meanings for
of the collected items. Thus, each tag used by a user is
                                                           different users, and spelling variations etc.
useful for profiling that user no matter how popular
this tag is. In a tag, a set of items are grouped together
according to a user's viewpoint, therefore, the frequent
                                                           3.3 User Profile Generation
topics of these items can be used to represent the
actual topics of the tag. Since the major topics of each   User profile is used to describe user's interests and
item can be represented by its popular tags, the           preferences information. Usually, a user-item rating
frequent popular tags of the collected items in a tag      matrix is used in collaborative filtering based
can be used to represent that tag's actual covered or      recommender systems to profile users' interests,
related topics.                                            which are used to find similar users through
Definition 3 (Tag Representation): Let  be a tag           calculating the similarity of item ratings or the
                                                           overlaps of item sets [14]. With the tag information,
used by user  ,  = 1,2,..., be the set of
                                                           users can be described with the matrix (user, (tag,
popular tags, the representation of  is defined as a set   item)), where (tag, item) is a sub matrix representing
of weighted popular social tags as described below:
                                                           the relationship between the tag set and item set of
&(,) = {(, 5(,,))|   , 5(,,)) >                        each user. Binary values "1" and "0" are used to
0}, where 5(,,) is the weight of , 5(,,)                specify whether a tag or an item has been used or
[0,1],  5(,,) = 1.                                       tagged by a user or not. Through calculating the
   The weight of  or 5(,,) can be measured               overlaps of tags and items or each user's sub
through calculating the total frequency of  for all the   relationship of tags and items, neighborhood can be




                                                          6

formed to do collaborative filtering to recommend          discussed before, a tag can be represented with a set of
items to a target user [12][3].                            popular social tags derived from the collected items
    As mentioned before, the free-style vocabulary of      with that tag. We can calculate the score of user  to
tags causes a lot of noise in tags which resulted in       topic  in each tag ,9 denoted as ,9 for the user
inaccurate user profiles and incorrect neighbors.          , shown as below:
Moreover, because of the long tails of items and tags,     7,,9 = 7,9  5 ,9,,9,, = 1..,9 =
the size of the matrix is very big and the overlaps of     1..                                    (2)
commonly used tags and tagged items are very low,             The user's interest score to the topic , 7(,),
which makes it difficult to find similar users through
                                                           is calculated by summing up the user's interests to the
calculating the overlaps of tags and items. To solve
                                                           topic in all his tags:
these problems, we propose to profile users' interests                 7(,) =  7,,9                     (3)
to topics by using a set of popular tags and convert the                              9=1
                                                              With Equation 3, users' interest distributions over
binary matrix (user, (tag, item)) into a much smaller
                                                           their own original tags are converted to users' interest
sized user-topics matrix. The popular tags will be used
                                                           distributions over the topics of items that are
to represent each user's interested topics and numeric
                                                           represented by the popular tags. Using this user
scores will be used to represent how much the user are
                                                           profiling approach, the noise of social tags can be
interested in these topics.
                                                           greatly removed while each user's personal viewpoint
Definition 4 (User Profile): Let  be a user,  =            of classifications or collections will still remain.
1,2,..., be the set of popular tags, the user          Moreover, since the size of the converted matrix is
profile of  is defined as a |C|-sized vector with          much smaller than the size of the matrix (user, (tag,
scores reflecting user's interests to the popular tags,    item)), the information sharing among different users
which               is             donated              as can be improved as well.
6333 = 6,1,6,2,...,6,,...,6,|| =
"7(,1),7(,2),...,7(,),...,7,#.                     3.4 Neighborhood Formation

7(,) is the score to 6, that represents the              Neighborhood formation is to generate a set of like-
degree of 's interests to the popular tag .               minded peers for a target user. Forming a

    A matrix 6 with size || × ||, can be used to           neighborhood for a target user ui U with standard
                                                           "best-K-neighbors" technique involves computing the
represent the user profiles for all users in . Each row
333 in the matrix 6 represents the user profile of user .6 distances between uiand all other users and selecting
                                                           the top K neighbors with shortest distances to ui .
In order to facilitate the similarity measure of any two
                                                           Based on user profiles, the similarity of users can be
users, user-wise normalization is applied. We suppose
                                                           calculated through various proximity measures.
each   has the same total interest score N and             Pearson correlation and cosine similarity are widely
 7(,) = 8, where N is the normalization                 used to calculate the similarity based on numeric
factor, which can be any positive number. Thus,            values.
7(,)  [0,8].                                                Based on the user profiles discussed above, for any
    To calculate each user's topic interest degree         two users  and  with profile 6 and 6, the Pearson
7, , firstly, we calculate the user's interest           correlation is used to calculate the similarity, which is
distribution for his/her own original tags. Let  =         defined as below:
,1,,9,...,,            be     the     tag      set     of    7 ,
, ,1,,9,...,,, 7(,9) be the score to measure                         q vi,y - vi   vj,y - vj
how much  is interested in ,9, then the score vector          =        y=1                                   (4)
(7(,1),7,9,...,7(,)) will represent 's interest                 q (vi,y - vi)2  q (vj,y - vj)2)
                                                                  y=1                y=1

distribution over his/her own tags,  7,9 = 8.                 Using the similarity measure approach, we can
                                       9=1                 generate the neighborhood of the target user , which
    A common sense is that, if a user is more interested
                                                           includes K nearest neighbour users who have similar
in a tag or topic, usually the user may collect more       topic interests with . The neighbourhood of , is
items under that tag or about that topic. That means,
                                                           denoted as:
the number of items in a tag is an important indicator
                                                                 () = {| ?7,,
about how much the user is interested in the tag. Let
| ,9,| denote the number of items in the tag ,9            where maxK {} is to get the top K values.
used by user , we use the proportion of | ,9,|
to the total number of items in all tags of  to            3.5 Recommendation Generation
measure the user's interest degree to the tag ,9. Thus,    For each target user , a set of candidate items will be
7,9 can be calculated as shown as follows:                generated     from    the  items    tagged    by  's
                 7,9 = 8          | ,9,|           (1)     neighbourhood formed based on the similarity of
                                9=1 | ,9,|                 users, which is denoted as () , () =
    By using Equation 1, we can obtain the user-tag
matrix that describes tag interests of all the users. As   {9|9, (),9  ()}, where 




                                                          7

is the item set of user  . With the typical              books and corresponding tags information as well. For
collaborative filtering approach, those items that have  each user in the test dataset, the top N items will be
been collected by the nearest neighbors will be          recommended to the user. If any item in the
recommended to the target user.                          recommendation list is in the target user's testing set,
   As discussed in Section 3.2, the aggregated social    then the item is counted as a hit.
tags describe the content information of items and the
topics of each item can be represented by popular        4.2 Parameterization
social tags. Thus, we propose to combine the content
                                                         The global popularities of tags are shown in Figure 2.
information of items formed by popular social tags
                                                         We can see that the user number of tags follows the
with the typical collaborative filtering approach to
                                                         power law distribution, which means that a small
generate recommendations. Those items that not only
                                                         number of tags are used by a large number of users
have been collected by the nearest neighbors but also
                                                         while a large number of tags are only used by a small
have the most similar topics to the target user's
                                                         number of users. Among 37120 tags, there are about
interests will be recommended to the target user,
                                                         67% tags (i.e., 25006 tags) which are only used by one
which     makes     the    proposed    recommendation
                                                         user.
generation approach actually get the benefits of the
content based recommendation approaches [8].
   For each candidate item 9(), let (,9) be
the set of users in () who have tagged the item 9,
the prediction score of how much  may be interested
in 9 is calculated in terms of the aspects of how
similar those users who have the item 9 and how
similar the item's topics with 's topic interest.
   With Equation 4, the similarity of two users can be
measured. Similarly, the Pearson correlation is used to
calculate the similarity of the topic interests of user
 and the topics of the candidate item 9, which is               Figure 2: The distribution of social tags.
denoted as below:

7( ,9) =             +=16,+-6( -9,+--9)                      After calculating the local popularity of each tag
                                              (5)
                  +=1(6,+- 6)2 +=1(-9,+--9)2)            for each item, we get =2. Thus, we set =2. To
   Thus, the prediction score denoted as A(,9) can       evaluate the effectiveness of the selected popular tag
                                                         set, we compared the top 5 precision and recall results
be calculated with Equation 6.
                                                         of the threshold =2 with the results of  =1,  =3, 
A(,9) =      7 ( ,9)        (,9) 7 ,
                                              (6)        =4, and  =5. With threshold  =1, 37120 tags are
                         |(,9)|                          selected, which is the whole tag set. Thus, each item
 The top N items with larger prediction scores will be   was represented with all the tags. Different from the
recommended to the target user .                         Topic-Tag approach, each tag was represented with
                                                         the selected tags. With threshold  =2, 12214 tags are
                                                         selected. When threshold  =3, 7428 tags were
4 Experiments and Evaluations                            selected and there were 1188 books that have no
                                                         selected tags describes them. With threshold  =4,
4.1 Experiment setup                                     5297 tags were selected and there were 1668 books
                                                         that have no selected tags describes them. With
We conducted the experiments using the dataset
obtained from Amazon.com. The dataset was crawled        threshold  =5, 4104 tags were selected and there
                                                         were 2452 books that have no selected tags describes
from amazon.com on April, 2008. The items of the
                                                         them. The top 5 precision and recall results with
dataset are books. To avoid too sparse, in pre-
                                                         different threshold are shown in Figure 3.
processing, we removed the books that are only
tagged by one user. The final dataset comprises 5177
users, 37120 tags, 31724 books and 242496 records.
   The precision and recall are used to evaluate the
recommendation performance. The whole dataset is
split into a training dataset and a test dataset with 5-
folded and the split percentage is 80% for the training
dataset and 20% for the test dataset, respectively.
Because our purpose is to recommend books to users,
the test dataset only contain users' books information.
Each record in the test dataset consists of the books
that are tagged by one user. The training dataset,        Figure 3. The top 5 precision and recall evaluation
which is used to build user profiles, contains users'          results with different threshold  values.




                                                        8

   From the results of Figure 3, we can see the results
of  =2 was better than other values. Thus, the popular
tags can be used to represent the topics of items and
tags. And, since some books may don't have any
selected tags describing their topics when the
threshold is too high, the results are worse.


4.3 Comparison

To evaluate the effectiveness of the proposed
approach, we compared the precision and recall of the
recommended top N items produced by the following
approaches:                                                        Figure 5: Recall evaluation results.
x  Topic-PopularTag approach. This is the proposed
   approach that uses the popular tag to represent
                                                         4.4 Discussions
   items' topics, tags' actual topics and users' topic
   interests.                                            From the experimental results, we can see that the
x  Topic-Tag approach. This approach uses users'         proposed     approach      outperformed     the    other
   interest distribution to their original tags to make  approaches, which means the proposed collaborative
   recommendation.         Different    from      Topic- filtering approach based on popular social tags is
   PopularTag approach, this approach only uses the      effective. Since the dataset is very sparse (i.e., the
   users' original tags to profile users and doesn't     average number of items that each user has is about
   include the tag representations.                      12.6), the overall precision and recall values are low.
x  Singular Value Decomposition (SVD). This is a         The approach Topic-Tag approach performed the
   wildly used approach to reduce the dimensions of      worst, which means that although tags implies users'
   a matrix and reduce noise. In this paper, the         interests and preferences information, since the social
   standard SVD based recommendation approach [8]        tags contains a lot of noise, it's inaccurate to profile
   was implemented based on the user-tag matrix.         users with their original tags directly. The comparison
x  Tso-Sutter's approach. This approach is proposed      between the approaches of Tso-Sutter and Liang and
   by Tso-Sutter that uses two derived binary            the Standard CF approach shows that social tags are
   matrixes      user-item,     user-tag     to    make  helpful to improve the user profiling accuracy when
   recommendation [9], which is an extended              the social tags are used together with the users'
   standard collaborative filtering approach.            collected items. Moreover, the comparison between
x  Liang's approach. This approach is proposed by        the proposed Topic-PopularTag approach and the
   Liang that uses three derived binary matrixes user-   SVD approach suggests that the proposed approach
   item, user-tag to tag-item sub matrix to make         performs better than the traditional dimension
   recommendation [12], which is an extended             reduction approach. The proposed approach not only
   standard collaborative filtering approach.            reduce the dimension through using a much smaller
x  Standard CF approach. This is the standard            sized user-topic matrix to profile users but also
   collaborative filtering (CF) approach [14] that uses  significantly improves the accuracy of user profiling
   the implicit item ratings or the binary matrix user-  and information sharing through representing the
   item only. This is the baseline approach.             personal or unpopular tags with a set of popular tags.
We compared the proposed approach that has the
threshold  =2 with other state of art approaches, the    5. Conclusions
precision and recall results are shown in Figure 4 and
                                                         In this paper, we propose a collaborative filtering
Figure 5.
                                                         approach that combines each user's personal viewpoint
                                                         of the classifications of items and the common
                                                         viewpoint of many users about the classifications of
                                                         items to make personalized item recommendation. The
                                                         popular tags are used to represent items' major topics,
                                                         tags' actual covered or related topics and users' topic
                                                         interests. Moreover, a user profiling approach that
                                                         converts users' interest distribution for their own
                                                         original tags to users' interest distribution for topics
                                                         that are represented with the popular tags are proposed
                                                         to improve user profiling accuracy and information
                                                         sharing. Also, we propose a recommendation
                                                         generation approach that incorporates the item content
      Figure 4: Precision evaluation results.


                                                        9

information formed by the collaborative working of         [7] Burke, R., "Hybrid Recommender Systems: Survey
tagging to generate recommended items that are not         and Experiments", User Modeling and User-Adapted
only have been collected by most similar users but         Interaction, 12(2002), pp. 331-370.
also have the most similar topics with the target user's   [8] Sarwar, B. M., Karypis, G., Konstan, J. A., and Riedl,
interests.                                                 J. "Application of Dimensionality Reduction in
   The experiments show that the proposed approach         Recommender System ü A Case Study." In Proc.of
outperforms other approaches. Since the social tags        WebKDD'00, 2000.
can be used to describe any types of items or
                                                           [9] K.H.L. Tso-Sutter, L.B. Marinho and L.Schmidt-
resources, this research can be used to recommend
                                                           Thieme, "Tag-aware Recommender Systems by Fusion
various kinds of items to users, which provides
                                                           of Collaborative Filtering Algorithms", In Proc. Applied
possible solutions to the recommendation of those
                                                           Computing'08, 2008, pp.1995-1999.
items that the traditional collaborative filtering
                                                           [10] Heymann, P., Ramage, D., and Garcia-Molina, H.,
approaches or content based approaches fail to work
                                                           "Social tag prediction", In Proc. of SIGIR'08, 2008, pp.
well such as people. Moreover, this research made a
                                                           531­538.
contribution to      the improvement of information
sharing, organization and retrieval of online tagging      [11] Gemmis, M. de, Lops, P., Semeraro, G., and Basile,

systems as well as the improvement of the                  P.,    "Integrating tags in a semantic content-based
                                                           recommender", In Proc. of the 2008 ACM conference on
recommendation        performances      of    traditional
                                                           Recommender systems, 2008, pp. 163-170.
recommender systems (i.e., in e-commerce websites)
through incorporating this new type of user                [12] Liang, H., Xu, Y.,        Li, Y., and Nayak, R.,
information in web 2.0.                                    "Collaborative Filtering Recommender Systems Using
                                                           Tag Information", In Proc. of The 2008 IEEE/WIC/ACM
                                                           International Conference on Web Intelligence (WI-08)
References
                                                           Workshops, 2008, pp. 59-62.

[1] Bao, S., Wu, X., Fei, B., Xue, G., Su, Z. and Yu, Y.,  [13] Al-Khalifa, H.S. and Davis, H. C., "Exploring the
"Optimizing Web Search Using Social Annotations", In       Value of Folksonomies for Creating Semantic Metadata",
Proc. of WWW'07, 2007, pp. 501-510.                        International Journal on Semantic Web and Information
                                                           Systems, 3,1 (2007), pp. 13-39.
[2] Li, X., Guo, L., and Zhao, Y. E., "Tag-based social
interest discovery", In Proc. of WWW'08, 2008, pp. 675-    [14] Shardanand, U. and Maes,P., "Social Information
684.                                                       Filtering: Algorithms for Automating `Word of Mouth'",
                                                           In Proc. of SIGCHI, 1995, pp. 210 -217.
[3] Tso-Sutter, K.H.L., Marinho, L.B. and Schmidt-
Thieme, L., "Tag-aware Recommender Systems by              [15] Suchanek, F. M., Vojnovi´c, M., Gunawardena D.,
Fusion of Collaborative Filtering Algorithms", In Proc.    "Social tags: Meaning and Suggestions", In Proc.of
of Applied Computing, 2008, pp. 1995-1999.                 CIKM'08, 2008, pp. 223-232

[4] Bischoff, K., Firan, C. S., Nejdl, W., Paiu, R., "Can  [16] Sen, S., Vig, J., Riedl, J., "Tagommenders:
All Tags be Used for Search?", In Proc. of CIKM'08,        Connecting Users to Items through Tags", In Proc. of
2008, pp. 193-202.                                         WWW'09, 2009, pp. 671-680

[5] Sen, S., S. Lam, A. Rashid, D. Cosley, D.              [17] Au Yeung, C. M., Gibbins, N. and Shadbolt, N.,
Frankowski, J.Osterhouse, M. Harper, and J. Riedl.,        "Contextualizing    Tags    in  Collaborative    Tagging
"Tagging, communities, vocabulary, evolution", In Proc.    Systems", In Proc. of the 20th ACM Conference on
of CSCW '06, 2006, pp. 181-190.                            Hypertext         and         Hypermedia,           2009.

[6] What Is Web 2.0.
http://www.oreillynet.com/pub/a/oreilly/tim/news/2005/0
9/30/what-is-web-20.html




                                                         10

                                External Evaluation of Topic Models

                          David Newman           Sarvnaz Karimi         Lawrence Cavedon

                                   NICTA and The University of Melbourne
                                        Parkville, Victoria 3010, Australia

                              {david.newman, sarvnaz.karimi, lawrence.cavedon}@nicta.com.au



Abstract Topic models can learn topics that are                problem of automatic assignment of a short label for a
highly interpretable, semantically-coherent and can            topic, and Griffiths and Steyvers [2006] who applied
be used similarly to subject headings. But sometimes           topic models to word sense distinction tasks. Wallach
learned topics are lists of words that do not convey           et al. [2009] proposed methods for evaluating topic
much useful information.        We propose models that         models, but they focused on the statistics of the model,
score the usefulness of topics, including a model              not the meaning of individual topics.
that computes a score based on pointwise mutual                   The challenge of helping a user understand a dis-
information (PMI) of pairs of words in a topic. Our            covered topic is exacerbated by the variable semantic
PMI score, computed using word-pair co-occurrence              quality of topics produced by a topic model. Certain
statistics from external data sources, has relatively          types of document collections, for example collections
good agreement with human scoring. We also show                of abstracts of research papers, produce mostly high-
that the ability to identify less useful topics can improve    quality interpretable topics which have clear semantic
the results of a topic-based document similarity metric.       meaning. However, the broader class of document col-
                                                               lections -- for example emails, blogs, news articles and
Keywords Topic Modeling, Evaluation, Document
                                                               books -- tend to produce a wider mix of topics. The
Similarity, Natural Language Processing, Information
                                                               novelty of our work is targetting this challenge by fo-
Retrieval
                                                               cusing on evaluation of topics using their degree of use-
                                                               fulness to humans.
1 Introduction
                                                                  In this work we first ask humans to decide whether
Topic models are unsupervised probabilistic models for         individual learned topics are useful or not (we define
document collections, and are generally regarded as the        what is meant by useful). We then propose models
state-of-the-art for extracting course-grained semantic        that use external text data sources, such as Wikipedia
information from collections of text documents. The            or Google hits, to predict human judgements. Finally,
extracted semantic content is useful for a variety of          we show how an assessment of useful and useless topics
applications including automatic categorization and            can improve the outcome of a document similarity task.
faceted browsing. The topic model technique learns a
set of thematic topics from words that tend to co-occur        2 Topic Modeling
in documents. The technique assigns a small number
                                                               The topic model -- also known as latent Dirichlet
of topics to each document, and those topics can then
                                                               allocation or discrete principal component analysis
be used to explain and retrieve documents. However
                                                               (PCA) -- is a Bayesian graphical model for text
this explanation of a document is only useful if we can
                                                               document collections represented by bags-of-words
understand what is meant by a given topic.
                                                               (see Blei et al. [2003], Griffiths and Steyvers [2004],
    Since the introduction of the original topic model
                                                               Buntine and Jakulin [2004]). In a topic model, each
approach [Blei et al., 2003, Griffiths and Steyvers,
                                                               document in the collection of D documents is modeled
2004], many researchers have modified and extended
                                                               as a multinomial distribution over T topics, where
topic modeling in a variety of ways. However, there
                                                               each topic is a multinomial distribution over W words.
has been less effort on understanding the semantic
                                                               Typically, only a small number of words are important
nature of topics learned by topic models. While the
                                                               (have high likelihood) in each topic, and only a small
list of the most likely (i.e. important) words in a topic
                                                               number of topics are present in each document.
provides good transparency to defining a topic, how
                                                                  The collapsed Gibbs [Geman and Geman, 1984]
can humans best interpret and understand the gist of
                                                               sampled topic model simultaneously learns the topics
a topic? Some researchers have started to address this
                                                               and the mixture of topics in documents by iteratively
problem, including Mei et al. [2007] who looked at the
                                                               sampling the topic assignment z to every word in every
Proceedings of the 14th Australasian Document Comput-          document, using the Gibbs sampling update
ing Symposium, Sydney, Australia, 4 December 2009.
Copyright for this article remains with the authors.




                                                           11

                     p(zid = t|xid = w, z¬id)                      Collections Modeled

               Nwt + 
                  ¬id                                              We used two documentcollections: a collectionof news
                   ¬id            Ntd   ¬id + 
                                       N ¬id + T    ,
                                                                   articles, and a collection of books. These collections
              w Nwt + W               t  td
                                                                   were chosen to produce sets of topics that have more
where zid = t is the assignment of the ith word in doc-
                                                                   variable quality than one typically observes when topic
ument d to topic t, xid = w indicates that the current
                                                                   modeling collections of scientific literature. A collec-
observed word is w, and z¬id is the vector of all topic            tion of D = 55,000 news articles was selected from
assignments not including the current word. Nwt repre-
                                                                   Linguistic Data Corporation's gigaword corpus, and a
sents integer count arrays (with the subscripts denoting           collection of D = 12,000 books was downloaded from
what is counted), and  and  are Dirichlet priors.
                                                                   the Internet Archive. We refer to these collections as
    The maximum a posterior (MAP) estimates of the
                                                                   "News Articles" and "Books" throughoutthe remainder
topics p(w|t), t = 1...T and the mixture of topics in
                                                                   of this paper.
documents p(t|d), d = 1...D are given by
                                                                       Standard procedures were used to create the bags-
                                                                   of-words for the two collections. After tokenization,
                p(w|t) =         Nwt + 
                                               ,                   and removing stopwords and words that occurred fewer

                               w  Nwt + W                          than ten times, we learned topic models of News Arti-
                   p(t|d) = Ntd        +                           cles using T = 50 (T50) and T = 200 (T200) topics,
                                                                   and a topic model of Books using T = 400 (T400)
                                  tNtd + T     .

                                                                   topics. For each topic model, we printed the set of
Pathology of Learned Topics
                                                                   T topics. We define a topic as the list of ten most
Despite referring to the distributions p(w|t) as topics,           probable words in the topic. This cutoff at ten words is
suggesting that they have sensible semantic meaning,               arbitrary, but it balances between having enough words
they are in fact just statistics that explain count data ac-       to convey the meaning of a topic, but not too many
cording to the underlyinggenerative model. To be more              words to complicate human judgements or our scoring
explicit, while many learned topics convey information             models.
similar to what is conveyed by a subject heading, topics
themselves are not subject headings, and they some-                3 Human Scoring of Topics
times are not at all related to a subject heading.
                                                                   We selected 117 topics from News Articles, including
    Since ourfocus in this paperis studyingandevaluat-
ing the wide range of topics learnedby topic models, we            all 50 topics from the T50 topic model, and 67 selected

present examples of less useful topics learned by topic            topics from the T200 topic model. We selected 120

models. Note that these topics are not simply artifacts            topics fromthe T400topic model of Books. To increase
                                                                   the expected number of useful and useless topics, we
from one particular model started from some particular
                                                                   pre-scored topics using our scoring models (described
random initialization ­ they are stable features present
                                                                   later) to select a mix of useful, useless, and in-between
in the data that can be repeatedly learned from different
                                                                   topics to make up the sample. We asked nine human
models, hyperparameter settings and random initializa-
                                                                   subjects to score each of the 237 topics on a 3-point
tions. The following list shows an illustrative selection
                                                                   scale where 3="useful" and 1="useless".
of less useful topics:
                                                                       We provided a rubric and some guidelines on how
   · north south carolina korea korean southern kim daewoo
                                                                   to judge whether a topic was useful or useless. In addi-
     government country million flag thoreau economic war
                                                                   tion to showing several examples of useful and useless
     ... This topic has associated Carolina with Korea via the
                                                                   topics, we gave the followinginstructions to people per-
     words north and south.
   · friend thought wanted went knew wasn't love asked guy         forming the evaluation:
                                                                       The topics learned by a topic model are usually
     took remember kid doing couldn't kind ... This is a typi-
     cal "prose" style topic often learned from collections of     sensible, meaningful, interpretable and coherent. But
     emails, stories or news articles.                             some topics learned (while statistically reasonable) are
   · google domain search public copyright helping query-          not particularly useful for human use. To evaluate our
     ing user automated file accessible publisher commercial       methods, we would like your judgment on how "useful"
     legal ... This is a topic of boilerplate copyright text that  some learned topics are. Here, we are purposefully
     occurred in a large subset of a corpus.                       vague about what is "useful" ... it is some combination
   · effect significant increase decrease significantly change     of coherent, meaningful, interpretable, words are
     resulted measured changes caused ... This is a topic of       related, subject-heading like, something you could
     comparisons that was learned from a large collection of       easily label, etc.
     MEDLINE abstracts.                                                Figure 1 shows selected useful and useless topics
   · weekend december monday scott wood going camp                 from News Articles, as scored by nine people. For
     richard bring miles think tent bike dec pretty ... This       our purposes, the usefulness of a topic can be thought
     topic includes a combination of several commonly
                                                                   of as whether one could imagine using the topic in a
     occurring pathologies including lists of names, days of
                                                                   search interface to retrieve documentsabout a particular
     week, and months of year.




                                                                 12

 Selected useful topics (unanimous score=3):                                                                                           News Articles (corrcoef=0.78)
 space earth moon science scientist light nasa mission planet mars ...
 health disease aids virus vaccine infection hiv cases infected asthma ...
                                                                                                                                     3
 bush campaign party candidate republican mccain political presidential ...
 stock market investor fund trading investment firm exchange companies ...
 health care insurance patient hospital medical cost medicare coverage ...                                                          2.5
                                                                                                  scores
 car ford vehicle model auto truck engine sport wheel motor ...
 cell human animal scientist research gene researcher brain university ...                                                           2
                                                                                                        other
 health drug patient medical doctor hospital care cancer treatment disease ...
                                                                                                             of
                                                                                                                                    1.5
 Selected useless topics (unanimous score=1):
                                                                                                               Mean
 king bond berry bill ray rate james treas byrd key ...
                                                                                                                                     1
 dog moment hand face love self eye turn young character ...
 art budget bos code exp attn review add client sent ...
                                                                                                                                        1   1.5     2     2.5   3
 max crowd hand flag sam white young looked black stood ...
                                                                                                                                             Score left out
 constitution color review coxnet page art photos available budget book ...
 category houston filed thompson hearst following bonfire mean tag appear ...
 johnson jones miller scott robinson george lawrence murphy mason ...                                                                    Books (corrcoef=0.81)

 brook stone steven hewlett packard edge borge nov buck given ...

                                                                                                                                     3

Figure 1: Selected useful and useless topics from                                                                                   2.5
                                                                                                                   scores
collection of News Articles. Each line represents one
                                                                                                                                     2
topic.                                                                                                                   other
                                                                                                                              of
                                                                                                                                    1.5

                                                                                                                                Mean
 Selected useful topics (unanimous score=3):                                                                                         1
 steam engine valve cylinder pressure piston boiler air pump pipe ...
 furniture chair table cabinet wood leg mahogany piece oak louis ...
                                                                                                                                        1   1.5     2     2.5   3
 building architecture plan churches design architect century erected ...
                                                                                                                                             Score left out
 cathedral church tower choir chapel window built gothic nave transept ...
 god worship religion sacred ancient image temple sun earth symbol ...
 loom cloth thread warp weaving machine wool cotton yarn mill ...
                                                                                   Figure 3: Inter-rater reliability, computed by leave-one-
 window nave aisle transept chapel tower arch pointed arches roof ...
 cases bladder disease aneurism tumour sac hernia artery ligature pain ...         out, showing high agreement between the nine humans.

 Selected useless topics (unanimous score=1):
 entire finally condition position considered result follow highest greatest ...
                                                                                   This inter-rater correlation is an upper bound on how
 aud lie bad pro hut pre able nature led want ...
 soon short longer carried rest turned raised filled turn allowed ...              well we can expect our scoring models to perform.
 act sense adv person ppr plant sax genus applied dis ...
 httle hke hfe hght able turn power lost bring eye ...
                                                                                   4 Scoring Model I: Pointwise Mutual In-
 soon gave returned replied told appeared arrived received return saw ...
 person occasion purpose respect answer short act sort receive rest ...
 want look going deal try bad tell sure feel remember ...                               formation

                                                                                   The intuition behind our first scoring model, pointwise
Figure 2: Selected useful and useless topics from                                  mutual information (PMI) using external data, comes
collection of Books.                                                               from the observation that occasionally a topic has some
                                                                                   odd-words-out in the list of ten words. This leads to
subject. An indicator of usefulness is the ease by which                           the idea of a scoring model based on word association
one could think of a short label to describe a topic (for                          between pairs of words, for all word pairs in a topic.
example "space exploration" could be a label for the                               But instead of using the collection itself to measure
first topic). The useless News Articles topics display                             word association (which could reinforce noise or un-
little coherence and relatedness, and one would not ex-                            usual word statistics), we use a large external text data
pect them to be useful as categories or facets in a search                         source to provide regularization.
interface.                                                                             Specifically, we measured co-occurrence of word
     We see similar results in Figure 2, which shows se-                           pairs from two huge external text datasets: all articles
lected useful and useless topics from the Books collec-                            from English Wikipedia, and the Google n-grams data
tion. Again, the useful topics could directly relate to                            set. For Wikipedia we counted a co-occurrence as
subject headings, and be used in a user interface for                              words wi and wj co-occurring in a 10-word window
browse-by-subject. Note that the useless topics from                               in any article, and for Google n-grams, we counted
both collections are not chance artifacts produced by                              a co-occurrence as wi and wj co-occurring in any of
the models, but are in fact stable and robust statistical                          the 5-grams. These co-occurrences are counted over
features in the data sets.                                                         corpora of 1B and 1T words respectively, so they
     Our human scoring of the 237 topics has high                                  produce reasonably reliable statistics.
inter-rater reliability, as shown in Figure 3.                           Each          We choose pointwise mutual information as the
human score has high agreement with the mean of                                    measure of word association, and define the following
the remaining scores (Pearson correlation coefficient                              scoring formula for a topic w:
     =     0.78 . . . 0.81).        In the following sections we
present models to predict these human judgements.
                                                                                   PMI-Score(w) = median{PMI(wi,wj),ij  1...10},




                                                                                 13

                                3.0                                                                                               News Articles (corrcoef=0.72)
                      Dance             Opera
                                                                                                                                 8


                             3.2     2.9
                   4.2                         1.4
                                                                                                                                 6

                                                                                  score
                          2.9          2.7
                                                                                                                                 4
                Music                          Band
                                 3.5

                                                                                       PMI-Wiki
                                                                                                                                 2
                        4.1                4.5

                                Rock
                                                                                                                                 0
                                                                                                                                   1   1.5     2     2.5   3
                                                                                                                                     Mean human score
Figure 4: Illustration of pointwise mutual information
                                                                                                                                    Books (corrcoef=0.73)
between word pairs.
                                                                                                                                 8



             PMI(wi,wj) = log           p(wi, wj)                                                                                6
                                      p(wi)p(wj) ,                                             score
                                                                                                                                 4
where the top-ten list of words in a topic is denoted
by w = (w1,...,w10), and we exclude the self PMI                                                    PMI-Wiki
                                                                                                                                 2
case of i = j. The PMI-Score for each topic is the
median PMI for all pairs of words in a topic (so for
                                                                                                                                 0
a topic defined by the top-10 words, the PMI-Score is                                                                              1   1.5     2     2.5   3
                                                                                                                                     Mean human score
the median of 55 PMIs). Note that if two words are
statistically independent, then their PMI is zero.
                                                                   Figure 5: Scatterplot of PMI-Wiki-Score vs. mean
    Our PMI-Score is illustrated in Figure 4 for a
                                                                   human score.
topic of five words: "music band rock dance opera".             1

Using co-occurrence frequencies from Wikipedia,
                                                                                                                                  News Articles (corrcoef=0.78)
we see unsurprising high-scoring word pairs, such
                                                                                                                                 8
as PMI(rock,band)=4.5, and PMI(dance,music)=4.2.
Some pairs exhibit greater independence, such as
                                                                                                                                 6
PMI(opera,band)=1.4. The PMI-Wiki-Score for this         2
                                                                                                            score

topic is the median of all the PMIs, or PMI-Wiki-
                                                                                                                                 4
Score=3.1.
    We see broad agreement between the PMI-Wiki-
                                                                                                                 PMI-Google 2
Score and the human scoring in Figure 5, which shows
a scatterplot for all 237 topics. The correlation between
                                                                                                                                 0
                                                                                                                                   1   1.5     2     2.5   3
the PMI-Wiki-Score and the mean human score is
                                                                                                                                     Mean human score
 = 0.72 for News Articles and  = 0.73 for Books
                                                                                                                                    Books (corrcoef=0.70)
(we define correlation  as the Pearson correlation
                                                                                                                                 8
coefficient). This correlation is relatively high given
that the inter-rater-correlation is only slightly higher at
                                                                                                                                 6
 = 0.78 . . . 0.81.                                                                                              score
    Using the Google 5-grams data instead of English
                                                                                                                                 4
Wikipedia for the external data source produces similar
results, shown in Figure 6. In this case, the pointwise
                                                                                                                      PMI-Google 2
mutual information values are computed using word
statistics from the 1 billion Google 5-grams instead of
                                                                                                                                 0
2 million Wikipedia articles. The correlations are in a                                                                            1   1.5     2     2.5   3
                                                                                                                                     Mean human score
similar range ( = 0.70...0.78) with a slightly higher
correlation of  = 0.78 for News Articles.
                                                                   Figure 6: Scatterplot of PMI-Google-Score vs. mean
    Why does our PMI-Score model agree so well with
                                                                   human score.
human scoring of topics? Our intuition is that humans
consider associations of pairs of words (or the associa-
                                                                   human process is somewhat approximated by the cal-
tion between one word and all the other words) to de-
                                                                   culation of the PMI-Score.
termine the relatedness and usefulness of a topic. This

   1We illustrate using 5 words instead of 10 for simplicity.      5 Scoring Model II: Google
   2This is the PMI-Score computed using frequency counts from
Wikipedia.                                                         In this section we present a second scoring scheme,
                                                                   again based on a large external data source: this time



                                                                 14

the entire World Wide Web crawled by Google. We                                                                                   News Articles (corrcoef=0.78)

present two scoring formulas that use the Google search                                                                        300

engine:
                                                                                                                               250

         Google-titles-match(w) = 1[wi = vj],                                                                                  200

                                                                                                                               150
where i = 1,...,10 and j = 1,...,|V |, and vj are
                                                                                                                               100
all the unique terms mentioned in the titles from the
                                                                                         Google-titles-match
top-100 search results, and 1 is the indicator function to                                                                      50

count matches; and                                                                                                               0
                                                                                                                                   1   1.5     2     2.5   3
                                                                                                                                     Mean human score
Google-log-hits(w) = log(# results from search for w),
                                                                                                                                    Books (corrcoef=0.52)
where w is the search string "+w1 +w2 +w3 ... +w10".                                                                           300

We use the Google advanced search option `+' to search
                                                                                                                               250
exactly as is and prevent Google from using synonyms.
                                                                                                                               200
Our intuition is that the mention of topic words in URL
titles -- or the prevalence of documents that mention                                                                          150

all ten words in the topic -- may better correlate with a                                                                      100

human notion of the usefulness of a topic.                                                                  Google-titles-match
                                                                                                                                50
    For example, issuing the query to Google: "+space
                                                                                                                                 0
+earth +moon +science +scientist +light +nasa                                                                                      1   1.5     2     2.5   3

+mission +planet +mars" returns 171,000 results (so                                                                                  Mean human score

Google-log-hits(w)=5.2), and the following list shows
                                                                          Figure 7: Scatterplot of Google-titles-match score vs.
the titles and URLs of the first 6 results:
                                                                          mean human score.
  1. NASA - STEREO Hunts for Remains of an Ancient
      Planet near Earth (science.nasa.gov/headlines/y2009/...)
                                                                          are not surprised that Google-titles-match fails to give
  2. NASA - Like Mars, Like Earth (www.nasa.gov/audience/
                                                                          this topic a high score. The second topic is mostly
      foreducators/k-4/features/...)
                                                                          about NASA and space exploration, but is polluted
  3. NASA - Like Mars, Like Earth (www.nasa.gov/audience/                 by the words "firefighter" and "worcester", which
      forstudents/5-8/features/...)                                       will severely limit the number of results returned. By
  4. ASP: The Silicon Valley Astronomy Lectures Podcasts                  using the median, the PMI-Score of this topic is less
      (www.astrosociety.org/education/podcast/index.html)                 sensitive to these words that don't fit the topic, but the

  5. NASA calls for ambitious outer solar system mission -                Google-titles-match has less hope of producinga useful
      space ... (www.newscientist.com/article/...)                        list of search results when all ten words are included in
                                                                          the search query. Topics from Books follow, and we
  6. NASA International Space Station Mission Shuttle
      Earth Science ... (spacestation-shuttle.blogspot.com/2009/08/...)   see a similar problem to the cooking topic from News
                                                                          Articles, where the words in the topic clearly convey
    The underlinedwords show mentions of topic words
                                                                          something semantically coherent, but fail to evoke
in the URL titles, with the first six titles giving a to-
                                                                          URL titles that mention those general terms.
tal of 17 mentions. The top-100 URL titles include a
                                                                              We see less promising results from our Google-log-
total of 194 matches, so for this topic Google-titles-
                                                                          hits score, which has relatively low correlation with the
match(w)=194.                                                             mean human scoring ( = -0.09...0.49), as shown
    We see surprisingly good agreement between the
                                                                          in the scatterplots in Figure 8. For this scoring for-
Google-titles-match score and the human scoring in
                                                                          mula we observed the reverseof the problemof Google-
Figure 7 for the News Articles (                   = 0.78), and a         titles-match, namely we saw overlyfavorablescoring of
lower level of agreement for Books ( = 0.52). In the                      many topics that received a low human score. Table 2
PMI-Scores there was no clear pattern of outliers in the
                                                                          shows selected topics having a low human score (not
scatterplots against the mean human score. However,
                                                                          useful), but a high Google-log-hits score. The topics in
we see a definite constraint of the Google-titles-match
                                                                          this table all exhibit the similar characteristic of all ten
score, where there are many topics that received a high
                                                                          words being relatively common words. Consequently
human score, but a low Google-titles-match score.
                                                                          there exist manyweb pages that contain these words (is-
Table 1 shows selected topics having a high human
                                                                          suing these topics as queries returned between 250,000
score (useful), but a low Google-titles-match score.
                                                                          and 10,000,000 results). This behavior of Google-log-
The first three topics listed (from News Articles) show
                                                                          hits and failure to agree with human scoring (in this
different types of problems. The first topic is clearly
                                                                          case) is relatively easy to understand.
about cooking, but does not mention the word cooking.
Furthermore, it is unlikely that URL titles would
include words such as "teaspoon" or "pepper", so we



                                                                        15

       Human       Titles-match     Topic
       2.6         8                cup add tablespoon salt pepper teaspoon oil heat sugar pan ...
       2.4         4                space nasa moon mission shuttle firefighter astronaut launch worcester rocket ...
       2.3         0                oct series braves game yankees league bba met championship red ...

       2.9         25               church altar churches stone chapel cathedral vestment service pulpit chancel ...
       3.0         6                cases bladder disease aneurism tumour sac hernia artery ligature pain ...
       2.8         23               art ancient statues statue marble phidias artist winckelmann pliny image ...
       3.0         3                window nave aisle transept chapel tower arch pointed arches roof ...
       2.9         18               crop land wheat corn cattle acre grain farmer manure plough ...
       2.8         32               account cost item profit balance statement sale credit shown loss ...
       2.9         20               pompeii herculaneum room naples painting inscription excavation marble bronze bath ...
       3.0         21               window nave choir arch tower churches aisle chapel transept capital ...
       3.0         31               drawing draw pencil pen drawn model cast sketches ink outline ...

             Table 1: Disagreement between high human scores and low Google-titles-match scores.


             Human        log hits    Topic
             1.0          5.4         dog moment hand face love self eye turn young character ...
             1.2          7.0         change mean different better result number example likely problem possible ...
             1.2          6.4         fact change important different example sense mean matter reason women ...
             1.1          5.9         friend thought wanted went knew wasn't love asked guy took ...
             1.1          5.6         thought feel doesn't guy asked wanted tell friend doing went ...
             1.1          6.1         bad doesn't maybe tell let guy mean isn't better ask ...

             1.0          6.7         entire finally condition position considered result follow highest greatest fact ...
             1.0          6.3         soon short longer carried rest turned raised filled turn allowed ...
             1.1          6.1         modern view study turned face detail standing born return spring ...
             1.2          6.3         sort deal simple fashion easy exactly call reason shape simply ...
             1.1          6.4         proper require care properly required prevent laid making taking allowed ...
             1.0          6.7         person occasion purpose respect answer short act sort receive rest ...
             1.0          6.1         want look going deal try bad tell sure feel remember ...
             1.2          6.3         saw cried looked heard stood asked sat answered began knew ...


               Table 2: Disagreement between low human scores and high Google-log-hits scores.



6 Document Similarity                                             then they are likely to be mistakenly considered simi-
                                                                  lar using document similarity metrics that rely on term
Discovering semantically similar documents in
                                                                  frequencies. Below, we explain our experimental setup
a collection of unstructured text has practical
                                                                  and results.
applications, such as search by example.              Many
studies have been proposed to calculate inter-document
                                                                  Count-Based Similarity
similarity since 1950s.         For example, Grangier
and Bengio [2005] use hyperlinks to score linked                  We used the Okapi BM25 [Walker et al., 1997] rank-
                                                                                                                     3
documents on the Web higher than unlinked for                     ing function implemented in the Zettair search engine.
information retrieval tasks. Kaiser et al. [2009] use             Similarity scores are based on term frequency and in-
Wikipedia to find similar documents for a focused                 verse document frequencies in a document collection.
crawler (they also provide a good literature review on
                                                                  Topic-Based Similarity
recent approaches that use support vector machines,
latent semantic analysis (LSA), or explicit semantic              A document similarity measure using topics was com-
analysis).   Lee et al. [2005] empirically compare                puted using Hellinger distance. For every pair of docu-
between three categories of binary, count, and LSA                ments di and dj in a collection, and a set T of learned
similarity models over a small corpus of human judged             topics, Hellinger distance is computed as below:
texts and concluded that evaluation of such models
should occur in the context of their applications.
                                                                                              T                                2
    Humans judge two texts to be similar if they share                                    1  
                                                                      dist(di,dj) =                    p(t|di) -        p(t|dj)  ,
the same concepts or topics [Kaiser et al., 2009]. We                                     2
                                                                                             t=1
use our learned topics from News Articles to find sim-
ilar documents and compare them against count-based
                                                                                                    
models implemented in a search engine. Our prelim-                                                                              2
                                                                                         1
inary findings show that if documents contain useless               dist (di,dj) =       2                p(t|di) -       p(t|dj) ,
text -- words that are not related to the main topic of                                    tuseful

the text or bear no content, such as advertisements --                3http://www.seg.rmit.edu.au/zettair/




                                                               16

                                                    News Articles (corrcoef=0.49)


                                                  7

                                                                                                          2000
                                                  6

                                                  5

                                                  4                                     st                    1500

                                                  3

                                                                                          ocumen
                Google-log10-hits                 2                                             D
                                                                                                 f
                                                                                                  o               1000
                                                  1                                                er
                                                                                                     b
                                                  0                                                   um
                                                     1   1.5     2     2.5   3                          N
                                                                                                                      500
                                                       Mean human score


                                                      Books (corrcoef=-0.09)
                                                                                                                         0
                                                                                                                            0 4 8 13192531 37 43 49  55   61 677379859197
                                                  7
                                                                                                                                            Useless Text (%)
                                                  6

                                                  5                                    Figure 9: Number of documents versus proportion
                                                  4                                    of usefuless content. 4.3% of documents have more
                                                  3                                    than 50% useless text and 16.4% have more than 30%

                                 Google-log10-hits2                                    useless text.
                                                  1

                                                  0                                    scoring of similar or not-similar was used. The criteria
                                                     1   1.5     2     2.5   3
                                                       Mean human score                for similarity was the overall subject of the documents,
                                                                                       for example, both being about a specific sport. For 32
Figure 8: Scatterplot of Google-log-hits score vs. mean                                of 50 cases (64%), all methods successfully resulted in
human score.                                                                           documents judged to be similar by the human judge. In
                                                                                       only one case did Okapi outperform both topic-based
where p(t|di) and p(t|dj) are probabilities of topics                                  methods. Using the useful-topics metric (dist ) led to                        

in documents i and j. We provide two formulas for                                      94% accuracy against similarity judgements; all topics
Hellinger distance, one based on all topics, and dist                                  (dist) was 88% accurate; Okapi was 70% accurate.
that uses just the "useful" topics.                                                    Also, the overlap between the ranked outputs of the
                                                                                       two systems, Okapi and useful topics, was very low:
Experimental Setup                                                                     30% in Top-1 overlapped (the documents were the
                                                                                       same for the both systems).
Fifty documents were randomly selected from News
                                                                                                                          Figure 10 shows an illustrative example where us-
Articles based on their proportion of useful and useless
                                                                                       ing topic modeling, in particular using good topics (i.e.
topics. An overview of the documents in the collection
                                                                                       dist ), outperforms Okapi when the original document
                                                                                                                          
based on their percentages of useless text is shown in
                                                                                       contains a large proportion of non-content text.
Figure 9. Our aim is to improve document similar-
                                                                                                                          While he experiments described in this section are
ity calculations on the right tail of this graph where
                                                                                       limited in scope, they constitute an initial investigation
the documents contain a larger proportion of useless
                                                                                       into the task-level effectiveness of topic-based metrics
text which could mislead document similarity methods
                                                                                       that ignore "useless" topics. We believe that the results
that rely on the frequency of terms. We therefore first
                                                                                       indicate that, for texts that contain "noise", identifying
extracted those documents that contained at least 30%
                                                                                       the "useful" topics in a topic model has promising ap-
useful content (based on PMI-Wiki-Score) and at least
                                                                                       plications.
40% non-content text. We then calculated the simi-
larity scores of 50 randomly selected documents from
this subset with other documents in the collection. For                                7 Conclusion
count-based methods, we used each of these 50 full                                     Evaluation of topic modeling -- the analysis of large
documents as queries to retrieve a ranked list of simi-                                sets of unstructureddocumentsand assignment ofseries
lar documents using the Zettair search engine. For the                                 of representative words as topics to clusters of docu-
topic-based method, two approaches were used: using                                    ments -- has hardly been investigated. In particular,
all the topics generated for the collection (T200), and                                meaning of the topics and human perception of their
using useful topics as based on the topics' PMI-Wiki-                                  usefulness had not been studied before. Here, we inves-
Score.                                                                                 tigated topic modeling evaluation using external data
    In a preliminary experiment, a human judge was                                     (Wikipedia documents, Google n-grams, and Google
presented with original documents and the top most                                     hits), and compared our proposed methods with human
similar document (Top-1) extracted by each method.                                     judgmentson usefulness of the topics. According to our
The humanjudge was not aware of the order of methods                                   experiments on collections of news articles and books,
which the documents were retrieved. A simple binary                                    a scoring method using pointwise mutual information



                                                                                     17

Original Document                         Okapi BM25 (Zettair)                   All Topics                              Useful Topics
At last! A biography that skips the More New Yorkers would vote against We may be living in a high-tech era              The Clinton administration, in a move
saint-or-sinner debate.      As Dusko Hillary Rodham Clinton as a U.S. but it still takes a low-tech truck to            intended to bolster opponents of Pres-
Doder and Louise Branson abundantly Senate candidate than vote for her, deliver something you've ordered over            ident Slobodan Milosevic, has agreed
document,      Slobodan      Milosevic, a new poll indicates.       The survey the Internet, which is why Forbes to lift economic sanctions on Serbia as
almost from the start, epitomized the by the Zogby International polling magazine picked Atlanta-based United            soon as there is a free election there,
Balkan-variety bad seed. The child of organization      shows    the    probable Parcel Service as its "company of the senior administration officials said on
parents who both committed suicide, Democratic nominee carrying an year." "With 157,000 ground vehicles,                 Tuesday. The administration had pre-
Milosevic aligned himself with a "unfavorable rating" of 48.4 percent 610 aircraft and $11 billion invested              viously vowed that it would not lift
woman who hungered for power to among likely voters, as opposed to in technology, UPS moves both atoms the sanctions until Milosevic had been
avenge the ignominious death of her her "favorable rating" of 46.3 percent. and bits," says Forbes in announcing removed from power.               But officials
mother. Milosevic betrayed a college It marks the first time the potential its "platinum list" of "America's best calculate that the new strategy should
classmate, a mentor of two decades, candidate's statistical negatives have big companies." According to Forbes, allow the Serbian opposition to in-
and his next-door neighbor in lunging eclipsed her positives in her still- UPS's role as a shipper of 6 percent crease popular pressure on Milosevic,
to the top of Yugoslavia's diseased undeclared campaign, pollster John of the nation's gross domestic product to call early elections, since holding
post-Tito political leadership.      And eet, a city councilman in a tight race. makes it "the missing link in the a free election would mean an end to
"Milosevic: Portrait of a Tyrant"...      "But I hope you of Utica said Tuesday. burgeoning world of E-commerce."        an oil embargo, an air-travel ban and
...                                       ...                                    ...                                     other sanctions that have weakened an
(gm)                                      (gm)                                   Story Filed By Cox Newspapers (gm)      already devastated Serbian economy.
FOR WEDNESDAY AMs                         FOR WEDNESDAY AMs                      Here are the stories New York Times Secretary of State Madeleine Albright
Here are today's top news stories Here are today's top news stories from editors are planning for Tuesday, is expected to make the announcement
from The New York Times News The New York Times News Service Dec. 28 Page 1. The NYT frontpage Wednesday, but it carries a risk: that
Service for ally at LaSalle University for AMs of Wednesday, Dec. 22:            advisory, with layout description, will bickering opposition parties would so
for of Wednesday,         Dec.        22: INTERNATIONAL          ("i"     code) move by 7:30 p.m.       ET. The NYT fragment the election results that Milo-
INTERNATIONAL             ("i"     code) CHINA-INTERNET           (Beijing)    - News Service Night                      sevic might be able to cling to power
CHINA-INTERNET            (Beijing)    - With the ambivalent blessing of the Supervisor       is   Pat   Ryan     (888- or, far less likely, that he would win
With the ambivalent blessing of the Chinese government, locally produced 346-9867).                ISRAEL-POLITICS outright in the balloting.
Chinese government, locally produced web sites and chat rooms have spread (Jerusalem) - The Shas political party, ...
web sites and chat rooms have spread rapidly here in the last two years,...      which represents Sephardic Jews of Although the constitution of the Yu-
rapidly here in the last two years,...    RUSSIA-U.S.-AID (Washington) - Middle Eastern and North African goslav federation of Serbia and neigh-
RUSSIA-U.S.-AID (Washington) - The State Department, invoking a descent,                   announced Monday that boring Montenegro does not grant
The State Department, invoking a seldom-used law, may block a $500 it had decided to quit the coalition Milosevic direct power to call new
seldom-used law, may block a $500 million loan package for Russia's oil government of Israeli Prime Minister elections, the reality is that his powers
million loan package for Russia's oil sector. By David E. Sanger.                Ehud Barak.                             are dictatorial
sector. By David E. Sanger...             ...                                    ...                                     ...

Figure 10: An example of top ranked similar documents returned by three methods: Okapi scores generated by
Zettair, topic-based similarity using all topics (dist), and topic-based similarity only using useful topics. Using
only useful topics (dist ) produces the best result.
                                  




on Wikipedia documents and Google n-grams has great                                D. Grangier and S. Bengio.               Inferring document similar-
potential to distinguish useful (or meaningful) topics                                ity from hyperlinks. In Proceedings of the 14th ACM
from useless ones. This finding is supported by high                                  international conference on Information and knowledge

correlation between our scoring approaches and human                                  management, pages 359­360, Bremen, Germany, 2005.

judgements on the same topics. We also showed a pos-                               T. Griffiths and M. Steyvers. Finding scientific topics. In
sible application for distinguished useful topics in ex-                              Proceedings of the National Academy of Sciences, volume
traction of similar documents in a collection.                                        101, pages 5228­5235, 2004.

Acknowledgements NICTA is funded by the                                            T. Griffiths and M. Steyvers. Probabilistic topic models. In
Australian government as represented by Department                                    Latent Semantic Analysis: A Road to Meaning, 2006.
of Broadband, Communication and Digital Economy,
                                                                                   F. Kaiser, H. Schwarz, and M. Jakob. Using Wikipedia-based
and the Australian Research Council through the ICT
                                                                                      conceptual contexts to calculate document similarity. In
centre of Excellence programme. DN has also been                                      Proceedings of the 2009 Third International Conference on
supported by a grant from the Institute of Museum and                                 Digital Society, pages 322­327, Cancun, Mexico, 2009.
Library Services, and a Google Research Award.
                                                                                   M. D. Lee, B. Pincombe, and M. Welsh. An empirical evalua-
Authors are thankful to Timothy Baldwin for valuable
                                                                                      tion of models of text document similarity. In Proceedings
discussions.
                                                                                      of the 27th Annual Conference of the Cognitive Science
                                                                                      Society, pages 1254­1259, Mahwah, NJ, 2005.
References
                                                                                   Q. Mei, X. Shen, and C. Zhai.                   Automatic labeling of
D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent Dirichlet                              multinomial topic models. In Proceedings of The 30th
   allocation. Journal of Machine Learning Research, 3:993­                           International Conference on Knowledge Discovery and
   1022, 2003.                                                                        Data Mining, pages 490­499, 2007.

W. L. Buntine and A. Jakulin. Applying discrete PCA in                             S. Walker, S. Robertson, M. Boughanem, G. Jones, and
   data analysis. In Proceedings of the 20th Uncertainty                              K. Sparck Jones. Okapi at TREC-6 automatic ad hoc, VLC,
   in Artificial Intelligence Conference, pages 59­66, Banff,                         routing, filtering and QSDR. In Proceedings of the 6th Text
   Canada, 2004.                                                                      REtrieval Conference, pages 125­136, 1997.

                                                                                   H. M. Wallach, I. Murray, R. Salakhutdinov, and D. M.
S. Geman and D. Geman. Stochastic relaxation, Gibbs distri-
                                                                                      Mimno. Evaluation methods for topic models. In Proceed-
   butions, and the bayesian restoration of images. volume 6,
                                                                                      ings of The 26th International Conference On Machine
   pages 721­741, November 1984.
                                                                                      Learning, pages 1105­1112, Quebec, Canada, 2009.



                                                                               18

        Id - Dynamic Views on Static and Dynamic Disassembly Listings

                            Nicholas Sherlock                        Andrew Trotman

                            Computer Science                        Computer Science
                           University of Otago                     University of Otago
                       Otago 9010 New Zealand                  Otago 9010 New Zealand
                        n.sherlock@gmail.com                    andrew@cs.otago.ac.nz


Abstract     Disassemblers     are  tools   which   allow   storage media from tapes, to 8, 51/4 and 31/2 inch floppy
software   developers    and   researchers    to  analyse   disks, to CD-ROMs, DVDs, Blu-ray, and increasingly,
the machine code of computer programs.             Typical  removable flash-memory based storage.         With each
disassemblers convert a compiled program into a             new hardware generation, our old software becomes
static disassembly document which lists the machine         obsolete and is either rebuilt or abandoned.
instructions of the program. Information which would            This   creates  a  problem     for researchers    and
indicate the purpose of routines, such as comments and      historians.   While design manuals can be scanned
symbol names, are not present in the compiled program.      and stored accessibly in a digital library, and data
Researchers must hand-annotate the disassembly in a         can be retrieved from old media with somewhat
text editor to record their findings about the purpose of   more effort and expense, storing the software from
the code.                                                   these old machines in a useful format is an entirely
   Although running programs can change their layout        different problem.    Performing analysis on software
dynamically, the disassembly can only show a snapshot       which is stored in the library becomes increasingly
of a program's layout. If a different view of a program is  difficult with time. This is because a piece of software
required, the document must be recreated from scratch,      cannot be used, examined, or understood in isolation.
making it difficult to preserve user annotations.           Its behaviour is defined by its interaction with the
   In this paper we demonstrate a system which al-          hardware it was built for. Obsolete hardware becomes
lows a disassembly listing to be refined by user input      progressively more scarce with time. Preserving old
while retaining user annotations.      Users are able to    hardware by building modern replicas requires an
dynamically change the interpretation of the layout of      increasingly infeasible amount of effort and resources
the program in order to effectively analyse programs        as microelectronics become more complex.
which can alter their own memory layout. We allow               If an accurate description of the hardware is pro-
users to combine the independent analysis of several        vided or can be discovered, it can be replaced by a
program modules in order to examine the interaction         software-based "emulator". An emulator in this context
between modules.                                            is a program which simulates the action of an old hard-
   By exploring the obsolete "Poly" computer system,        ware platform on (typically many) modern platforms.
we demonstrate that our disassembler can be used to         In this way, researchers can examine the runtime be-
reconstruct and document entire software distributions.     haviour of old software without having to perform a
                                                            costly hardware reconstruction of an old platform.
Keywords      Digital Libraries, Cognitive Aspects of
                                                                A second problem is the difficulty of examining the
Documents, Document Workflow
                                                            algorithms and implementation details of obsolete soft-
                                                            ware when human-readable source code has been lost
1    Introduction
                                                            or was never provided. It is also a problem for mod-
The   rate   of   computer     hardware    and   software   ern software.    For example, in order to build a new
development     is  increasing   exponentially.      Five   program which interoperates with an existing program,
years ago, our desktop computers were all powered by        some knowledge of the original program's internal op-
single-core CPUs. Two years ago, they had dual-core         eration is required. Even if the source code for a pro-
CPUs. And today, they are likely to have four or eight      gram is available, you may still want to examine the
cores. The Macintosh series of computers have seen          machine instructions that the compiler generates to en-
large architectural changes, switching from Motorola        sure that the generated instruction sequences are correct
CPUs to PowerPCs and finally to Intel x86 CPUs.             or efficient. Machine code is a more primitive level
In successive steps, we have changed our removable          of abstraction which can reveal surprising negative per-
                                                            formance implications of innocuous-looking high-level
Proceedings of the 14th Australasian Document Comput-       code.
ing Symposium, Sydney, Australia, 4 December 2009.              If only the machine code that makes up the com-
Copyright for this article remains with the authors.        piled program is available, it must first be translated



                                                          19

into a more abstract form that humans can understand       In one prototype, a client could be turned into a Proteus
so that it can be analysed. A program which performs       server with the addition of a disk drive. The computers
this translation is called a "disassembler". Much of the   can be seen in Figure 1.
information found in the high-level source code of a           The Poly never gained much ground in the computer
program, including comments and the names of vari-         market and few machines were produced. Although the
ables and routines, is lost in the compilation process.    Poly demonstrated innovative technologies and ideas,
To understand a compiled program, a researcher must        and is an important part of New Zealand's computing
recover this lost information. They can achieve this by    history, little is now known about it. In particular, the
inspecting the disassembly listing with reference to the   Poly's networking capabilities were far ahead of con-
behaviour of the running program. They can then share      temporary computers, and it was provided with inno-
their findings with other researchers by annotating the    vative classroom software to take advantage of those
disassembly.                                               features. But with only a couple of working Polys in
    Several factors make this process difficult.     The   existence and little surviving documentation, the exact
disassembler's interpretation of the program must be       functionality of the software is largely a mystery.
dynamically altered to analyse programs which can              In order to make the Poly's software available to re-
change their layout at runtime.      In order to change    searchers, we would have to document it in a form that
the interpretation of the program, the disassembler        would be useful long after the last Poly stops operating.
must be re-run.      This creates a new, independent
disassembly document, which makes it difficult to          3    Disassembly
preserve annotations that the researcher has already
                                                           48A6    34 14               PSHS X,B ;Ref from $CD39
made.
                                                           48A8    8E 5B 19            LDX #$5B19
    Even if a program does not change its layout dy-
                                                           48AB    C6 05               LDB #$5
namically, the researcher must still frequently change
                                                           48AD    E7 80               STB ,X+
the disassembler's interpretation of the program. This is
                                                           48AF    35 04               PULS B
because the disassembler cannot distinguish code from
                                                           48B1    E7 80               STB ,X+
data in the compiled program with perfect accuracy.
                                                           48B3    35 20               PULS Y
Human judgements are required to correct the disas-
                                                           48B5    EC A4               LDD ,Y
sembler's mistakes.
                                                           48B7    ED 84               STD ,X
    In  order   to   allow   researchers  to  effectively
                                                           48B9    8E 5B 19            LDX #$5B19
document entire programs, software support is required
                                                           48BC    10 8E 00 04         LDY #$04
to assist the user in navigating and imposing structure
on large disassembly documents,         but this is not
provided with a traditional disassembler.      Although
                                                                Figure 2: A fragment of a disassembly listing
programs being analysed are often composed of several
related modules which can be examined independently,           A tool called a "disassembler" examines a program

disassemblers typically do not provide any way             binary (that is, the machine code that the computer will

of linking disassemblies together in order to share        execute, not the source code which is used to generate

information about interacting modules.                     it) and creates a text file called a disassembly listing.

    In this paper, we will present our disassembly, de-    A disassembly listing shows the machine code instruc-

bugging and emulation system which we used to re-          tion that appears at each memory address within the

construct and document the software and hardware of        program as a human-readable mnemonic code. It also

the "Poly" computer system. We will show that our          shows the data stored inside the program, such as the

disassembler can solve the problems inherent in docu-      text of string literals or numeric literals from the source

menting the software of the Poly by using it to create a   code.

digital Poly software library which researchers will be        Figure 2 shows a fragment of a program disassem-

able to examine long into the future.                      bly for the Poly's Motorola 6809 CPU[8]. The left-
                                                           most element is the memory address of the disassem-
                                                           bled instruction. Next is a hexadecimal representation
2    The Poly computer system
                                                           of the machine code that the CPU will execute. Fi-
The Poly was a computer system developed in New            nally, a human-readable interpretation of the machine
Zealand in the early 1980s. It was comprised of a server   code is displayed. The first part of the instruction is a
computer called the "Proteus" with a series of fat-client  mnemonic which represents the instruction being per-
"Poly" machines attached by a token ring network. It       formed (for example, PSHS is an instruction to push a
was designed to be used in a classroom setting where       value onto the stack). Any arguments to the instruc-
a teacher would set work on the server computer to         tion follow the mnemonic.       X, B and other symbols
be distributed to each student's computer. When the        refer to registers on the CPU and values starting with
students finished their work, their results would be sent  a hash symbol are numeric literals. There is effectively
back to the server computer to be saved to disk. The       a one-to-one mapping between the machine code and
server and the client machines had similar architectures.  the mnemonic representation shown to the researcher.



                                                         20

             (a) Two Poly client machines sit side-by-side       (b) A Proteus server and its CPU and memory board (inset)


                                    Figure 1: The key components of the Poly system


    There are two major difficulties in building a useful
disassembler program. The primary difficulty is that it
is impossible in general to automatically decide which
parts of the program binary are data and which parts are
code which will be executed. This problem is equiva-
lent to the halting problem[5]. Because of this, disas-
semblers must sometimes guess where a machine in-
struction begins in memory and so will make some in-
correct guesses. Wrong guesses might identify the be-
ginning of a sequence of instructions at the wrong off-         Figure 3: A virtual memory address is translated using
set (so that the interpretation of the sequence begins          the memory map into a physical address
halfway through a machine instruction, generating in-
                                                                    With a traditional disassembler, the researcher
correct output,) or incorrectly identify data as code or
                                                                would have to disassemble the program once for
vice versa, which hampers correct interpretation of the
                                                                every memory mapping they wanted to examine,
program.    Some code locations can not be identified
                                                                and    maintain     the     different  disassembly      listings
because their addresses are computed at runtime by the
                                                                independently,      even when information should be
program in a way that the disassembler cannot predict.
                                                                shared between them. The same physical memory page
For example, a program may read the address of the
                                                                can even appear in virtual memory in more than one
routine to execute from an external file.
                                                                place simultaneously, making it difficult to manually
    The    second    difficulty    is  encountered        when
                                                                keep annotations consistent and up to date.
analysing software that was built for small systems
                                                                    Many small-CPU based systems, including systems
like the Poly. Like many computers of its time, the
                                                                of about the Poly's age, use dynamic memory maps to
Poly had more physical memory available than it could
                                                                overcome the limitations of restrictively small address
simultaneously address. Its CPU's memory address bus
                                                                spaces. For example, the Apple //e[9], ZX Spectrum
is 16-bits wide, allowing it to address 64kB of virtual
                                                                128[1] and Commodore 128[6], which, like the Poly,
memory at any one time.          The Poly has 128kB of
                                                                have 16-bit address busses and can support 128kB or
physical RAM plus 8kB of BIOS and memory-mapped
                                                                more of memory. Software written for 16-bit operating
peripherals. Software on the Poly dynamically changes
                                                                systems such as MS-DOS on more modern PCs or soft-
the mapping of the 8kB virtual memory pages to the
                                                                ware for embedded systems also use this technique. To
128 + 8kB physical address space by changing the
                                                                effectively analyse these systems, a new kind of disas-
entries in a memory map.
                                                                sembler is required.
    In Figure 3, a 16-bit virtual memory address is
translated into a 17-bit address in physical memory in a
                                                                4    Background
series of steps. In "protected" mode (operating system
mode), some addresses are directed to hardware and              Disassemblers are available for nearly every platform.
the BIOS. Otherwise, the three most-significant bits of         Disassembly tools are available in two main contexts:
the virtual address are combined with a bank-select bit         As a static disassembler tool to examine stored pro-
and used as an index into the programmable memory               grams on disk, or dynamic disassemblers which exam-
map. The memory map replaces the three higher bits              ine snapshots of running programs.
of the virtual address with four bits of its own, creating
a 17-bit address in physical memory.



                                                              21

4.1    Static analysis                                           The jump may have a definite target, and be taken
                                                             at runtime, but the target of the jump which is stored in
The tool "objdump"[2] typifies static disassembly
                                                             the instruction is overwritten at runtime before the jump
tools. A binary program on disk is provided as input
                                                             is ever called. This is seen on modern architectures.
to objdump, and the output is a disassembly listing
                                                             A module of code (such as a Windows dynamic-link
document.
                                                             library or a Unix shared object) which a program uses
    The generated listing may be explored and anno-
                                                             may be dynamically loaded at an unpredictable position
tated with a simple text editor, but this approach has
                                                             in its address space. To be able to call routines from the
two serious disadvantages. Firstly, a text editor treats
                                                             module, the program needs to know their addresses. To
the disassembly as unstructured text and so can offer
                                                             achieve this, an "import table" is generated in the ap-
very little software support for common annotation op-
                                                             plication. The import table consists of a series of stubs.
erations. It will not offer cross-referencing support, so
                                                             The stubs are small routines which contain a jump to
any references that the code makes to other parts of
                                                             an address which is initially some default value (NULL).
the program must be followed manually. If the analyst
                                                             When the library is loaded, the memory locations of
gives a descriptive label to a block of code, that label
                                                             its routines are discovered and used to rewrite the code
will not be propagated to the places where the code is
                                                             in the import table. To call an imported routine from
called. The analyst cannot effectively experiment with
                                                             within the program, a call to the stub is made some time
different interpretations for data stored in the program.
                                                             after the library is loaded. Calling the routine before the
For example, to reinterpret a number as signed or un-
                                                             library is loaded results in undefined behaviour.
signed will require the researcher to manually convert
                                                                 In order to correct code which has been misidenti-
the number using some other tool, or re-run the disas-
                                                             fied as data, or vice-versa, the user must run the disas-
sembler to create an entirely new disassembly. These
                                                             sembler again with that new information. This produces
problems dramatically slow down analysis and make
                                                             an entirely independent disassembly listing which must
understanding the program much more difficult.
                                                             then be manually merged with the listing the user has
    Secondly, a static disassembler cannot always cor-
                                                             annotated. This is an error-prone and tedious process.
rectly distinguish code from data in the analysed pro-
                                                             The user is unlikely to want to experiment with differ-
gram. For example, the code may include a jump whose
                                                             ent interpretations of a memory address, because each
target is an address which is computed at runtime. This
                                                             experiment is so costly to run in terms of user effort.
is common in object-oriented code, where the address
of a virtual method must be looked up in an object's         4.2     Dynamic analysis
virtual address table. In procedural code, this technique
is more likely to be used with a jump table--a table of      A debugger like the free tool "gdb"[7] is designed to
routine addresses that selected from at runtime, often by    allow the user to inspect and interact with running pro-
an equivalent of the "switch" statement in C. Data-flow      grams. If no debugging information or source code is
analysis techniques could be used to discover the targets    provided which would allow it to show the high-level
for some of these computed jumps[4]. For instance, the       code that corresponds to the running machine code, it
instruction sequence LDX #0x5B19 / JMP X (storing            uses an embedded disassembler to show the disassem-
the value 0x5B19 into the register X, followed by a jump     bly of the code that is currently executing.
to the value stored in X) is clearly a jump to the location      This approach has several advantages. Code can be
0x5B19. Even so, some jumps are computed in a way            distinguished from data with certainty, since the debug-
that no disassembler could possibly understand (e.g. by      ger only needs to show the disassembly for instructions
using data available at runtime which is not present in      which are currently executing or have previously exe-
the image being disassembled, such as data contained         cuted. The user can have the debugger interpret any
in a message received on the network.)                       memory location in multiple ways. For example, they
    The disassembler might identify a jump with a            could view one location as both an array of integers and
known target which in fact points to data, not code. If      an array of characters, and discover that the data only
the Poly jumped to that location, it would likely have       makes sense when interpreted as an array of integers.
unexpected results, perhaps crashing.       If we assume         The analyst can interact with the running program
that the the Poly code does not crash, it is reasonable      to see what inputs a piece of code receives, or precisely
to assume that it does not take bad jumps. There are at      what action it takes as a result. The running program
least two possible causes of this situation.                 may be modified by the analyst to explore areas of code
    It may be impossible for the flow of execution           that would not normally execute. For example, they
to ever reach the jump, so the bad jump is never             can force the code to follow an error-handling branch
executed in practice. For instance, a program might          in order to examine that mechanism, even if they do not
check the state of a "debugging mode flag", and, based       know what inputs to the program are needed to cause
on the value it finds, jump to some logging routine          the error to be triggered in normal execution.
which ended up being cut from the final binary. The              The main disadvantage of this approach is that the
debugging mode flag is never set in delivered software       user is typically unable to add any annotations to the
so the bad jump is never taken.                              disassembly. If they discover the purpose of a routine,




                                                           22

they cannot give it a human-readable label which would      memory. On systems with address spaces larger than
allow it to be understood the next time it is encountered.  the amount of physical memory available, like modern
Even if annotations are supported, the debugger will not    32- and 64-bit computers, the reverse tends to be true.
provide any way to save them and load them again later,     Programs move around in physical memory but stay in
since it has no expectation that the memory layout of the   a fixed position in virtual memory.
program will be similar the second time the program is          Initially, no code has been identified in the image
run. Analysis with a debugger is ephemeral, it cannot       (it is all considered to be data). To begin the disassem-
be effectively used to produce a document which could       bly process, you must identify the start of a machine
record the user's findings to be shared with other re-      instruction in the image. The CPU has to perform the
searchers.                                                  exact same task when the Poly boots. The CPU begins
                                                            by reading an address from an interrupt table at a fixed
4.3     Interactive disassembly
                                                            location in memory, which is called the "reset vector".
Traditional disassemblers are frequently used in situa-     This is the location where execution begins after boot.
tions where the disassembly is only useful for a short      Id begins disassembly at this point. If the instruction at
amount of time, like a single session, and saving an-       the reset vector is a jump to a different location, Id can
notations is less important. For example, a common          follow the jump and recursively identify code there. If
task for a traditional disassembler is examining the ma-    the instruction is not a jump, execution will continue
chine code generated by a procedure in a high-level         to the next location in memory, so Id identifies an in-
language to diagnose performance or code generation         struction there. Following jumps from the entry points
issues. Since they are typically used to examine a pro-     defined in the interrupt table identifies much of the code
gram which is currently in development (and therefore       in the image--around 60% for the Poly's BIOS. The
changing dramatically from a machine code perspec-          remainder of the code is often interrupt handlers whose
tive), the ability to save annotations is not valuable.     address is determined at runtime in a fashion that is
    If a disassembly listing is to be modified and ex-      currently too difficult for Id to discover.
amined over an extended period of time (i.e. several            To assist Id, the user can create new entry points
analysis sessions), or shared with other people, it must    (the beginnings of instruction sequences) at any time.
be able to change dynamically as more information is        Id automatically adds disassembly for those previously-
discovered by a human researcher. The researcher will       unidentified locations to the disassembly listing. If the
work with the disassembler to analyse a program. This       disassembler has wrongly identified data as code, the
is the approach that we decided to take with our own        user can convert it back to data.
disassembler.                                                   While the physical representation of the data in the
    The only interactive disassembler that we are aware     program is known from the disassembly, the informa-
of in common usage is IDA[3]. But IDA does not sup-         tion that the data encodes cannot always be inferred au-
port the dynamic memory model of the Poly. While it         tomatically. For example, Id knows that the operands of
supports debugging live code for some targets, it does      instructions which specify the targets of read or writes
not integrate with our Poly emulator.                       to memory locations are memory addresses. It can use
                                                            syntax colouring to distinguish these addresses from
5    Id, the interactive disassembler                       other numeric literals found in the program. When pos-
                                                            sible, the symbolic name that the user has given to the
To assist our reconstruction of the Poly platform, we       target address is shown in place of the raw address.
developed "Id", an interactive disassembler which sup-          However, operands to instructions which are merely
ports the Poly's CPU and binary layouts. Id is an ap-       stored into registers or into memory locations have no
plication for Windows with a Graphical User Interface.      special meaning defined by the instruction set. Id al-
The main pane of Id is the disassembly listing. Sur-        lows the user to experiment with different interpreta-
rounding the listing are panels that give extra informa-    tions of the data in order to discover what information
tion about the binary being disassembled. For instance,     the data is encoding. For example, Id can interpret data
one panel is a list of all of the symbol names created so   as strings, arrays, addresses, or numeric types of vari-
far in the image. Id can be seen in Figure 4.               ous sizes and formats. As even simple operations such
    To support the changing layout of programs on the       as adding a constant to a number stored in a register can
Poly, all of the items that Id identifies in its disassem-  have multiple reasonable meanings, this user-directed
bly are tagged with the physical address that they are      assistance is crucial to documenting the purpose of the
stored at, not the virtual memory addresses that they       program. For example, the machine code and effect
appear at with one possible memory mapping.           This  of subtracting 16 from a number stored in a register is
allows the user to change the virtual memory map while      identical to that of adding 65520 (0xFFF0 in hexadec-
they are examining the disassembly, and have the dis-       imal notation). But these two operations suggest very
assembly listing change dynamically to reflect the new      different purposes for the code being disassembled. In
interpretation of the program's layout. This approach       the first case, the program may be accessing data that
works well with the Poly because code and resources on      appears immediately before a previously-computed ad-
this platform typically have a fixed location in physical



                                                          23

Figure 4: Id's main GUI with the Proteus operating system loaded for disassembly. Andrew Trotman's comments
appear after semicolons. All names in the disassembly are user-entered.


dress. This is the pattern expected if the program is        as "num clients", and provide the extended comment
iterating over an array of elements whose size is 16-        "number of Poly clients currently connected" to clar-
bytes in reverse order. In the second case, the code may     ify the meaning of the location. A comment at a re-
be calculating the location of a dynamically-determined      ferring site which increments this value might then be
field of a structure which is located at the fixed address   customised: "record newly-attached client".
0xFFF0. This is the pattern expected if the program is           As the user adds labels to locations in the mem-
looking up the current location of one of the installed      ory map, the propogation of these labels to referring
interrupt routines (as the interrupt-vector table is lo-     sites makes the meaning of previously unexplored code
cated at 0xFFF0). By specifying the signedness of the        clearer. The analysis process shows a "jigsaw-like" ef-
operand, the user can disambiguate these two cases.          fect. As with a jigsaw, a series of interlinked pieces
                                                             meet at common interfaces. As the jigsaw is completed,
5.1    Annotation                                            the possible shape and location of the unplaced pieces

Id is a hypertext document system. The start of each         is futher and further constrained. This implies that the

routine or piece of data can be given a title by the user.   initial analysis--discovering the large-scale structure of

Id then provides a "names" pane, which lists every ti-       the program--is the most difficult phase, with analysis

tle in the program, sorted by module. This structured        becoming more rapid as the final "pieces" are placed.

outline allows the document to be navigated rapidly.             Id's hyperlinks are bidirectional. The user can select

    References to memory locations found in the code,        a title and discover all of the links which point to it

such as the targets of jumps or the targets of memory        by using Id's "cross-references" window. Id helps the

reading instructions, are shown as hyperlinks. The user      user choose interesting referrers for further analysis by

can double click on the hyperlink to jump directly to        showing some context from each incoming link (the

its target. If there is a user-supplied title at the target  closest few instructions and comments). By examining

location, it is shown as the anchor text in the disas-       a routine's referrers, the researcher can determine (by

sembly in preference to the target's raw memory ad-          trial and error) what kind of inputs will be provided to

dress. The user can further describe the purpose of a        the routine and what sort of return value is expected in

location by adding an extended comment to the title.         response. Examining referrers is critical to discovering

This extended comment becomes the default comment            the purpose of a target routine or memory address.

which appears automatically at all referral sites. The           If the memory map changes, the names of the

user can edit the comment at a referral site in order to     targets of links in the code are updated accordingly.

record the exact way that the target is being used. For      In protected mode, a call to a routine at the address

example, the researcher might label a memory location        0xF030 might send instructions to hardware to print



                                                           24

text to the display. But in unprotected mode, a different   Poly's operating system and BIOS. If you created a
routine will reside at that address. It could also be a     disassembly of a text editor program, you could choose
line-printing routine, but it might first copy the user's   to either link to the disassembly of the Poly's operating
text to a buffer area before switching to protected mode    system (to examine the program's effect on the Poly),
to call the BIOS's line-printing routine. Id allows calls   or link with the Proteus's operating system (to examine
to these two routines to be distinguished by allowing       the program's effect on the Proteus.)       This linking
them to have different names.                               can be easily changed while you are disassembling.
    Broken hyperlinks, which are links that point to lo-    The gutter of the disassembly pane is colour-coded to
cations which do not currently exist in the disassem-       show the module that a line of code belongs to. Cross-
bly, are highlighted for the user.    The presence of a     module references are dynamically resolved based on
broken link could indicate that the target has not yet      the current selection of modules.      New information
been loaded from disk, or that the memory map is ex-        identified about linked modules is automatically used
pected to change before the link will be accessed by the    to update those documents when the parent document
program. This highlighting is particularly valuable for     is saved.    This system allows reuse of disassembly
discovering the connections between loadable modules        information--the information you discover about the
of code.                                                    operating system while analysing a text editor program
    While the user can instruct Id to reinterpret a lo-     is made available when examining the interaction
cation of the disassembly, the text of the disassembly      between a database program and the operating system.
listing itself cannot be directly edited by the user. This      The module system allows the user to document an
allows Id to ensure that the disassembly listing always     entire operating system and its attendant application
corresponds precisely to the machine code. In fact, the     suite as a set of linked disassemblies.
listing could be used to reconstruct the original binary
program. Id does allow comments to be added to any          5.3    Debugging
line of the disassembly as the purpose of routines are      A goal of Id was to unify the capabilities of static and
discovered. Once a piece of code has been understood,       dynamic disassemblers. Id includes a debugger which
adding a comment allows the purpose of the sometimes        can attach to a running instance of our Poly emulator.
confusing assembly to be shared with other researchers.     This allows the runtime behaviour of a disassembled
                                                            program to be examined. It also allows the creation
5.2    Modules
                                                            of new disassemblies based on code which is loaded
Disassembly will typically be performed on a "mod-          in memory at runtime from unknown sources. For ex-
ule".   A module is a set of code and data which is         ample, the applications on the client Poly machine are
tightly bound together. For instance, one module might      loaded from the server across the network. Using the
be the Poly's BIOS, which is the code that controls         debugger, a copy of a network-loaded application can
the interaction between the hardware and the software.      be dumped from the client for later analysis.
This code is stored in permanent memory chips on the            The debugger integrates with the disassembler
motherboard and appears in the Poly's virtual address       tightly.    For example, the debugger observes the
space when it enters "protected mode". Another mod-         instructions which are executed at runtime in order to
ule might be a boot sector read from a floppy disk. Id      discover the location of code in the image which it
understands the format of the Poly's file system and        could not have discovered with a static analysis of the
programs, so it can simulate the action of the Poly's pro-  program on disk. The identified code is automatically
gram loader and load a program from disk as a module        merged into the document that the user has already
into the correct physical memory locations for analysis.    created.   The user is notified if the newly-identified
    There are substantial cross-references between          code is inconsistent with previous disassembly.      For
modules, so the analysis of one module can be used          example, if a newly-identified instruction lies within
to understand a different module.      For example, the     a previously-defined instruction (which is vanishingly
BIOS is responsible for loading the boot sector from a      rare in valid code), the user is asked to decide which
floppy disk, then it transfers control to the boot sector   interpretation to accept.
program. By disassembling the BIOS, the entry-point             By using the debugger, the user can discover cross-
of the boot sector's code can be discovered.         In a   references at runtime which cannot be discovered with
traditional system, the disassembly document of each        static analysis. For example, the user can set break-
module is independent so there is no inter-module           points which pause execution when a certain memory
linking.                                                    location is read to or written from. When the break-
    To support the analysis of large systems, Id's          point is triggered, the user has identified a link which
module system allows multiple disassemblies which           references their memory address. This is particularly
were    created   independently    to  be  composed     to  useful in debugging the behaviour of hardware devices
appear as one coherent document.           For instance,    (which appear at virtual memory addresses in the Poly's
one disassembly might be the Proteus's operating            memory map).
system and BIOS. Another disassembly might be the




                                                          25

         (a) Emulated text-mode battleships game on the Poly client     (b) Emulated graphics-mode on the Poly client


                                              Figure 5: Emulated Poly machines


   In Figure 4, the debugger has been attached to an                   Our disassembler's Poly-specific knowledge is seg-
emulator which is simulating the Proteus server.               A    mented from its dynamic document engine, so it is rel-
breakpoint has been placed in part of a network routine.            atively easy to add support for more processors and
Execution will automatically pause at that point if the             architectures. This makes Id a very flexible tool for the
Proteus executes that instruction, allowing the user to             disassembly and documentation of small systems.
inspect the contents of memory and the CPU registers.
In Figure 5, two Poly clients with attached debuggers               References
are executing programs delivered by the Proteus server.
                                                                    [1] Various   authors.     128K    ZX    Spectrum  Technical
                                                                        Information.          World    Of    Spectrum,   http:
6    Conclusions
                                                                        //www.worldofspectrum.org/faq/reference/
                                                                        128kreference.htm, 2009. Accessed 17th September,
We used the dynamic document features of our interac-
                                                                        2009.
tive disassembler system, Id, in our analysis and reverse
engineering of the Poly.      By using it, we were able             [2] Inc. Free Software Foundation. GNU Binutils. http://

to investigate the Poly's networking code in order to                   www.gnu.org/software/binutils/, 2008. Accessed
                                                                        9 October, 2008.
to solve timing and hardware issues in our emulator.
Our Poly emulator is now able to successfully emulate               [3] Ilfak Guilfanov. IDA Pro Disassembler--multi-processor,
a Poly client attached to an emulated Proteus server.                   windows hosted disassembler and debugger. Hex-Rays,

   The disassemblies that we created with Id will form                  http://www.hex-rays.com/idapro/, 2009.              Ac-
                                                                        cessed 17th September, 2009.
the basis of a online library of information about the
Poly's software and hardware. Because Id allows re-                 [4] Matthew S. Hecht. Flow Analysis of Computer Programs.
searchers to link and share information between their                   Elsevier Science Inc., New York, NY, USA, 1977.

new disassemblies and our existing disassemblies of the             [5] R. N. Horspool and N. Marovac.       An approach to the
operating system, BIOS, and other system utilities in                   problem of detranslation of computer programs.      The
our library, analysis of the Poly system can be achieved                Computer Journal, Volume 23, Number 3, pages 223­
more rapidly and easily than with any other competing                   229, 1980.

disassembly system.                                                 [6] Lance Lyon.     Commodore 128 Alive!          Commodore
   The    documents      produced      all  contribute     to   a       128, http://www.commodore128.org, 2009.             Ac-
long-lasting store of knowledge about the Poly.                         cessed 17th September, 2009.
The disassemblies are in a format that is readable                  [7] The GNU Project.     GDB: The GNU project debugger.
even without using Id.        They consist of plain-text,               http://www.gnu.org/software/gdb/, 2009.             Ac-
human-readable disassembly listings similar to what                     cessed 15 September, 2009.
Id displays in its main pane, stored inside "zip"                   [8] T. Ritter and Boney J. The 6809. Byte Magazine, 1979.
compressed archives. This ensures that the information
                                                                    [9] Steven Weyhrich.     Apple ][ History Chap 7.     Apple
discovered with Id will be accessible long into the
                                                                        2  History,   http://apple2history.org/history/
future.
                                                                        ah07.html, 2009. Accessed 17th September, 2009.




                                                                  26

            Interestingness Measures for Multi-Level Association Rules

                         Gavin Shaw                                             Yue Xu

          School of Information Technology                     School of Information Technology
        Queensland University of Technology                  Queensland University of Technology
                 Brisbane QLD Australia                               Brisbane QLD Australia
              g4.shaw@student.qut.edu.au                                 yue.xu@qut.edu.au

                                                   Shlomo Geva

                                       School of Information Technology
                                    Queensland University of Technology
                                             Brisbane QLD Australia
                                               s.geva@qut.edu.au


Abstract    Association rule mining is one technique       both objective and subjective points of view [3] [8].
that is widely used when querying databases, especially    The most common measure is the support-confidence
those that are transactional, in order to obtain useful    approach [1] [2] [6], but there are numerous other
associations or correlations among sets of items. Much     measures [2] [3] [6] to name a few.          All of these
work has been done focusing on efficiency, effectiveness   measures were proposed for association rules derived
and redundancy. There has also been a focusing on the      from single level or flat datasets, which were most
quality of rules from single level datasets with many      commonly transactional datasets.       Today multi-level
interestingness measures proposed.        However, with    datasets are more common in many domains. With this
multi-level datasets now being common there is a lack      increase in usage there is a big demand for techniques
of interestingness measures developed for multi-level      to discover multi-level and cross-level association
and cross-level rules.     Single level measures do not    rules and also techniques to measure interestingness
take into account the hierarchy found in a multi-level     of rules derived from multi-level datasets.         Some
dataset. This leaves the Support-Confidence approach,      approaches for multi-level and cross-level frequent
which does not consider the hierarchy anyway and has       itemset discovery (the first step in rule mining) have
other drawbacks, as one of the few measures available.     been proposed [4] [5] [10].         However, multi-level
    In this paper we propose two approaches which          datasets are often a source of numerous rules and in
measure multi-level association rules to help evaluate     fact the rules can be so numerous it can be much more
their interestingness. These measures of diversity and     difficult to determine which ones are interesting [1]
peculiarity can be used to help identify those rules from  [2].  Moreover, the existing interestingness measures
multi-level datasets that are potentially useful.          for single level association rules can not accurately
                                                           measure the interestingness of multi-level rules since
Keywords      Information     Retrieval,  Interestingness
                                                           they do not take into consideration the concept of the
Measures, Association Rules, Multi-Level Datasets
                                                           hierarchical structure that exists in multi-level datasets.
                                                           In this paper as our contribution we propose measures
1    Introduction
                                                           particularly for asessing the interestingness of multi-
Association rule mining was first introduced in [1]        level association rules by examining the diversity
and since then has become both an important and            and distance among rules.        These measures can be
widespread tool in use. It allows associations between     determined during rule discovery phase for use during
a set of items in large datasets to be discovered and      post-processing to help users determine the interesting
often a huge amount of associations are found. Thus in     rules. To the authors' best knowledge, this paper is the
order for a user to be able to handle the discovered rules first attempt to investigate the interestingness measures
it is necessary to be able to screen / measure the rules   focused on multi-level datasets.
so that only those that are interesting are presented to       The paper is organised as follows.          Section 2
the user. This is the role interestingness measures play.  discusses related work. The theory, background and
In an effort to help discover the interesting rules, work  assumptions    behind    our   proposed    interestingness
has focused on measuring rules in various ways from        measures are presented in Section 3. Experiments and
                                                           results are presented in Section 4. Lastly, Section 5
Proceedings of the 14th Australasian Document Comput-      concludes the paper.
ing Symposium, Sydney, Australia, 4 December 2009.
Copyright for this article remains with the authors.


                                                           27

2     Related Work                                            peculiarity (also known as distance). These measures
                                                              were chosen as they are considered to be objective (rely
For as long as association rule mining has been around,
                                                              on just the data).
there has been a need to determine which rules are in-
teresting. Originally this started with using the concepts
                                                              3   Concepts and Calculations of The Pro-
of support and confidence [1]. Since then, many more
                                                                  posed Interestingness Measures
measures have been proposed [2] [3] [6]. The Support-
Confidence approach is appealing due to the antimono-         In this section we present the key parts of the theory
tonicity property of the support. However, the support        and background and formula behind our proposed mea-
component will ignore itemsets with a low support even        sures. We also present the assumptions we have made
though these itemsets may generate rules with a high          for our measures.
confidence (which is used to indicate the level of inter-
estingness) [6]. Also, the Support-Confidence approach        3.1    Assumptions and Definitions
does not necessarily ensure that the rules are truly in-
                                                              Here we outline the assumptions we have made. Figure
teresting, especially when the confidence is equal to the
                                                              1 depicts an example of the general structure of a multi-
marginal frequency of the consequent [6]. Based on this
                                                              level dataset. As shown, there is a tree-like hierarchi-
argument, other measures for determing the interesting-
                                                              cal structure to the concepts or items involved in the
ness of a rule is needed.
                                                              dataset. Thus items at the bottom are descendant from
    Broadly speaking, all of these existing measures fall
                                                              higher level items. An item at a higher level can contain
into three categories; objective based measures (based
                                                              multiple lower level items.
on the raw data), subjective based (based on the raw
data and the user) and semantic based measures (based
on the semantic and explanations of the patterns) [3].
    In the survey presented in [3] there are nine criteria
listed that can be used to determine if a pattern or rule
is interesting. These nine criteria are; conciseness, cov-
erage, reliability, peculiarity, diversity, novelty, surpris-
ingness, utility and actionability or applicability. The
first five criteria are considered to be objective, with the
                                                                    Figure 1: Example of a multi-level dataset.
next two, novelty and surprisingness being considered
to be subjective. The final two criteria are considered to        With this hierarchy we have made the two following

be semantic.                                                  assumptions.

    Despite all the different measures, studies and             1. That each step in the hierarchy tree is of equal
works undertaken, there is no widely agreed upon                   length / weight. Thus the step from 1-*-* to 1-
formal definition of what interestingness is in the                1-* is of equal distance to the step from 2-*-* to
context of patterns and association rules [3].         More        2-1-* or 1-1-* to 1-1-1.
recently several surveys of interestingness measures
have been presented [3] [6] [7] [8].        One survey [8]      2. That the order of sibling items is not important
evaluated the strengths and weaknesses of various                  and the order could be changed (along with any
measures from the point of view of the level or extent             descendants) without any effect.
of user interaction.      Another survey [7] looked at
                                                                3. That    each   concept/item     has   an   ancestor
classifying various interestingness measures into five
                                                                   concept/item (except for the root) so that no
formal and five experimental classes, along with eight
                                                                   concepts/items or group(s) of concepts/items are
evaluation properties.     However, all of these surveys
                                                                   isolated from the rest of the hierarchy.
result in different outcomes over how useful, suitable
etc., an interestingness measure is.         Therefore the        Before presenting our proposed measures we firstly
usefulness of a measure can be considered to be               define several terms and formula used.
subjective.
                                                                · n1 and n2: represent two items / concepts in the
    All of these measures mentioned above are for rules
                                                                   multi-level dataset.
derived from single level datasets. They work on items
on a single level but do not have the capacity for com-         · ca: (common ancestor) is the closest item that is
paring different levels or rules containing items from             an ancestor to both n1 and n2.
multiple levels simultaneously. Our research has found
that up to now, little work has been done when it comes         · TreeHeight: is the maximum number of items on a
                                                                   path in the multi-level dataset (not counting root)
to interestingness measures for multi-level datasets that
                                                                   from the root to a item located at the lowest con-
can handle items from muliple levels within one rule or
                                                                   cept level.
rule set.
    Here in our work we propose to measure the inter-           · h: represents the entire multi-level dataset hierar-
estingness of multi-level rules in terms of diversity and
                                                                   chy.


                                                              28

   · Hierarchy level of an item: the hierarchy level of        Where  and  are weighting factors such that  +
     the root is 1. The hierarchy level of an item in the   = 1. The values of  and  need to be determined
     dataset is larger than the level of its direct parent experimentally and for our experiments are both set at
     by 1.                                                 0.5. Equation 2 & 3 consists of two parts, the average
                                                           hierarchical relationship distance and the concept dis-
   · Number of Levels Difference:
                                                           tance among the items in the rule, respectively. In the
       NLD(x, y)       =   | hierarchy level of x -        following subsections we will define the two aspects in

                           hierarchy level of y | (1)      detail.


     is the number of hierarchy levels difference be-      3.2.1   Hierarchical Relationship Distance

     tween items x and y.                                  The HRD of two items measures how close two items
                                                           are in terms of a hierarchical relationship from a com-
3.2    Diversity
                                                           mon ancestor item (or root). The further apart they are
Here we define a diversity measure for multi-level as-     in a hierarchical relation; that is the greater the number
sociation rules which takes items' structural informa-     of concept levels difference between two items and their
tion into consideration. The diversity defined here is a   common ancestor, the more diverse the two items are
measure of the difference or distance between the items    and the more diverse the rule will be.
within a rule, based on their positions in the hierarchy.      Here for the HRD component, diversity is inversely
Two different aspects of the items in a rule are consid-   related to the closeness of items in terms of a hierar-
ered to measure the diversity of the rule.                 chical relationship. The closer the two items are, the
                                                           less diverse they are. The further / more distant the
  1. Hierarchical relationship distance (HRD) between
                                                           relationship, the more diverse. For maximum HRD di-
     items.
                                                           versity the two items need to have no common ancestor
  2. Concept level distance (LD) between items.            and both be located at the lowest concept level in the
                                                           dataset.
   We propose that the diversity of a rule can be mea-
                                                               HRD focuses on measuring the horizontal (or
sured using two different approaches. The first, mea-
                                                           width) distance between two items. Usually the greater
sures the overall diversity of a rule by combining the
                                                           the horizontal distance, the greater the distance to a
items in the antecedent with the items in the consequent
                                                           common ancestor and therefore the more diverse the
into a single set.   If the items within this combined
                                                           two items are.     Due to the second assumption, we
itemset are very different, then the rule will have a high
                                                           can not measure the horizontal distance without also
overall diversity, regardless of whether the items were
                                                           utilising the vertical (height) distance.
from the antecedent or consequent.
                                                               Thus to determine the Hierarchical Relationship
   Let R be a rule with n items and DOR denotes the
                                                           Distance (HRD) component of the diversity the
overall diversity of R, the diversity of R can be deter-
                                                           following is proposed:
mined as follows:
                     n   -1  n
                  1                   HRD(i, j)              HRD(n1, n2) =      (NLD(n1, ca) + NLD(n2, ca))
    D        =          i=1    j=i+1               +
       OR                    n(n - 1)                                                    2 × T reeHeight
                     n      n                                                                                      (4)
                  1      -1           LD(i, j)
                        i=1    j=i+1                           The Hierarchical Relationship Distance between
                                                       (2)
                           n(n - 1)                        two items is defined as the ratio between the average
                                                           number of levels between the two items and their
   The second, measures the diversity between the
                                                           common ancestor and the height of the tree.           Thus
items in the antecedent and those in the consequent.
                                                           if two items share a direct parent, the HRD value
Those rules which have a high difference between their
                                                           of the two items becomes the lowest value which
antecedent itemsets and consequent itemsets will have      is 1/T reeHeight, while if the two items have no
a high antecedent-consequent diversity. However, this
                                                           common ancestor or their common ancestor is the
approach does not consider the difference between
                                                           root, the HRD values of the two items can score high.
items within the antecedent and/or consequent like the
                                                           Maximum HRD value, which is 1, is achieved when the
overall diversity approach.
                                                           two items have no common ancestor (or the common
   Let R be a rule R : A  C, with n items in A             ancestor is the root) and both items are at the lowest
and m items in C and DACR denotes the antecedent           concept level possible in the hierarchy. If n1 and n2
to consequent diversity of R, the diversity of R can be    are the same item, then HRD becomes 1/T reeHeight.
determined as follows:
                       n   -1 m                            3.2.2   Concept Level Distance
                    2                HRD(i, j)
     D        =           i=1    j=1              +
       ACR                    n(n - 1)                     This aspect is based on the hierarchical levels of the two
                       n      m                            items. The idea is that the more levels between the two
                    2      -1        LD(i, j)
                         i=1     j=1                   (3) items, the more diverse they will be. Thus two items on
                            n(n - 1)

                                                           29

the same hierarchy level are not very diverse, but two        datasets.   The original measure is a syntax-based
items on different levels are more diverse as they have       distance metric in the following form:
different degrees of specificity or abstractness.               P (R1, R2) =      1 × |(X1  Y1)(X2  Y2)|+
    LD differs from HRD in that HRD measures the dis-
tance from a common ancestor item (or root), whereas                              2 × |X1X2| + 3 × |Y1Y2| (6)
LD measures the distance between the two items them-              The  operator denotes the symmetric difference
selves. LD focuses on measuring the distance between          between two item sets, thus XY is equivalent to X -
two items in terms of their height (vertical) difference      Y  Y - X. 1, 2 and 3 are the weighting factors
(HRD considers the width (horizontal) distance).              to be applied to different parts of the rule. Equation 6
    Thus, we propose to use the ratio between the level       measures the peculiarity of two rules by a weighted sum
difference (NLD) of two items and the height of the           of the cardinalities of the symmetric difference between
tree (eg. the maximum level difference) to measure the        the two rule's antecedents, consequents and the rules
Level Distance of the two items as defined as follows:        themselves.
                                                                  We propose an enhancement to this measure to al-
           LD(n1, n2) =        NLD(n1, n2)
                                                         (5)  low it to handle a hierarchy. Under the existing mea-
                            (T reeHeight - 1)
                                                              sure, every item is unique and therefore none share any

    This means that two items on the same concept level       kind of 'syntax' similarity.    However, we argue that

will have a LD of 0, while an item at the highest concept     the items 1-*-*-*, 1-1-*-*, 1-1-1-* and 1-1-1-1 (based

level and another at the lowest concept lvel will have an     on Figure 1) all have a relationship with each other.

LD of 1, as they are as far apart as possible in the given    Thus they are not completely different and should have

hierarchy.                                                    a 'syntax' similarity due to their relation through the
                                                              dataset's hierarchy.
3.3     Peculiarity                                               The greater the P (R1, R2) value is, the greater the
                                                              difference (thus lower similarity) and so the greater
Peculiarity is an objective measure that determines how
                                                              the distance between those two rules. Therefore, the
far away one association rule is from others. The fur-
                                                              further apart the relation is between two items, the
ther away the rule is, the more peculiar. It is usually
                                                              greater the difference and distance. Thus if we have,
done through the use of a distance measure to determine
                                                              R1 : 1 - 1 - 1 -   1 -  -  - 
how far apart rules are from each other. Peculiar rules
                                                              R2 : 1 - 1 -  -   1 -  -  - 
are usually few in number (often generated from outly-
                                                              R3 : 1 - 1 - 1 - 1  1 -  -  - 
ing data) and significantly different from the rest of the
                                                              We believe that the following should hold; P (R1, R3) <
rule set. It is also possible that these peculiar rules can
                                                              P (R2, R3) as 1-1-*-* and 1-1-1-1 are further removed
be interesting as they may be unknown. One proposal
                                                              from each other than 1-1-1-* and 1-1-1-1.
for measuring peculiarity is the neighbourhood-based
                                                                  The difference between any two hierarchically re-
unexpectednedd measure first proposed in [2]. In this
                                                              lated items / nodes must be less than 1. Thus (for the
proposal it is argued that a rule's interestingness is influ-
                                                              above rules) 1 > P (R2, R3) > P (R1, R2) > 0. In
enced by the rules that surround it in its neighbourhood.
                                                              order to achieve this we modify Equation 6 by calculat-
    The measure is based on the idea of determining
                                                              ing the diversity of the symmetric difference between
and mesuring the symmetric difference between two
                                                              two rules instead of the cardinality of the symmetric
rules, which forms the basis of the distance between
                                                              difference. The cardinality of the symmetric difference
them. From this it was proposed [2] that unexpected
                                                              measures the difference between two rules in terms of
confidence (where the confidence of a rule R is far from
                                                              the number of different items in the rules. The diversity
the average confidence of the rules in R's neighbour-
                                                              of the symmetric difference takes into consideration the
hood) and sparsity (where the number of mined rules in
                                                              hierarchical difference of the items in the symmetric
a neighbourhood is far less than that of all the poten-
                                                              difference to measure the difference of the two rules.
tial rules for that neighbourhood) could be determined,
                                                              We recite Equation 2 in terms of a set of items below,
measured and used as interestingness measures [2] [3].
                                                              where S is a set containing n items:
    This measure [2] for determing the symmetric dif-                               n       n
ference was developed for single level datasets where                                    -1
                                                                                                    HRD(i, j)
                                                                  P D(S)    =          i=1    j=i+1             +
each item was equally weighted. Thus the mesure is                                          n(n - 1)
actually a count of the number of items that are not                                n   -1  n
                                                                                                    LD(i, j)
common between the two rules. In a multi-level dataset,                                i=1    j=i+1                (7)
each item cannot be regarded as being equal due to the                                      n(n - 1)
hierarchy. Thus the measure proposed in [2] needs to be           Thus the neighbourhood-based distance measure
enhanced to be useful with these datasets. Here we will       between two rules shown in Equation 6 now becomes;
present an enhancement as part of our proposed work.
    We believe it is possible to take the distance            P M(R1, R2) = 1 × P D((X1  Y1)(X2  Y2))+

measure presented in [2] and enhance it for multi-level                   2 × P D(X1X2) + 3 × P D(Y1Y2)
                                                                                                                       (8)


                                                              30

    Let RS be the ruleset of {R1, R2, ..., Rn} then the  be difficult as the vast majority have very similar sup-
average distance of a rule Ri to the ruleset RS can be   port values. This would mean the more interesting or
determined by:                                           important rules would be lost.
                n                                            The confidence curve shows that the rules are spread
                                   P M(R , R )           out from 0.5 (which is the minimum confidence thresh-
     P M     =     RjRS and j=i            i  j
         ave                                         (9)
                           |RS| - 1                      old) up to close to 1. The distribution of rules in this
                                                         area is fairly consistant and even, ranging from as low
4    Experimental Results                                as 2,181 rules for 0.95 to 1, to as high as 4,430 rules for
                                                         0.85 to 0.9. Using confidence to determine the interest-
In this section we present experimental results of our   ing rules is more practical than support, but still leaves
proposed interestingness measures being used for asso-   over 2,000 rules in the top bin.
ciation rule discovery from a multi-level dataset.           The overall diversity curve shows that the majority
                                                         of rules (23,665) here have an average overall diversity
4.1    Dataset and Setup
                                                         value of between 0.3 to 0.4. The curve however, also
The dataset used for our experiments is a real world     shows that there are some rules which have an over-
dataset,  the BookCrossing dataset (obtained from        all diveristy value below the majority, in the range of
http://www.informatik.uni-freiburg.de/    cziegler/BX/)  0.15 to 0.25 and some that are above the majority, in
[10].    From this dataset we built a multi-level        the range of 0.45 up to 0.7. The rules located above
transactional dataset that contains 92,005 user records  the majority are different to the rules that make up the
and 960 leaf items, with 3 concept / hierarchy levels.   majority and could be of interest as these rules have a
    To discover the frequent itemsets we use the         high overall diversity.
MLT2 L1 algorithm proposed in [4] [5] with each              The antecedent-consequent diversity curve is simi-
concept level having its own minimum support.            lar to that of the overall diversity. It has a similar spread
From these frequent itemsets we then derive the          of rules, but the antecedent-consequent diversity curve
frequent closed itemsets and generators using the        peaks earlier at 0.3 to 0.35 (where as the overall diver-
CLOSE+ algorithm proposed in [9]. From this we then      sity curve peaks at 0.35 to 0.4), with 12,408 rules. The
derive the non-redundant association rules using the     curve then drops down to a low number of rules at 0.45
MinMaxApprox (MMA) rule mining algorithm [9].            to 0.5, before peaking again at 0.5 to 0.55, wih 2,564
                                                         rules. The shape of this curve with that of the overall di-
4.2    Results                                           versity seems to show that the two diversity approaches

For the experiment we simply use the previously men-     are related. Using the antecedent-consequent diversity

tioned rule mining algorithm to extract the rules from   allows rules with differing antecedents and consequents

the multi-level dataset. For this experiment we assign   to be discovered when support and confidence will not

a reducing minimum support threshold to each level.      identify them.

The minimum supports are set to 10% for the first hi-        Lastly, the distance curve shows the largest spread

erarchy level, 7.5% for the second and 5% for the thrid  of rules across a curve. There are rules which have a

level (the lowest). During the rule extraction process   low distance from the rule set (0 to 0.1 which corre-

we determine the diversity and peculiarity distance of   sponds to a distance of 33,903.7 to 35,899.56) up to

the rules that meet the confidence threshold. With two   higher distances (such as 0.7 and above which corre-

measures known for each rule, we are also able to de-    sponds to a distance of 47,874.88 to 53,862.52). The

termine the minimum, maximum and average diversity       distance curve peaks at 0.3 to 0.35 (which is a distance

and peculiarity distance for the rule set.               of between 39,891.35 and 40,889.29). Using the dis-
                                                         tance curve to find interesting rules allows those that
4.2.1   Statistical Analysis                             are close to the ruleset (small distance away) or those
                                                         that are much further away (greater distance) to be dis-
Firstly, we compare the distribution curves of the pro-
                                                         covered.
posed measures (diversity and distance) against the dis-
                                                             Next, we look at the trends of the various measures
tribution curves of support and confidence for the rule
                                                         when compared against the proposed diversity and dis-
set. The distribution curves are shown in Figure 2. The
                                                         tance measures.
value of each measure ranges from 0 to 1. The values
                                                             Figure 3 shows the trend of the average support, av-
of the distance measure are based on the minimum dis-
                                                         erage confidence, average antecedent-consequent diver-
tance (in this case 33,903.7) being equal to 0 and the
                                                         sity and average distance values against that of overall
maximum distance (in this case being 53,862.5) being
                                                         diversity. As can be seen the average support remains
equal to 1. The range between these two has been uni-
                                                         fairly constant. There is tendancy for the support to
formly divided into 20 bins.
                                                         increase for those rules with a high overall diversity.
    As Figure 2 shows, the support curve shows that
                                                         Even so, this shows that support does not always agree
the majority of association rules only have a support
                                                         with overall diversity, so an overall diversity measure
of between 0.05 and 0.1. Thus for this dataset distin-
                                                         can be useful to find a different set of interesting rules.
guishing interesting rules based on their support would


                                                         31

          Figure 2: Distribution curves for the proposed interestingness measures, support and confidence.




                     Figure 3: Trends of measures against the proposed overall diversity measure.


    The confidence in Figure 3 is also fairly constant        upwards trend as the distance increases) in Figure 5
(usually varying by less than 0.1) until the end. Again       would indicate that potentially the more overall diverse
this shows that the confidence will not always discover       rules have a higher distance from the rest of the rule set
those rules that are more diverse overall.                    and therefore are further away. This would also imply
    The average antecedent-consequent diversity tends         that those rules with a higher distance are usually more
to have a consistant upward trend as the overall diver-       diverse overall as well.
sity increases. This shows that both the overall diversity       Figure 4 shows the trends of average support, aver-
and antecedent-consequent diversity are related/linked        age confidence, average overall diversity and average
(which is not unexpected). It is quite possible that the      distance against that of antecedent-consequent diver-
greatest degree of diversity for a rule comes from com-       sity. Like in Figure 3, the support remains fairly con-
paring the items in the antecedent against those in the       stant regardless of the antecedent-consequent diversity
consequent and not from comparing the items within            value.
just the antecedent and/or consequent.                           The confidence tends to decrease as the antecedent-
    The distance has an overall upwards trend, although       consequent diversity increases, so the more diverse
it is not a constant rate nor consistant (as there is a small rules will not always be picked up by confidence.
decrease from 0.2 to 0.3). This, along with the trend of         The    overall   diversity  tends   to   increase   as
the average overall diversity (which shows a consistant       antecedent-consequent     diversity  increases   (similar


                                                              32

            Figure 4: Trends of measures against the proposed antecedent-consequent diversity measure.




                        Figure 5: Trends of measures against the proposed distance measure.


to Figure 3). So again the biggest diversity in a rule         Like the average overall diversity, the average
is often in the difference between the antecedent and     antecedent-consequent diversity also trends upwards
consequent.                                               as the distance increases. The rate of rise is similar to
    The distance also tends to increase (gradually at     that of the overall diversity initially at lower distance
first for lower antecedent-consequent diversity values).  values, but becomes much steeper at high distance
There is a big jump in the distance trend when the        values.   This shows that it seems the most distant
antecedent-consequent diversity increases from 0.65 to    rules also have the highest diversity between their
0.75. The highest distance values are achieved when       antecedent and consequent as Figure 5 shows the
the antecedent-consequent diversity reaches its highest   average antecedent-consequent diversity to be over 0.8
values (this is also shown in Figure 5).                  for the rules with the highest distance from the rest of
    Figure 5 shows the trend of the average support, av-  the rule set.
erage confidence, average overall diversity and average        The confidence trend in Figure 5 also shows that
antecedent-consequent diversity values against that of    confidence will not always discover those rules far away
distance. As shown, the support remains very constant     from the rule set (0.8 / 49,870.76 and above), as at
regardless of the distance. This shows that support can   these distances the confidence values are at their lowest
not be used to discover rules that have a low or high     points. For rules with a low distance value, confidence
peculiarity distance.                                     may also not be the best measure as at these values


                                                         33

(0 / 33,903.7 to 0.1 / 35,899.58) confidence values are     datasets. These proposed interestingness measures are
not at their highest. The highest confidence value(s)       diversity and peculiarity (distance) respectively.
occur when the distance is 47,874.86 to 48,872.82 (0.7          Diversity is a measure that compares items within a
to 0.75).                                                   rule and peculiarity compares items in two rules to see
                                                            how different they are.
4.2.2   Examples of Proposed Measures                           In our experiments we have shown how diversity

If we look closer at the discovered rules we find the       and peculiarity distance can be used to identify poten-

following examples that show how diversity and pecu-        tially interesting rules that normally would not be con-

liarity distance can be useful in identifying potentially   sidered as interesting using the traditional support and

interesting rules that would not normally be identified     confidence approach.

as such. (Note that the hypen breaks the concept levels,    Acknowledgements          Computational      resources    and
while a comma indicates a new item).                        services used in this work were provided by the HPC
    Example 1:                                              and Research Support Unit, Queensland University of
    R1=BookClubs-Lit.&Fiction-Pop.Fiction                   Technology, Brisbane, Australia.
Subjects-Lit.&Fiction-General
    Supp 12.228% Conf 81.5% OverallDiv 0.5                  References
    R2=BookClubs-Lit.&Fiction-Pop.Fiction               
                                                              [1] R. Agrawal, T. Imielinski and A. Swami.          Mining
Subjects-Mystery&Thrillers
                                                                  Association Rules between Sets of Items in Large
    Supp 7.9% Conf 52.67% OverallDiv 0.67
                                                                  Databases. In ACM SIGMOD International Conference
R1 has a higher support and confidence than R2, but               on Management of Data (SIGMOD'93), pages 207­216,
R2 has a higher overall diversity. If we used either the          Washington D.C., USA, May 1993.
support or confidence measure then R1 would always            [2] G. Dong and J. Li.       Interestingness of Discovered
be chosen as the more interesting rule. However, our
                                                                  Association Rules in terms of Neighbourhood-Based
proposed overall diversity measure indicates that R2 is           Unexpectedness. In Second Pacific-Asia Conference on
more interesting due to its diversity score, which can be         Knowledge Discovery and Data Mining (PAKDD'98),
attributed to its more general consequent.                        pages 72­86, Melbourne, Australia, April 1998.
    Example 2:
                                                              [3] L. Geng and H. J. Hamilton. Interestingness Measures
    R3=Subjects-Biographies&Memoirs-General,                      for Data Mining: A Survey. ACM Computing Surveys
Subjects-Lit.&Fiction-Authors(A..Z)  BookClubs-                   (CSUR), Volume 38, pages 9, 2006.
Lit.&Fiction
                                                              [4] J. Han and Y. Fu. Discovery of Multiple-Level Associa-
    Supp 5.59% Conf 60.9% Ant-ConDiv 0.67
                                                                  tion Rules from Large Databases. In 21st International
R3 has low support and reasonbly low confidence, but              Conference on Very Large Databases (VLDB'95), pages
it has high antecedent-consequent diversity (the aver-            420­431, Zurich, Switzerland, September 1995.
age is 0.35). If we use support or confidence this rule
                                                              [5] J. Han and Y. Fu. Mining Multiple-Level Association
will probably not be chosen as interesting as its support
                                                                  Rules in Large Databases.       IEEE Transactions on
value is lower than the average support value for this            Knowledge and Data Engineering, Volume 11, pages
rule set (5.8%) and its confidence is relatively low and          798­805, 1999.
is also lower than the average confidence of the rule
                                                              [6] S. Lallich, O. Teytaud and E. Prudhomme. Association
set (74.4%). However, if we use antecedent-consequent
                                                                  rule interestingness: measure and statistical validation.
diversity, then it will be selected as it has a high value.       Quality Measures in Data Mining, Volume 43, pages
Hence this rule may be of interest because of the di-             251­276, 2006.
versity between its antecedent and consequent itemsets,
                                                              [7] P. Lenca, B. Vaillant, B. Meyer and S. Lallich.      As-
which come from different branches of the hierarchy.
                                                                  sociation rule interestingness: experimental and theo-
    Example 3:                                                    retical studies. Studies in Computational Intelligence,
    R4=BookClubs,       Subjects-Lit.&Fiction-WorldLit.           Volume 43, pages 51­76, 2007.
 Subjects-Lit.&Fiction-GenreFiction,             Subjects-
                                                              [8] K. McGarry.     A Survey of Interestingness Measures
Mystery&Thrillers
                                                                  for Knowledge Discovery. The Knowledge Engineering
    Supp 6.7% Conf 57.7% Dist 50,311.4                            Review, Volume 20, pages 39­61, 2005.
R4 has a noticably higher than average distance and is
                                                              [9] N. Pasquier, R. Taouil, Y. Bastide and G. Stumme.
much further away from the rule set. This may be of
                                                                  Generating a Condensed Representation for Association
interest to a user. But if support and confidence are
                                                                  Rules.    Journal of Intelligent Information Systems,
used, this rule is considered to not be of interest due           Volume 24, pages 29­60, 2005.
to their low values.
                                                            [10] C.-N. Ziegler, S. M. McNee, J. A. Konstan and
                                                                  G. Lausen. Improving Recommendation Lists Through
5    Conclusion                                                   Topic Diversification. In 14th International Conference
                                                                  on World Wide Web (WWW'05), pages 22­32, Chiba,
In this paper we have proposed two interestingness
                                                                  Japan, May 2005.
measures for association rules derived from multi-level



                                                            34

Do Users Find Looking at Text More Useful than Visual Representations?
                    A Comparison of Three Search Result Interfaces

                Hilal Al Maqbali          Falk Scholer     James A. Thom           Mingfang Wu

                       School of Computer Science and Information Technology
                                                 RMIT University
                                      GPO Box 2476, Melbourne 3001
                                                Victoria, Australia
    h.almaqbali@student.rmit.edu.au,{falk.scholer,james.thom,mingfang.wu}@rmit.edu.au


Abstract                                                   text summaries, clustering information, and visual
   The organisation, content and presentation of doc-      thumbnail images:
ument surrogates has a substantial impact on the effec-
                                                           C Carrot2 (http://www.carrot2.org),
tiveness of web search result interfaces. Most interfaces
include textual information, including for example the
                                                           M Middlespot (http://www.middlespot.com), and
document title, URL, and a short query-biased sum-
mary of the content. Other interfaces include additional   N Nexplore (http://www.nexplore.com)
browsing features, such as topic clustering, or thumb-
                                                               A preliminary analysis of overall task completion
nails of the web pages. In this study we analyse three
                                                           time with the different interfaces was presented in a
search interfaces, and compare the effectiveness of tex-
                                                           previous paper [1]. In this paper, we investigate how
tual information and additional browsing features. Our
                                                           much time users spent looking at different regions of
analysis indicates that most users spend a substantially
                                                           the screen, in particular comparing the time spent look-
larger proportion of time looking at text information,
                                                           ing at text surrogates of the result pages with time spent
and that those interfaces that focus on text-based rep-
                                                           looking at more visual representations. Our analysis
resentations of document content tend to lead to quicker
                                                           indicates that most users find text surrogates to be more
task completion times for named-page finding search
                                                           useful.
tasks.
                                                               The remainder of this paper is organised as follows.
Keywords     Information     Retrieval,   User    Studies  Some related work is presented in Section 2; our exper-
Involving Documents, Web Documents, Eye Tracking           iment design, including the different search interfaces,
                                                           users, and topics used, is described in Section 3; the
1    Introduction                                          results of the experiment are analysed in Section 4; and
                                                           discussion and conclusions are given in Section 5.
Search engines are a key tool for supporting users in
finding information on the world wide web. These in-
formation retrieval systems aim to find relevant docu-     2    Related Work
ments in response to a user query. While the perfor-       The presentation of search results influences users' as-
mance of the underlying ranking function ­ responsi-       similation and guides users to look for the information
ble for identifying good answer resources ­ is clearly     that is relevant to them. In the past, quite a few studies
of great importance, the organisation, content and pre-    explored visual presentation of search results [3, 10,
sentation of document surrogates in the search results     11]. A proper visual representation can communicate
interface can also have a substantial impact on overall    some kinds of information much more rapidly and ef-
search effectiveness.                                      fectively than textual representation. However, visual-
   One recent study [9] found that only 21% of users       isation of textually represented information is difficult
found relevant results when querying a search engine,      and challenging [6].
and that 75% were disappointed with the results re-            The effectiveness of visual representations largely
turned. The way users interact with the search result      depends on whether the representation is highly cou-
interface may be one factor in the poor user experience.   pled with a search task and on the inherent structure
   This    paper   analyses    three  search   interfaces  of documents to be presented. Joho and Jose [8] in-
that  make    use   of    different  features   including  vestigated how textual and visual forms of information
                                                           enabled users to more effectively interact with search
Proceedings of the 14th Australasian Document Comput-
                                                           answer interfaces in undertaking relevance assessments
ing Symposium, Sydney, Australia, 4 December 2009.
                                                           and reformulating queries.
Copyright for this article remains with the authors.




                                                         35

    Cutrell and Guan [4] found that adding extra con-               Interface Features      C        M       N
textual information to the document surrogates can im-              Text features         66%     17%     56%
prove the effectiveness on search answer interfaces for             Browse features       19%     75%      7%
informational tasks. Hearst and Pedersen [7] is one of              Other regions         16%      8%     37%
many studies that has investigated the effectiveness of
clustering search results. Compared with the results of           Table 1: The distribution of interface features.

the study described in this paper, where we find the Car-
rot2 interface that supported clustering to be ineffective   documents. Nexplore has more visual features such as
when undertaking a navigational task searching for a         highlighting of query terms, thumbnails, background
single correct answer, they found clustering of answers      colour and highlighting the abstract of the retrieved
was effective in supporting a user's task that involved      document when the mouse is moved over it.
finding a set of relevant answer documents.                     In this paper, we consider the following areas within
    A previous study by Dziadosz and Chandrasekar [5]        each interface page displaying the ranked list of an-
had found that the combination of thumbnails and text        swers:
summary to be more effective for users than either
                                                             Surrogate text: Search engines provide surrogates for
thumbnails or text summaries alone.        However, our
                                                                  answer page in the ranked list of answers. This
study suggests that the combination of thumbnails and
                                                                  surrogate text may include the URL of the answer,
text is only effective when they are not large, since
                                                                  as well as text from the answer web page title,
users mostly look at the text summaries and it is not
                                                                  and a synopsis of the answer web page. The sur-
effective to use too much of the screen real estate on
                                                                  rogate text for the answer documents is in each
the images of answer pages.
                                                                  of the regions marked (1) on the respective an-
                                                                  swer interfaces: Figure 1 for Carrot2 (accounting
3    Experimental methodology
                                                                  for approximately 66% of the screen), Figure 2
To investigate the relative attention that users pay to           for Middlespot (17%), and Figure 3 for Nexplore
different interface components, we conducted a user               (56%).
study that involved carrying out a series of named-page
                                                             Browse features: The visual browse features for the
finding search tasks using a variety of search interfaces.
                                                                  answer documents are in each of the regions
3.1    User study                                                 marked (2) on respective answer interfaces.
                                                                  Figure 1 shows the clustering area in Carrot2
Our study was carried out at RMIT University Open
                                                                  which occupies approximately 19% of the screen.
Day in August 2009. Subjects participated in the exper-
                                                                  Figure 2 shows large images of the answer pages
iment were mostly high school students with an interest
                                                                  that are displayed in Middlespot and occupying
in computer science who were visitors to our laboratory.
                                                                  approximately 75% of the screen. Figure 3 shows
Participants were given a plain language statement out-
                                                                  a much smaller region, approximately 7% of the
lining the goals of the experiment, the types of tasks
                                                                  screen, containing the thumbnails displayed by
to be undertaken, and the data that would be collected.
                                                                  Nexplore.
Based on this information, 35 volunteers chose to par-
ticipate in the experiments. No training was given with      Other regions: Each interface also had some other
the different search interfaces.                                  regions, such as banners and the surrounding
    Each    participant  undertook   three   navigational         screen, including a region at the bottom of the
search tasks (described below), using different search            screen (not shown in the figures) that contained
interfaces. Information about visual attention given to           the topic and some instructions, This accounted
the different screen components was collected using               for approximately 16% of the screen with the
a Tobii T60 eye tracker.      This non-intrusive device           Carrot2 interface, 8% with Middlespot interface,
records the gaze position, providing information on               and 37% with Nexplore (since this last interface
fixations and saccades (brief rapid eye movements).               included a separate area for Wiki Search).

3.2    Search interface features                                As summarised in Table 1, significant portions of
                                                             the Carrot2 and Nexplore interfaces are given to surro-
Our experiment involved users using three different
                                                             gate text. The great majority of the Middlespot inter-
search   result   interfaces  that  contained    different
                                                             face, on the other hand, is occupied by visual browse
amounts of surrogate text and visual browse features
                                                             features.
about answer documents on the result pages.
    The three interfaces were selected because they          3.3   Topics
provide a variety of additional novel features, not just
                                                             One taxonomic study [2] shows that web search tasks
a ranked list of text extracts. Carrot2 does not present
                                                             can be classified as informational, transactional or nav-
visual features, however it clusters its search results. In
                                                             igational. Navigational tasks are used in our study be-
Middlespot, screenshots are presented for the retrieved
                                                             cause we assume that users become more interested in



                                                           36

Figure 1: Carrot2 interface (www.carrot2.org). Areas marked 1 and 2 indicate Text and Browse features,
respectively. Descriptions of the features are provided in the main text.




 Figure 2: Middlespot interface (www.middlespot.com). Descriptions of features are provided in the main text.




   Figure 3: Nexplore interface (www.nexplore.com). Descriptions of features are provided in the main text.




                                                         37

       Trial    1st task      2nd task    3rd task          4.1     Interface features
       1        M- H (4)      C- G (4)    N- A (3)
                                                            Different search interface features attract highly vari-
       2        M- G (4)      C- A (4)    N- H (4)
                                                            able amounts of user attention. Figure 4 shows the pro-
       3        M- A (4)      C- H (4)    N- G (3)
                                                            portions of total viewing time that users spent looking
       4        C- G (3)     N- A (3)     M- H (3)
                                                            at text, browse and other features for each trial (that
       5        C- A (5)     N- H (5)     M- G (5)
                                                            is, over all search interfaces and all users). The solid
       6        C- H (4)     N- G (4)     M- A (3)
                                                            line shows the median time, while the boxes show the
       7        N- A (3)     M- H (3)     C- G (3)
                                                            25th to 75th percentiles. Whiskers show the range of
       8        N- H (3)     M- G (3)     C- A (3)
                                                            the data, with outliers (observations more extreme than
       9        N- G (3)     M- A (3)     C- H (3)
                                                            1.5 times the interquartile range). Since the time data is
                                                            not normally distributed (Shapiro-Wilk, p < 0.0001),
             Table 2: Experimental design.
                                                            we analyse multi-level factors using the Kruskal-Wallis
                                                            test, a non-parametric alternative to ANOVA. Pairswise
using additional web search interface features to get
                                                            comparisons are made using the Wilcoxon signed-rank
their desired information.
                                                            test. The relative times for the different features vary
    For each interface, users were given a navigational     significantly (Kruskal-Wallis, p < 0.0001). In partic-
search task, for which they were asked to find a specific
                                                            ular, users spend significantly more time viewing text
single correct answer page for the given topic. The top-
                                                            features compared to browse features (Wilcoxon, p <
ics were chosen to cover areas that were likely to be of    0.0001) and other (p < 0.0001).        The difference in
interest to young searchers, and where searchers were
                                                            viewing patterns between browse and other is not sig-
unlikely to be hindered due to lack of general knowl-       nificant (p = 0.6504).
edge about the domain. The three topics were:
                                                                Figure 5 shows the median time (over all search
                                                            answer interfaces) users spent looking at different re-
A: Find the ARIA chart of the top 50 music singles in
                                                            gions of the screen, broken down by cases where users
     Australia (query terms: top australia aria)
                                                            identified the correct or incorrect answer document for
G: Find the MSN games website (query term: msn)             each search trial. The text region was the area of the
                                                            screen that users spent most of their time looking at,
H: Find the official homepage of the 2009 movie Harry       users found slightly more correct answers if they spent a
     Potter (query terms: magical potter)                   bit more time in this area; while when users spent more
                                                            time looking at the visual browse regions these were not
    These topics,    and their corresponding answer
                                                            effective and could often lead users to the incorrect an-
documents, represent different aspects of navigational
                                                            swers rather than correct answers. Time spent looking
searches: the answer for the first topic is a single web
                                                            at both text and browse regions is significantly differ-
page presenting the required (named) information; the
                                                            ent between correct and incorrect answers (Wilcoxon,
second is the hub page for a prime sub-part of the
                                                            p = 0.0060 for text regions and p = 0.0303 for browse
overall MSN website; and, the third is the home page
                                                            regions) while the difference is not significant for other
(or index) of an overall website.
                                                            areas of the screen (p = 0.7669).
    After reading a topic, the user would click a "start"
                                                                Figure 6 shows the distribution of the proportion of
button to load the results of issuing the predefined query
                                                            time that users spent viewing different features, split by
terms (as indicated above) into one of the three search
                                                            the three interfaces. For the Carrot2 and Nexplore inter-
interfaces. The user could then interact with the search
                                                            faces, users spent substantially more time viewing the
result screen however they wanted to.
                                                            text features. However, for the Middlespot interface,
    We used a latin square experiment design with a
                                                            the browse features (in this case, the screenshots of web
block of nine trials varying the order in which topics
                                                            pages) attracted the greatest proportion of viewing time.
and interfaces were presented to users, each user was
presented with one topic for each interface.       Due to   4.2     Task completion time
some interruptions and other problems, not all com-
                                                            User task completion performance is evaluated by mea-
binations were completed exactly the same number of
                                                            suring the time taken to carry out a search task to the
times. Table 2 shows the number of times (in parenthe-
                                                            user's satisfaction. That is, we measure the time from
ses) each of the different combinations of interface (C,
                                                            when the search results screen is displayed to the user,
M, N) and topic (A, G, H) were completed as the first,
                                                            until the time that they indicate that they have found a
second or third task undertaken by one of the users.
                                                            desired answer (generally, by clicking on the hyperlink
                                                            in the search results list that they chose as their final
4    Results
                                                            answer). This is in contrast to our previous analysis [1],
We analyse user behaviour when carrying out the three       where task completion time was measured by taking the
search tasks using the Carrot2, Middlespot and Nex-         time that the user chose to exit the task (by explicitly
plore interfaces based on the relative attention paid to    pressing F10) as the endpoint.      This adds additional
different interface features, and task completion time.     variation to the results, since some users spend addi-



                                                          38

                            1.0                                                   


                                                                                  
                                                                                  



                               0.8




                                  0.6
                                                                                                          
                 Time
                     %
                                     0.4




                                        0.2



                                                                      
                                           0.0

                                                                     Text      Browse                  Other



                                              Figure 4: Relative time spent viewing different interface regions.




                                              1.0

                                                                                                        Correct
                                                                                                        Incorrect


                                                 0.8




                                                    0.6

                      Time
                          %

                                                       0.4




                                                          0.2




                                                             0.0

                                                                    Text      Browse                  Other



Figure 5: Median proportion of time spent viewing different regions when users found a correct or incorrect answer.




                                                                             39

                   1.0




                      0.8




                         0.6

                                                                                                                 
            Time                                                                                

                %
                                                                                                                 
                            0.4




                               0.2




                                  0.0


                                           Text   Browse   Other    Text   Browse   Other    Text    Browse    Other

                                                  Carrot2                Middlespot                  Nexplore



                                     Figure 6: Proportion of time spent viewing different components, by interface.


tional time viewing their chosen answer page, before                              Answer       Carrot2    Middlespot     Nexplore
indicating task completion.                                                       Correct        24            18           24
    Figure 7 shows the time taken to find an answer,                              Incorrect       9            14            7
in seconds, for each of the three interfaces.                         The
differences are weakly significant (Kruskal-Wallis,                           Table 3: Distribution of correct answers by interface.

p  =    0.0604).                      In particular, the Middlespot and
Nexplore differ significantly (Wilcoxon, p = 0.0129),                        4.3     Search success
while the other pairs do not (Middlespot and Carrot 2,
                                                                             Users were asked to indicate when they felt that they
p = 0.2474; Nexplore and Carrot2, p = 0.3225).
                                                                             had found the correct answer to the query. However,
    Variation can also be introduced by other sources.
                                                                             in many cases users did not in fact identify the correct
The effect of using different search topics was signifi-
                                                                             resource. Table 3 shows the number of incorrect and
cant (Kruskal-Wallis, p = 0.0330). Moreover, because
                                                                             correct answers found, split by the interface used. The
we used real search interfaces and live search results,
                                                                             results are strongly indicative of higher success rates
the rank of the correct answer items in the search re-
                                                                             with both the Carrot2 and Nexplore interfaces (72.7%
sults lists of the different interfaces varied somewhat.
                                                                             and 77.4% of answers are correct, compared to 56.2%
Although the ranks were similar on average (rank 7.6
                                                                             for Middlespot). However, the differences are not sta-
for Carrot2, 6.3 for Middlespot, and 6.0 for Nexplore)
                                                                             tistically significant (Fisher, p = 0.1746).
this did have a significant effect on task completion
                                                                                 We re-analyse the time taken for task completion,
time (Kruskal-Wallis, p = 0.0048). The different users
                                                                             using only those trials for which users identified the
participating in the experiment were not a significant
                                                                             correct resource in response to the information need.
source of variation (Kruskal-Wallis, p = 0.1227).
                                                                             For these responses, the difference between interfaces
    However, this analysis includes all user responses,
                                                                             is greater, and statistically significant (Kruskal-Wallis,
irrespective of whether the user actually found the cor-
                                                                             p = 0.0077). Differences between the interfaces on a
rect answer required for the query. We investigate this
                                                                             pairwise basis are also more pronounced: the median
next.
                                                                             task completion time with Middlespot at 23.71 seconds
                                                                             is significantly longer than that for Carrot2 at 12.81




                                                                           40

                                                              


                                       0
                                        0
                                         1



                                          0

                                                              


                                           08


                              Seconds

                                             06                                              




                                               04
                                                 2



                                                  0


                                                           Carrot2      Middlespot       Nexplore



                                                   Figure 7: Task completion times by interface.


seconds (Wilcoxon, p = 0.0112) and for Nexplore at                        navigational search tasks, text features are important in
12.21 seconds (Wilcoxon, p = 0.0027). The differ-                         guiding users to finding correct answers quickly.
ence between Carrot2 and Nexplore is not significant                          For the small sample of named-resource finding
(Wilcoxon, p = 0.7360).                                                   search tasks, it appears that text information can be
    Moreover, when considering only those results                         vital in supporting users to find the answers that they
where users successfully identified correct answers,                      need. Whether this would also apply to other search
the effects from topic and user variation are not                         tasks, such as informational tasks, will be the subject
significant (Kruskal-Wallis, p = 0.3445 and 0.2743,                       of future research.
respectively). The rank of the answer item only has a                         In future work we plan to conduct further user stud-
weakly significant effect (Kruskal-Wallis, p = 0.0619).                   ies over a wider range of tasks. We also plan to investi-
                                                                          gate the effect of the proportion of screen space that is
5    Discussion and Conclusions                                           given over to browsing features as a controlled variable
                                                                          (that is, systematically controlling the proportion).
Search result interfaces are an important component of
information retrieval systems, and can have substantial
                                                                          References
impact on overall search task performance. In this pa-
per, we have analysed three publicly available search                      [1] H. Ali [Al Maqbali], F. Scholer, J. A. Thom and M. Wu.
interfaces, and examined how user attention is split be-                       User interaction with novel web search interfaces. In

tween various features that the search providers make                          21st Annual Conference of the Australian Computer-
                                                                               Human Interaction Special Interest Group (CHISIG) of
available.
                                                                               the Human Factors and Ergonomics Society of Australia
    Our   analysis   has    shown                  that   users   spend
                                                                               (HFESA), 2009.
significantly different proportions of time interacting
with text,    browse and other components of the                           [2] A. Border. A taxonomy of web search. ACM SIGIR
                                                                               Forum, Volume 36, Number 2, pages 3­10, 2002.
interfaces.  Not surprisingly, these proportions differ
between the three interfaces; for Nexplore and Carrot2,                    [3] S. K. Card, J. D. Mackinlay and B. Shneideman. Mor-
text is preferred, while for Middlespot (which presents                        gan Kaufmann Publishers, 1999.

much less text to the user) browsing features are viewed                   [4] E. Cutrell and Z. Guan. What are you looking for?: an
more.                                                                          eye-tracking study of information usage in web search.
    We have also analysed how task completion time                             In CHI '07: Proceedings of the SIGCHI conference on
differs between the interfacts, and success rates in iden-                     Human factors in computing systems, pages 407­416,

tifying correct answers for given informatinon needs.                          San Jose, California, USA, April­May 2007.

The results show that users spent significantly longer                     [5] S. Dziadosz and R. Chandrasekar. Do thumbnail pre-
time to interact with the Middlespot interface but found                       views help users make better relevance decisions about
the fewest correct answers. We conclude that, for the                          web search results?     In SIGIR '02: Proceedings of
                                                                               the 25th annual international ACM SIGIR conference




                                                                       41

    on Research and development in information retrieval,
    pages 365­366, Tampere, Finland, August 2002.

 [6] M. Hearst. User Interfaces and Visualisation. Addison-
    Wesley, 1999.

 [7] M. A. Hearst and J. O. Pedersen.       Reexamining the
    cluster hypothesis: scatter/gather on retrieval results. In
    SIGIR '96: Proceedings of the 19th annual interna-
    tional ACM SIGIR conference on Research and devel-
    opment in information retrieval, pages 76­84, Zurich,
    Switzerland, August 1996.

 [8] H. Joho and J. M. Jose.    A comparative study of the
    effectiveness of search result presentation on the web. In
    Advances in Information Retrieval, Proceedings of 28th
    European Conference on IR Research, pages 302­313,
    April 2006.

 [9] R. S. Rele and A. T. Duchowski. Using eye tracking
    to evaluate alternative search results interfaces.       In
    Proceedings of Human Factors and Ergonomics Society
    Annual Meeting, pages 1459­1463, 2005.

[10] B. Shneiderman. The eyes have it: A task by data type
    taxonomy for information visualizations. In Proceed-
    ings of IEEE Symposium on Visual Languages, 1996.

[11] B. Shneiderman.   Extreme visualization: squeezing a
    billion records into a million pixels.    In Proceedings
    of 2008 ACM SIGMOD International Conference on
    Management of Data, pages 3­12, Vancouver, Canada,
    2008.




                                                               42

                                        Random Indexing K-tree

                       Christopher M. De Vries              Lance De Vine           Shlomo Geva

                                      Faculty of Science and Technology
                                    Queensland University of Technology
                                                 Brisbane, Australia

                  chris@de-vries.id.au           l.devine@qut.edu.au             s.geva@qut.edu.au


Abstract Random Indexing (RI) K-tree is the combi-             [10]. The algorithm is particularly suitable to clustering
nation of two algorithms for clustering. Many large            of large collections due to its low complexity. It is
scale problems exist in document clustering. RI K-tree         a hybrid of the B -tree and k-means algorithm. The
                                                                                    +

scales well with large inputs due to its low complexity.       B -tree algorithm is modified to work with multi di-
                                                                 +

It also exhibits features that are useful for managing         mensional vectors and k-means is used to perform node
a changing collection. Furthermore, it solves previ-           splits in the tree. K-tree is also related to Tree Struc-
ous issues with sparse document vectors when using K-          tured Vector Quantization (TSVQ) [9]. TSVQ recur-
tree. The algorithms and data structures are defined,          sively splits the data set, in a top-down fashion, using
explained and motivated. Specific modifications to K-          k-means. TSVQ does not generally produce balanced
tree are made for use with RI. Experiments have been           trees.
executed to measure quality. The results indicate that             K-tree achieves its efficiency through execution of
RI K-tree improves document cluster quality over the           the high cost k-means step over very small subsets of
original K-tree algorithm.                                     the data. The number of vectors clustered during any
                                                               step in the K-tree algorithm is determined by the tree
Keywords RandomIndexing,K-tree, Dimensionality
                                                               order (usually  1000) and it is independent of collec-
Reduction, B-tree, Search Tree, Clustering, Document
                                                               tion size. It is efficient in updating the collection while
Clustering, Vector Quantization, k-means
                                                               maintaining clustering properties through the use of a
                                                               nearest neighbour search tree that directs new vectors
1 Introduction
                                                               to the appropriate leaf node.
The purpose of this paper is to present and analyse                The K-tree forms a hierarchy of clusters. This hi-
the combination of Random Indexing (RI) with                   erarchy supports multi-granular clustering where gen-
the K-tree algorithm. Both RI and K-tree adapt to              eralisation or specialisation is observed as the tree is
changing data and decrease the cost of computationally         traversed from a leaf towards the root or vice versa.
intensive vector based applications. This combination          The granularity of clusters can be decided at run-time
is particularly suitable to the representation and             by selecting clusters that meet criteria such as distortion
clustering of very large document collections.                 or cluster size.
Documents are typically represented in vector
space as very sparse high dimensional vectors. RI              2.1 K-tree and Document Clustering
can reduce the dimensionality and sparsity of this
                                                               The K-tree algorithm is well suited to clustering large
representation. In turn, the condensed representation is       document collections due to its low time complexity.
highly effective when working with K-tree. The paper           The time complexity of building K-tree is O(n log n)
is focused on determining the effectiveness of using           where n is the number of bytes of data to cluster. This
RI with K-tree through experiments and comparative             is due to the divide and conquer properties inherent to
analysis of results.                                           the search tree. De Vries and Geva [5, 6] investigate the
    Sections 2 to 6 discuss K-tree, Random Indexing,           run-time performance and quality of K-tree by compar-
Document Representation, Experimental Setup and Ex-            ing results with other INEX submissions and CLUTO
perimental results respectively. The paper ends with a
                                                               [13]. CLUTO is a popular clustering tool kit used in the
conclusion in Section 7.                                       information retrieval community. K-tree has been com-
                                                               pared to k-means, including the CLUTO implementa-
2 K-tree                                                       tion, and provides comparable quality and a marked in-
                                                               crease in run-time performance. However, K-tree forms
K-tree [6, 1] is a height balanced cluster tree. It was first
                                                               a hierarchy of clusters and k-means does not. Com-
introduced in the context of signal processing by Geva
                                                               parison of the quality of the tree structure will be un-
Proceedings of the 14th Australasian Document Comput-          dertaken in further research. The run-time performance
ing Symposium, Sydney, Australia, 4 December 2009.             increase of K-tree is most noted when a large number
Copyright for this article remains with the authors.           of clusters are required. This is useful in terms of doc-



                                                             43

ument clustering because there are a huge number of
                                                              1.8

topics in a typical collection. The on-line and incre-
                                                              1.6
mental nature of the algorithm is useful for managing
changing document collections. Most clustering algo-
                                                              1.4

rithms are one shot and must be re-run when new data
                                                              1.2
arrives. K-tree adapts as new data arrives and has the
low time complexityof O(log n) for insertion of a single
                                                               1

document. Additionally, the tree structure also allows
                                                              0.8
for efficient disk based implementations when the size
of data sets exceeds that of main memory.
                                                              0.6



2.2 K-tree Definition                                         0.4



K-tree builds a nearest neighbour search tree over a set      0.2

                                                                  0.2    0.4    0.6    0.8    1    1.2    1.4    1.6    1.8
of real valued vectors V in d dimensional space.

                    v  V : v  Rd                                                   Figure 1: Level 1
                                                      (1)

                                                                  The original K-tree algorithm does not modify any
It is inspired by the B -tree where all data records are
                       +
                                                            of the centroids. They are simply the means of the
stored in leaf nodes. Tree nodes, N, consist of a se-
                                                            vectors they represent. The k-means implementation
quence of (vector, child node) pairs of length l. The
                                                            runs to complete convergence and seeds centroids via
tree order, m, restricts the number of vectors stored in
                                                            perturbation of the global mean. To create two seeds
any node to between one and m.
                                                            the global mean is calculated and then the two seeds
                       1  l  m                        (2)   are created by moving away from the mean in opposite
                                                            directions.
                N = (v1, c1), ..., (vl, cl)           (3)
                                                            2.4 K-tree Example
The tree consists of two types of nodes. Leaf nodes
contain the data vectors that were inserted into the tree.  Figures 1 to 3 are K-tree clusters in two dimensions.
Internal nodes contain clusters. A cluster vector is        1000 points were drawn from a random normal distri-
the mean of all data vectors contained in the leaves of     bution with a mean of 1.0 and standard deviation of 0.3.
all descendant nodes (i.e. the entire cluster sub-tree).    The orderof the K-tree, m, was 11. The greydots repre-
This follows the same recursive definition of a B -tree
                                                  +         sent the data set, the black dots represent the centroids
where each tree is made up of a set of smaller sub-trees.   and the lines represent the Voronoi tessellation of the
Upon construction of the tree, a nearest neighbour          centroids. Each of the data points contained within each
search tree is built in a bottom-up manner by splitting     tile of the tessellation are the nearest neighbours of the
full nodes using k-means [14] where k = 2. As the           centroid and belong to the same cluster. It can be seen
tree depth increases it forms a hierarchy of "clusters      that the probability distribution is modelled at different
of clusters" from the root to the above-leaf level.         granularities. The top level of the tree is level 1. It is
The above-leaf level contains the finest granularity        the coarsest grained clustering. In this example it splits
cluster vectors. Each leaf node stores the data vectors     the distribution in three. Level 2 is more granular and
pointed to by the above-leaf level. The efficiency of       splits the collection into 19 sub-clusters. The individual
K-tree stems from the low complexity of the B -tree
                                                  +         clusters in level 2 can only be arrived at through a near-
algorithm, combined with only ever executing k-means        est neighbour association with a parent cluster in level
on a relatively small number of vectors, defined by the     1 of the tree. Level 3 is the deepest level in the tree
tree order, and by using a small value of k.                consisting of cluster centroids. The 4th level is the data
                                                            set of vectors that were inserted into the tree.
2.3 Modifications to K-tree
                                                            2.5 Building K-tree
The K-tree algorithm was modified for use with RI.
This modified version will be referred to as "Modified      The K-tree is constructed dynamically as data vectors
K-tree" and the original K-tree will be referred to as      arrive. Initially the tree contains a single empty root
"Unmodified K-tree".                                        node at the leaf level. Vectors are inserted via a nearest
    All the document vectors created by RI are of unit      neighboursearch, terminating at the leaf level. The root
length in the modified K-tree. Therefore, all centroids     of an empty tree is a leaf, so the first m data vectors are
are normalised to unit length at all times. The k-means     stored in the root, at which point the node becomes full.
used for node splits in K-tree was changed to use ran-      When the m + 1 vector arrives the root is split using
domised seeding and restart if it did not convergewithin    k-means where k = 2, clustering all m + 1 vectors
six iterations. The process always converged quickly in     into two clusters. The two centroids that result from
our experiments; although it is possible to constrain the   k-means are then promoted to become the centroids in
number of restarts we did not find this to be necessary.    a new root. The vectors associated with each centroid



                                                          44

 1.8                                                             full. Figure 4 shows this construction process for a K-
                                                                 tree of order three (m = 3).
 1.6


                                                                 2.6 Sparsity and K-tree
 1.4


                                                                 K-tree was originally designed to operate with dense
 1.2
                                                                 vectors. When a sparse representation is used perfor-
                                                                 mance degrades even though there is significantly less
  1


                                                                 data to process. The clusters in the top levels of the tree
 0.8
                                                                 are means of most of the terms in the collection and are
                                                                 not sparse at all. The algorithm updates cluster centres
 0.6


                                                                 along the insertion path in the tree. Since document
 0.4
                                                                 vectors have very high dimensionality this becomes a
                                                                 very expensive process.
 0.2

     0.2    0.4    0.6    0.8    1    1.2    1.4    1.6    1.8
                                                                     The medoid K-tree [6] extended the algorithm to
                                                                 use a sparse representation and replace centroids with
                      Figure 2: Level 2
                                                                 document examples. This improved run-time perfor-

 1.8                                                             mance and decreased memory usage. Unfortunately it
                                                                 decreased quality when using sparse document vectors.
 1.6
                                                                 The document examples in the root of the tree were al-

 1.4                                                             most orthogonal to new documents being inserted. The
                                                                 documents were unlikely to have meaningful overlap in
 1.2
                                                                 vocabulary.

   1                                                                 The approach taken by De Vries and Geva at INEX
                                                                 2008 [5] is a simple approach to dimensionality reduc-
 0.8
                                                                 tion or feature selection. It is called TF-IDF culling

 0.6                                                             and it is performed by ranking terms. A rank is cal-
                                                                 culated by summing all weights for each term. The
 0.4
                                                                 weights are the BM25 weight for each term in each
 0.2                                                             document. This can also be explained as the sum of the
     0.2    0.4    0.6    0.8    1    1.2    1.4    1.6    1.8   column vector in the document by term matrix. The top
                                                                 n terms with the highest rank are selected, where n is
                      Figure 3: Level 3
                                                                 the desired dimensionality. This works particularlywell
are placed into a child node. This promotion process             with term occurrences due to the Zipf law distribution
has created a new root and two leaf nodes in the tree.           of terms [19]. The collection frequency of a term is
The tree is now two levels deep. Insertion of a new              inverselyproportionalto its rank accordingto collection
data vector follows a nearest neighbour search to find           frequency. Most of the term weights are contained in
the closest centroid in the root. The vector is inserted         the most frequent terms.
into the associated child. When a new vector is in-
serted the centroids are updated recursively along the           3 Random Indexing
nearest neighbour search path, all the way back to the
                                                                 Random Indexing (RI) [18] is an efficient, scalable and
root node. The propagated means are weighted by the
                                                                 incremental approach to the word space model. Word
number of data vectors contained beneath them. This
                                                                 space models use the distribution of terms to create high
ensures that any centroid in K-tree is the mean vector of
                                                                 dimensional document vectors. The directions of these
all the data vectors contained in the associated sub tree.
                                                                 document vectors represent various semantic meanings
This insertion process continues, splitting leaves when
                                                                 and contexts.
they become full, until the root node itself becomes
                                                                     Latent Semantic Analysis (LSA) [7] is a popular
full. K-means is then run on the root node containing
                                                                 word space model. LSA creates context vectors from
centroids. The vectors in the new root node become
                                                                 a document term occurrence matrix by performing Sin-
centroids of centroids. As the tree grows, internal and
                                                                 gular Value Decomposition (SVD). Dimensionality re-
leaf nodes are split in the same manner. The process of
                                                                 duction is achieved through projection of the document
promotion can potentially propagate to cause a full root
                                                                 term occurrence vectors onto the subspace spanned by
node at which point the construction of a new root fol-
                                                                 the vectors with the largest Eigen values in the decom-
lows and the tree depth is increased by one. At all times
                                                                 position. This projection is optimal in the sense that it
the tree is guaranteed to be height balanced. Although
                                                                 minimises the variance between the original matrix and
the tree is always height balanced nodes can contain as
                                                                 the projected matrix. In contrast, Random Indexingfirst
little as one vector. In this case the tree will contain
                                                                 creates randomcontext vectors of lower dimensionality,
many more levels than a tree where each node is half
                                                                 and then combines them to create a term occurrence
                                                                 matrix in the dimensionally reduced space. Each term



                                                               45

                         node                           vector                               child link               k-means performed on enclosed vectors




                                                                        root node

                                                  level 1                                  nodes above the codebook level are clusters of clusters




                                       level 2                                                           nodes above the leaves contain codebook vectors




                              level 3                                                                             leaf nodes contain the data vectors



                                                the dashed parts represent the nearest neighbour search




                                                       Figure 4: K-tree Construction


in the collection is assigned a random vector, and the                                 term matrix D and a term by index-vector matrix I. Al-
document term occurrence vector is then a superposi-                                   ternatively, I can be referred to as a random projection
tion of all the term random vectors. There is no matrix                                matrix. Each row vector in D represents a document,
decomposition and hence the process is efficient.                                      each row vector in I is an index vector, n is the number
    The RI process is conceptually very different from                                 of documents, t is the number of terms and r is the
LSA and does not have the same optimality properties.                                  dimensionality of the reduced spaced. R is the reduced
The context vectors used by RI should optimally be                                     matrix where each row vector represents a document.
orthogonal. Nearly orthogonal vectors can be used and
have been found to perform similarly [4]. These vec-                                                                   Dn     ×t t×r
                                                                                                                                   I          = Rn   ×r        (4)
tors can be drawn from a random Gaussian distribution.
                                                                                             RI has several advantages. It can be performed in-
The Johnson and Linden-Strauss lemma [11] states that
                                                                                       crementally and on-line as data arrives. Any document
if points are projected into a randomly selected sub-
                                                                                       can be indexed (i.e. encoded as an RI vector) inde-
space of sufficiently high dimensionality, then the dis-
                                                                                       pendently from all other documents in the collection.
tances between the points are approximately preserved.
                                                                                       This eliminates the need to build and store the entire
The same topologythat exists in the higher dimensional
                                                                                       document by term matrix. Additionally, newly encoun-
space is reflected in the lower dimensionalrandomlyse-
                                                                                       tered dimensions (terms) in the document collection are
lected subspace. Consequently, RI offers low complex-
                                                                                       easily accommodated without having to recalculate the
ity dimensionality reduction while still preserving the
                                                                                       projection of previously encoded documents. In con-
topological relationships amongst document vectors.
                                                                                       trast, SVD requires global analysis where the number

3.1 Random Indexing Definition                                                         of documents and terms are fixed. The time complexity
                                                                                       of RI is also very attractive. It is linear in the number of
In RI, each dimension in the original space is given a                                 terms in a document and independent of collection size.
randomly generated index vector. The index vectors
are high dimensional, sparse and ternary. Sparsity is                                  3.2 Choice of Index Vectors
controlled via a seed length that specifies the number of
                                                                                       The index vectors used in RI were chosen to be
randomly selected non-zero dimensions. Ternary vec-
                                                                                       sparse and ternary. Ternary index vectors for RI were
tors consist of randomly distributed +1 and -1 values in
                                                                                       introduced by Achlioptas [2] as being well suited for
the non-zero dimensions.
                                                                                       database environments. The primary concern of sparse
    In the context of document clustering, RI can be
                                                                                       index vectors is reducing time and space complexity.
viewed as a matrix multiplication of a document by
                                                                                       Bingham and Mannila [4] run experiments indicating



                                                                                 46

                                                           performs best with dense vectors, such as those pro-
                                                           duced by RI.


                                                           4 Document Representation

                                                           The INEX 2008 XML Mining collection was used to
                                                           complete the experiments. It contains 114,366 docu-
                                                           ments that are a subset of the XML Wikipedia corpus
                                                           [8]. 15 different categories were provided for the docu-
          Figure 5: Random Indexing Example
                                                           ments.

that sparse index vectors do not affect the quality of         Document content was represented with BM25

results. This is not the only choice when creating         [17]. Stop words were removed and the remaining

index vectors. Kanerva [12] introduces binary spatter      terms were stemmed using the Porter algorithm [16].

codes.     Plate [15] explores Holographic Reduced         BM25 is determined by term distributions within each

Representations that consist of dense vectors with         document and the entire collection.       BM25 works

floating point values.                                     with similar concepts as TF-IDF except that is has
                                                           two tuning parameters. The BM25 tuning parameters
3.3 Random Indexing Example                                were set to the same values as used for TREC [17],
                                                           K1 = 2 and b = 0.75. K1 influences the effect of term
In practice, to construct a document vector, the docu-
                                                           frequency and b influences document length.
ment vector is initially set to zero, and then the sparse
                                                               Links were represented using LF-IDF [5]. This re-
index vector for each term in the document is added
                                                           sulted in a document-to-document link matrix. If there
to the document vector. The weight of the added term
                                                           is a link between documents i and j then a value of
index vector may be determined by TF-IDF or another
                                                           one is added to position i,j and j,i in the matrix. If
weighting scheme. When all terms have been added,
                                                           two documents both link to each other a value of two
the document vector is normalised to unit length. There
                                                           is recorded in their respective vectors. Each row vector
is no need to explicitly form the random projection ma-
                                                           of the matrix represents a document as a vector of link
trix in Equation (4) up-front. The random index vectors
                                                           frequencies to and from other documents.
for each term can be generated and stored as they are
                                                               The motivation behind this representation is that
first encountered. The fact that each index vector is
                                                           documents with similar content will link to similar
sparse means that the vectors use less memory to store
                                                           documents. For example, in the current Wikipedia
and are faster to add.
                                                           both car manufacturers BMW and Jaguar link to the
    The effect of this approach is that each document
                                                           Automotive Industry document. Link frequencies were
will have a particular signature that can be compared
                                                           weighted with the same Inverse Document Frequency
with other documents via cosine similarity. The docu-
                                                           heuristic from TF-IDF. The idea is to decrease the
ment signature is thus a vector on the unit hyper-sphere.
                                                           weight of highly frequent links and increase the
    In the simple scenario in Figure 5 the index vectors
                                                           weight of less frequent links. Links to year documents
for the four words travel, mars, space and telescope, are
                                                           in the Wikipedia are examples of "stop links" that
added to the document vector as they are encountered
                                                           are weighted down by this heuristic.       Unlike term
in the text of the document. Afterwards, the document
                                                           frequencies in TF-IDF the link frequencies in LF-IDF
should be normalised.
                                                           are not normalised. De Vries and Geva [5] found that
    The sparse index vectors can be efficiently stored by
                                                           normalising link frequencies decreased classification
simply storing the position of the non-zero entries with
                                                           performance.
the sign of the position indicating whether it is one or
                                                               When document and link representations are com-
negative one.
                                                           bined they are both converted to unit vectors and con-

3.4 Random Indexing K-tree                                 catenated. Converting each representation to unit vec-
                                                           tors ensures that the weights of one representation do
The time complexity of K-tree depends on the length
                                                           not dominate the other. De Vries and Geva [5] found
of the document vectors. K-tree insertion incurs two
                                                           this to be effective for classification.
costs, finding the appropriateleaf node for insertion and
k-means invocation during node splits. It is therefore
                                                           5 Experimental Setup
desirable to operate with lower dimensional vector rep-
resentation.                                               Experiments have been run to measure the quality dif-
    The combination of RI with K-tree is a good fit.       ference between various configurations of K-tree. Sec-
Both algorithms operate in an on-line and incremental      tion 2.3 describes the modifications made to K-tree. Ta-
mode. This allows it to track the distribution of data     ble 1 lists all the configurations tested.
as it arrives and changes over time. K-tree insertions         The following conditions were used when running
and deletions allow flexibility when tracking data in      the experiments.
volatile and changing collections. Furthermore, K-tree



                                                         47

  1. Each K-tree configuration was run a total of 20           The unmodified K-tree using TF-IDF culling and
     times.                                                BM25 had unexpected results as seen in Table 3. The
                                                           average micro purity and entropy peaked at 400 dimen-
  2. The documentswere inserted in a differentrandom
                                                           sions. Performingthis dimensionalityreductionat these
     order each time K-tree is built.
                                                           lower dimensions had not been performed before. This
                                                           is an interesting and unexpectedresult and future exper-
  3. If RI was used, the index vectors were generated
                                                           iments will need to determineif the phenomenonoccurs
     statistically independently each time K-tree was
                                                           in different corpora.
     built.
                                                               Improvements in micro purity have been tested for
  4. For each K-tree built, k-means++ [3] was run 20       significance via t-tests. The null hypothesis is that both
     times on the codebook vectors to create 15 clus-      results come from the same distribution with the same
     ters.                                                 mean. In this case they are not significantly different.
                                                           If the null hypothesis is rejected then the difference is
  5. All document vectors were unitised after perform-
                                                           statistically significant.
     ing dimensionality reduction.
                                                               The modifications made to K-tree for use with RI
                                                           had a significant impact. The unmodified K-tree and
    The conditions listed above resulted in 400 mea-
                                                           modified K-tree were compared. Specifically, config-
surements for each K-tree configuration. For each of
                                                           urations B and D, and configurations C and E were
the 20 K-trees built, k-means++ was run 20 times. The
                                                           tested against each other. All dimensions were com-
repetition of the experiments is to measure the variance
                                                           pared against each other. The improved performance of
caused by the random insertion order into K-tree, the
                                                           the modified K-tree was statistically significant for all
randomised seeding process in k-means in the modi-
                                                           dimensions (100 vs 100, 200 vs 200 and so on) with a
fied K-tree and the randomised seeding process of k-
                                                                                                               100
means++.                                                   p-value of 0 or extremely close to 0 (p < 1 × 10- ).
                                                               The modified K-tree using RI was tested with two
    Assessment of clustering quality is based on
                                                           representations. Configurations D and E were tested
the INEX XML Mining track. The set of 114,366
                                                           at all dimensions. The null hypothesis was rejected
documents, belonging to 15 classes were used to
                                                           at all dimensions except 10000.        This means that
evaluate clustering quality of INEX submissions. The
                                                           BM25 performed significantly better than the BM25
cluster labels are taken from the Wikipedia itself.
                                                           + LF-IDF representation at all dimensions except
K-tree generates clusters in an unsupervised manner,
                                                           10000. At 10000 dimensions the difference was not
and it is not necessarily going to produce 15 clusters
                                                           considered statistically significant with a p-value of
at a particular level in the tree. In order to re-use
                                                           0.3. The increased performance of this representation
the INEX test collection, it was necessary to post
                                                           in classification did not apply to clustering when using
process the K-tree and to reduce a cluster level in the
                                                           RI. The LF-IDF representation may be interfering
tree to 15 clusters by using k-means++. Note that
                                                           with the BM25 representation and approaches such as
this is a low cost operation involving only a small
                                                           reducing the weight of LF-IDF in the RI process or
number of vectors, which is not required in an ordinary
                                                           performing RI separately on each representation and
application. It is done for the sole purpose of producing
                                                           then concatenating the reduced vectors may improve
comparable results with the INEX benchmark data.
                                                           performance.      Running k-means on the full sparse
The same approach was taken at INEX 2008 by De
                                                           vectors will also indicate if RI is responsible for this.
Vries and Geva [5]. For a comparison of entropy and
                                                           Further experimentation is required to provide more
purity to be meaningful they have to be measured on
                                                           evidence for this result.
the same number of clusters.
                                                               The unexpected results in configuration A were
    Micro averaged purity and entropy are compared.
                                                           tested against the best RI configuration, E. The highest
Micro averaging weights the score of a cluster by its
                                                           average at 400 dimensions in configuration A was
size. Purity and entropy are calculated by comparing
                                                           tested against all dimensions in configuration E (400 vs
the clustering solution to the labels provided. A higher
                                                           100, 400 vs 200, 400 vs 400, 400 vs 1000 and so on).
purity score indicates a higher quality solution because
                                                           The RI K-tree, configuration E, became statistically
the clusters are more pure with respect to the ground
                                                           more significant at 2000 dimensions with a p-value
truth. A lower entropy score indicates a higher quality
                                                                           6
solution because there is more order with respect to the   of 1.48 × 10- and thus rejected the null hypothesis.
                                                           For dimensions 4000 through 10000, the performance
ground truth.
                                                           difference was statistically significant, with a p-value
                                                           of 0 in all cases. Thus, RI K-tree improves results, even
6 Experimental Results
                                                           over the unexpected high results of configuration A,
Tables 3 to 7 contain results for the K-tree configura-    by embedding the original 200,000 dimensional term
tions tested listed in Table 1. Table 2 lists the meaning  space into at least a 2000 dimension reduced space.
of the symbols used. Figures 6 and 7 are graphical
representations of the average micro purity and entropy.




                                                         48

                                            ID       K-tree                 Representation
                                                                                                                      Symbol            Meaning

                                            A      Unmodified         TF-IDF Culling, BM25
                                                                                                                                 Average Micro Entropy
                                            B      Unmodified         RI, BM25 + LF-IDF
                                                                                                                                Standard Deviation of 
                                            C      Unmodified                 RI, BM25
                                                                                                                                  Average Micro Purity
                                            D       Modified          RI, BM25 + LF-IDF
                                                                                                                                Standard Deviation of 
                                            E       Modified                  RI, BM25

                                                                                                                         Table 2: Symbols for Results
                                                Table 1: K-tree Test Configurations


                                         0.5


                                                                                                              Dimensions                                    
                                        0.48



                                        0.46

                                                                                                                  100       2.6299   0.0194     0.3981   0.0067
                                        0.44

                      purity                                                                                      200       2.4018   0.0207     0.4590   0.0085
                                        0.42
                                                                                                                  400       2.2762   0.0263     0.4814   0.0093
                            micro        0.4
                                                                                                                  800       2.2680   0.0481     0.4768   0.0155
                                                                                                                 1000       2.2911   0.0600     0.4703   0.0192
                                        0.38



                                        0.36                                                                     2000       2.3302   0.0821     0.4569   0.0254
                                 average
                                                                                                     A
                                        0.34                                                         B           4000       2.3751   0.1103     0.4401   0.0331
                                                                                                     C
                                                                                                     D           8000       2.3868   0.1068     0.4402   0.0300
                                        0.32
                                                                                                     E           10000      2.3735   0.1062     0.4431   0.0306
                                            0   1000  2000  3000  4000  5000  6000  7000  8000  9000  10000
                                                                   dimensions
                                                                                                             Table 3: A: Unmodified K-tree, TF-IDF Culling, BM25

                                                Figure 6: Purity Versus Dimensions

                                        3.4


                                                                                                              Dimensions                                    
                                                                                             A

                                                                                             B
                                        3.2
                                                                                             C

                                                                                             D

                                                                                             E                    100       3.0307   0.0149     0.3093   0.0045
                                         3
                                                                                                                  200       2.9295   0.0206     0.3300   0.0079
   entropy
                                        2.8                                                                       400       2.7962   0.0379     0.3648   0.0143
                                                                                                                  800       2.6781   0.0718     0.3921   0.0236
          micro
                                                                                                                 1000       2.6509   0.0842     0.3959   0.0260
                                        2.6


                                                                                                                 2000       2.6315   0.1262     0.3908   0.0345
                                        2.4
               average                                                                                           4000       2.6380   0.1451     0.3860   0.0356
                                                                                                                 8000       2.6371   0.1571     0.3844   0.0382
                                        2.2

                                                                                                                 10000      2.6302   0.1540     0.3876   0.0385

                                         2
                                           0   1000  2000  3000  4000  5000   6000  7000  8000  9000  10000
                                                                  dimensions                                 Table 4: B: Unmodified K-tree, Random Indexing,
                                                                                                             BM25 + LF-IDF
                                              Figure 7: Entropy Versus Dimensions

6.1 INEX Results

The INEX XML Mining track is a collaborative evalu-                                                           Dimensions                                    
ation forum where research teams improve approaches
in supervised and unsupervised machine learning with                                                              100       2.9308   0.0213     0.3337   0.0089
XML documents. Participants make submissions and                                                                  200       2.7902   0.0335     0.3724   0.0126
the evaluation results are later released.                                                                        400       2.6151   0.0417     0.4089   0.0116
                                        The RI K-tree in configuration E performs on aver-                        800       2.5170   0.0703     0.4238   0.0197
age at a comparable level to the best results submitted                                                          1000       2.5066   0.0858     0.4234   0.0240
to the INEX 2008 XML Mining track. The top two                                                                   2000       2.4701   0.0938     0.4275   0.0258
results from the track had a micro purity of 0.49 and                                                            4000       2.4581   0.0979     0.4261   0.0271
0.50. These are not average scores for the approaches                                                            8000       2.4530   0.1139     0.4260   0.0318
but the best results participants found. The RI K-tree in                                                        10000      2.4417   0.1019     0.4283   0.0283
configuration E had a maximum micro entropy of 0.55.
                                                                                                             Table 5: C: Unmodified K-tree, Random Indexing,
This is 10% greater than the INEX submissions.
                                                                                                             BM25




                                                                                                           49

                                                                [4] E. Bingham and H. Mannila. Random projection in
  Dimensions                                            
                                                                    dimensionality reduction: applications to image and
                                                                    text data. In KDD '01: Proceedings of the seventh
      100         3.1527     0.0227      0.3105     0.0047
                                                                    ACM SIGKDD international conference on Knowledge
      200         3.0589     0.0266      0.3312     0.0065
                                                                    discovery and data mining, pages 245­250, New York,
      400         2.9014     0.0259      0.3726     0.0065          NY, USA, 2001. ACM.
      800         2.6690     0.0336      0.4204     0.0085
                                                                [5] C.M. De Vries and S. Geva. Document clustering with
     1000         2.5890     0.0319      0.4349     0.0090
                                                                    k-tree. Advances in Focused Retrieval: 7th Interna-
     2000         2.3882     0.0428      0.4700     0.0129
                                                                    tional Workshop of the Initiative for the Evaluation of
     4000         2.2558     0.0443      0.4879     0.0144          XML Retrieval, INEX 2008, Dagstuhl Castle, Germany,
     8000         2.1933     0.0473      0.4935     0.0162          December 15-18, 2008. Revised and Selected Papers,
     10000        2.1735     0.0496      0.4969     0.0171          pages 420­431, 2009.

                                                                [6] C.M. De Vries and S. Geva.          K-tree: large scale
Table 6: D: Modified K-tree, Random Indexing, BM25
                                                                    document clustering. In SIGIR '09: Proceedings of the
+ LF-IDF
                                                                    32nd international ACM SIGIR conference on Research
                                                                    and development in information retrieval, pages 718­
                                                                    719, New York, NY, USA, 2009. ACM.
  Dimensions                                            
                                                                [7] S. Deerwester, S.T. Dumais, G.W. Furnas, T.K. Lan-
                                                                    dauer and R. Harshman. Indexing by latent semantic
      100         3.0717     0.0263      0.3269     0.0074
                                                                    analysis. Journal of the American Society for Infor-
      200         2.9078     0.0291      0.3706     0.0087
                                                                    mation Science, Volume 41, Number 6, pages 391­407,
      400         2.6832     0.0293      0.4191     0.0077
                                                                    1990.
      800         2.4696     0.0350      0.4555     0.0106
                                                                [8] L. Denoyer and P. Gallinari.     The Wikipedia XML
     1000         2.4093     0.0399      0.4673     0.0115
                                                                    Corpus. SIGIR Forum, 2006.
     2000         2.2826     0.0422      0.4853     0.0137
     4000         2.2094     0.0416      0.4937     0.0141      [9] A. Gersho and R.M. Gray. Vector quantization and sig-

     8000         2.1764     0.0429      0.4975     0.0149          nal compression. Kluwer Academic Publishers, 1993.

     10000        2.1686     0.0440      0.4981     0.0161     [10] S. Geva. K-tree: a height balanced tree structured vector
                                                                    quantizer. Proceedings of the 2000 IEEE Signal Pro-
Table 7: E: Modified K-tree, Random Indexing, BM25                  cessing Society Workshop Neural Networks for Signal
                                                                    Processing X, 2000., Volume 1, pages 271­280 vol.1,
                                                                    2000.
7 Conclusion
                                                               [11] W.B. Johnson and J. Lindenstrauss.        Extensions of
RI K-tree was introduced as an attractive approach for
                                                                    Lipschitz mappings into a Hilbert space. Contemporary
large scale document clustering. This is the first time
                                                                    mathematics, Volume 26, Number 189-206, pages 1­1,
RI and K-tree have been combined. The results show                  1984.
that RI K-tree improves quality of clustering results,
                                                               [12] P. Kanerva. The spatter code for encoding concepts
even over the unexpected results found when using TF-
                                                                    at many levels.      In ICANN94, Proceedings of the
IDF culling. Further experiments are required to deter-
                                                                    International Conference on Artificial Neural Networks,
mine if the unexpected effect of TF-IDF culling at low              1994.
dimensions is an anomaly or actually exists in many
                                                               [13] G. Karypis. CLUTO-A Clustering Toolkit. 2002.
collections. Additionally, RI K-tree is an efficient and
                                                               [14] S. Lloyd.   Least squares quantization in pcm.       In-
high quality approach to overcome previous problems
                                                                    formation Theory, IEEE Transactions on, Volume 28,
with sparse representations when using K-tree. Unfor-
                                                                    Number 2, pages 129­137, March 1982.
tunately the combination of BM25 and LF-IDF repre-
sentations did not improve results in clustering as they       [15] T.A. Plate.   Distributed representations and nested
                                                                    compositional structure. Ph.D. thesis, 1994.
did in earlier classification results.
                                                               [16] M.F. Porter. An algorithm for suffix stripping. Pro-
                                                                    gram: Electronic Library and Information Systems,
References
                                                                    Volume 40, Number 3, pages 211­218, 2006.
 [1] K-tree project page, http://ktree.sourceforge.net. 2009.
                                                               [17] S.E. Robertson and K.S. Jones. Simple, proven ap-
 [2] D. Achlioptas. Database-friendly random projections:           proaches to text retrieval. Update, 1997.
     Johnson-Lindenstrauss with binary coins. Journal of
                                                               [18] M. Sahlgren. An introduction to random indexing. In
     Computer and System Sciences, Volume 66, Number 4,
                                                                    Methods and Applications of Semantic Indexing Work-
     pages 671­687, 2003.
                                                                    shop at the 7th International Conference on Terminol-
 [3] D. Arthur and S. Vassilvitskii. k-means++: the advan-          ogy and Knowledge Engineering, TKE 2005, 2005.
     tages of careful seeding. In SODA '07: Proceedings
                                                               [19] G.K. Zipf. Human behavior and the principle of least
     of the Eighteenth Annual ACM-SIAM Symposium on
                                                                    effort: An introduction to human ecology. Addison-
     Discrete Algorithms, pages 1027­1035, Philadelphia,
                                                                    Wesley Press, 1949.
     PA, USA, 2007. Society for Industrial and Applied
     Mathematics.



                                                             50

      Modelling Disagreement Between Judges for Information Retrieval
                                             System Evaluation

                                      Andrew Turpin          Falk Scholer

                                      School of Computer Science & IT
                                                RMIT University
                                                 GPO Box 2476
                                                 Melbourne 3001
                                 {andrew.turpin,falk.scholer}@rmit.edu.au


Abstract    The    batch    evaluation   of  information  therefore, is a matter of running each query to get a
retrieval systems typically makes use of a testbed        ranked list of documents, noting which is relevant or not
consisting of a collection of documents, a set of         according to the relevance judgements, and summaris-
queries, and for each query, a set of judgements          ing the ranked list of relevance values into an overall
indicating which documents are relevant. This paper       performance score. The alternative approach requires
presents a probabilistic model for predicting IR system   a group of human users, the designing of a suitable
rankings in a batch experiment when using document        experiment that controls for any biases you may wish to
relevance assessments from different judges, using        exclude (for example, education or computer literacy),
the precision-at-n family of metrics. In particular, if   defining an outcome metric (for example, time taken
a new judge agrees with the original judge with an        to find a useful answer document), and then measuring
agreement rate of , then a probability distribution       how users perform with different retrieval systems.
of the difference between the P@n scores of the two           The batch method is by far the cheapest, easiest, and
systems is derived in terms of .                          more repeatable of the two methodologies, and as such
    We then examine how the model could be used to        has dominated IR research for the last three decades.
predict system performance based on user evaluation       Recently, however, a series of papers has shown that the
of two IR systems, given a previous batch assessment of   two methodologies do not necessarily reach the same
the two systems together with a measure of the agree-     conclusions regarding relative system performance.
ment between the users and the judges used to gen-        That is, if batch experiments show system A to be better
erate the original batch relevance judgements. From       than system B, user experiments may show there is no
the analysis of data collected in previous user experi-   difference between the systems [1, 2, 5, 6, 7, 11, 13],
ments, it can be seen that simple agreement () between    or that system B is superior [12].
users varies widely between search tasks and informa-         Our recent work has focussed on trying to quantify
tion needs. A practical choice of parameters for the      and rectify this seeming mismatch between the two ex-
model from the available data is therefore difficult. We  perimental approaches [9, 10]. A key potential source
conclude that gathering agreement rates from users of     of mismatch is the different relevance criteria of the
a live search system requires careful consideration of    judges used to construct the "ground truth" batch judge-
topic and task effects.                                   ments, and the users in the user based experiment. De-
                                                          termining the relevance of a document to a query is a
Keywords      Information retrieval; Evaluation; User
                                                          complex, multi-faceted task [4]. It often depends on the
studies
                                                          reason that the relevance judgement is being made, a
                                                          task effect; the query itself, a topic effect; and of course
1    Introduction
                                                          the person making the judgement, a judge effect. There
To test whether one information retrieval system is bet-  are many other factors that influence human judgement
ter than another, researchers either adopt the Cranfield  in general, including motivational biases, preconcep-
methodology of batch assessment, or test their systems    tions, salience and availability, and perseverance [8];
with humans in a user experiment. The batch assess-       these may all have additional effects on the criteria that
ment methodology requires a collection of documents,      judges use to decide if a document is relevant or not.
a set of queries, and, for each query, a judgment on          In this paper we develop a probabilistic model of
some or all of the documents indicating whether they      agreement between relevance judges, and derive how
are relevant to that query or not. Assessing systems,     this is expected to affect the results of a batch-based
                                                          evaluation of IR system performance. We then inves-
Proceedings of the 14th Australasian Document Comput-     tigate how agreement values could be obtained from a
ing Symposium, Sydney, Australia, 4 December 2009.        user study, so that the model might be used to transfer
Copyright for this article remains with the authors.      the outcomes from a batch experiment to a new user



                                                        51

population. Analysis of the data from the user experi-          i    J [i]   J [i]            J [i]    J [i]    
                                                                      A        B       i        A       B        i
ments shows that agreement between users is subject to          1      1       0      1          0       0      0
substantial variation from both task and topic effects.         2      1       0      1          1       1      0
                                                                3      0       0      0          0       1      -1
2     Preliminaries                                             4      1       1      0          1       1      0
                                                                5      1       0      1          0       0      0
Let a batch evaluation testbed consist of:          a set                  (5) = 3/5              (5) = -1/5
of documents, D         =   {d1,...,d|D };
                                         |   N queries,
Q = {q1, . . . , qN}; and for each query-document pair      Table 1: An example calculation of (5) and (5)
a relevance judgement                                       when System A has a P@5 value of 0.8 and 0.4 with
                                                            different judgements, and System B has P@5 of 0.2 and
                 1,
R(d , q) =            document di is relevant to query q,   0.6 respectively.
     i           0,   otherwise.

    A system, returns a ranked list of m documents          the P@n score for System B might increase, and the
[d , . . . , d                                              P@n score for System A might decrease, so that B
  i1          im] for query q, which are mapped to a
vector of relevance judgements in retrieved order           becomes the superior system.
J = [R(d , q), . . . , R(d                                      If we assume that the new judge has some probabil-
             i1            im, q)].
    Judgement vectors can be reduced to a single score      ity of agreeing with the judge used to build the original
in various ways, and many different performance             corpus (independently for any query-document pair),
metrics have been proposed for representing the overall     then we could derive a probability distribution of the
performance of IR systems. In this paper we use the         new scores for System A and B. In turn, this can be used
precision-at-n documents metric, usually written P@n,       to derive a probability distribution on the difference be-
which is the proportion of the top n documents of the       tween the two systems, and we can hypothesise about
list that are relevant. Formally,                           how transferable system rankings are between judges.

                            1   n                           Definition 1 Let JA be the relevance vector given by
                    P@n =          J[i].
                            n                               System A for query q using corpus judgements R(d, q),
                               i=1                          and JB the relevance vector given by System B. For the
The score for a system is the mean P@n over all queries     same document lists, let J and J be the relevance
                                                                                         A       B
in the corpus. A system with a statistically significantly  vector given using judgements R(d, q) for System A
higher score than another is defined to be superior in the  and B respectively.
batch mode of system comparison, and is assumed to be
                                                                We can now define the difference in P@n scores
superior in the user mode of comparison. This has been
                                                            between the systems for query q using either set of rel-
shown to be the case for P@1 in user experiments when
                                                            evance judgements, and then derive a probability distri-
the measured outcome is "time to save first relevant
                                                            bution for that difference based on agreement probabil-
document" [9], and "satisfaction" [7], and for the tasks
                                                            ities between judges.
and users employed in those studies.
    For example, if System B has a score of P@3=0.33,
                                                            Definition 2 For some ranked position 1  i  n, let
then on average only 1 of the top 3 documents is
                                                                                                          
relevant, while if System A has a score of P@3=1.0,          i  = J [i] - JB[i], and i = JA[i] - JB[i]. Then
                                                                     A
                                                            the difference in P@n scores for the systems using ei-
then the top 3 documents are always relevant for all test
                                                            ther set of relevance judgements is given by (n) =
queries. It is implicitly assumed in IR experimentation     n       /n and (n) =       n      /n respectively.
that System A is superior to System B.                         i=1  i                     i=1  i

                                                                Table 1 shows an example of how (5) and (5)
3     Modelling changes in judges                           is calculated.  In this instance, using the second set
                                                            of judgements has decreased System A's superiority to
Using a testbed such as those from the Text REtrieval
Conference [16] will yield system rankings that             (5) = -0.2, that is, System B is now apparently
                                                            better than System A.
should be comparable with other experiments based
                                                                Without loss of generality, we will from now
on different queries with the same collection [15]
                                                            assume that System A has a higher P@n score than
or ­ with suitable standardisation ­ across different
queries and collections     [17].  That is, if System A     System B using the corpus judgements JA and JB.
                                                            Thus we are interested in deriving a probability
is found to be statistically significantly better than
                                                            distribution for (n), and in particular the probability
System B when running a batch experiment, then this
relationship should in general continue to hold for         that (n)  0; that is, System A remains superior
                                                            with a new set of judgements.
different queries, and collections.     If, however, you
kept the same set of documents and queries, but used
an alternate relevance judge, so that R(d, q) became        Definition 3 Let 0 be the probability that the new
R(d, q), then system rankings may alter. In particular,     judge agrees with a R(d, q) = 0 judgement in the




                                                          52

                     J [i]    J [i]   J [i]   J [i]                                      
                       A       B                           Probability
                                       A        B       i                  Probability×i
                       0        0       0       0      0   00              0
                       0        0       0       1     -1   0(1-0)          -0(1-0)
                       0        0       1       0      1   (1-0)0          0(1-0)
                       0        0       1       1      0   (1-0)(1-0)      0
                                                                           E00 = 0

                       0        1       0       0      0   0(1-1)          0
                       0        1       0       1     -1   01              -01
                       0        1       1       0      1   (1-0)(1-1)      (1-0)(1-1)
                       0        1       1       1      0   (1-0)1          0
                                                                           E01 = 1 - 0 - 1

                       1        0       0       0      0   (1-1)0          0
                       1        0       0       1     -1   (1-1)(1-0)      -(1-0)(1-1)
                       1        0       1       0      1   10              01
                       1        0       1       1      0   1(1-0)          0
                                                                           E10 = 0 + 1 - 1

                       1        1       0       0      0   (1-1)(1-1)      0
                       1        1       0       1     -1   (1-1)1          -1(1-1)
                       1        1       1       0      1   1(1-1)          1(1-1)
                       1        1       1       1      0   11              0
                                                                           E11 = 0

Table 2: All possible cases for judgement of a document in a ranked list at position i by the corpus and new
judges, with their corresponding probabilities. For each possible pair of JA and JB values, the expected value of
, labelled Ex for each x, is computed as the sum of the four entries above it.
 i




corpus, thus R(d, q) = 0, and 1 be the probability         Definition 4 For a given query q and Systems A and B,
that the new judge agrees with a R(d, q) = 1 judgement     let c00 be the number of rank positions in the top n for
in the corpus, hence R(d, q) = 1.                          query q where JA[i] = 0 and JB[i] = 0, and like-
                                                           wise for c10, c01 and c11. That is, cxy = |{JA[i] =
    For any rank i in the top n documents for a single     x and JB[i] = y, 1  i  n}|. Note, (n) = (c10 -
query, the entries in the relevance vectors for System A   c01)/n.
and System B for that position is either: JA[i] = 0
and JB[i] = 0, both systems returned an irrelevant             For each position in a ranked list, EJA[i]JB[i]gives
document in that position; JA[i] = 1 and JB[i] = 1,        the expected value of , and so the expectation of
                                                                                     i
both system returned a relevant document in that posi-     (n) can be calculated as:
tion; and the two discriminating cases JA[i] = 1 and                                          
J [i] = 0, or JA[i] = 0 and JB[i] = 1.                                                n
 B                                               Table 2        E[(n)]       =   E        /n
                                                                                          
shows, for each of these four possible cases, the four                                    i

possible relevance vector entries at a particular rank i                             i=1

that might result using different judgements (J [i] and                      =   (c00E00 + c01E01 + c10E10
                                                 A
J [i]). In addition to the  values for each case, the                            +c11E11)/n
 B                            i
probability of realising each combination is given in                        =   (1 - 0 - 1)(c01 - c10)/n
the second last column, which is the product of the                          =   (0 + 1 - 1)(n)                 (1)
appropriate agreement probabilities. For example, in
the first row the probability of JA[i] = 0 and JA[i] = 0
                                                 
                                                               Intuitively this makes sense. If new judges agree
is 0, and JB[i] = 0 and JB[i] = 0 is also 0, so
                                                           perfectly with the corpus judges, then 0 = 1 = 1,
total probability of that event is 00. In the second       then E[(n)] = (n): there is no expected difference
row, JA[i] = JA[i] = 0, but JB[i] = 0 is judged
                  
                                                           in the system's scores with either judgement set. If new
as J [i] = 1 with probability (1 - 0), so the total
     B                                                     judges disagree completely with the corpus judges, then
probability is 0(1 - 0). The final column is summed        0 = 1 = 0, then E[(n)] = -(n): that is, the
for each of the four possible cases of JA[i] and JB[i] to  expected system scores are the reverse of the original.
give the expected value of  for that case, labelled E00,
                             i                                 We can also compute the variance of (n). Recall
E01, E10, and E11 respectively.                            that Var(X) = E(X2) - E(X)2 by definition, so:



                                                         53

            Var((n))                                                                   0
                                                                                      1 1.                                              0.95
                   n                                                                                                      0.7       0.85
        =              
            Var        /n                                                                                          0.55
                       i                                                                                                         0.75
                  i=1                                                                     0.8                                               0.9
              n                                                                                                             0.6          0.8
        =        Var( )/n
                           2
                      i                                         relevant:
             i=1                                                                             0.6
              1  n                       
                                                                     0.25    0.4
        =                  2           2                                 with                                                           0.65
             n2       E[( ) ] - E[i]
                          i
                i=1                                                                             0.4
        =   (c00(20(1 - 0))                                                                                  0.1   0.2           0.45
                                                                                                                             0.35
            +c11(21(1 - 1))                                                                        0.2                                   0.5
            +(c01 + c10)(1 - 0 - 1 + 201)                                                                        0.05
                                                                             Agreement                                  0.15
                                                                                                                                    0.3
            -(1 - 0 - 1)2(c01 - c10)2)/n       2      (2)                                             0.0
    Equations 1 and 2 are for a single query, q, but are
                                                                                                           0.0    0.2   0.4     0.6    0.8     1.0
easily extended to a score computed over a set of N
queries because the P@n metric assigns equal weight to                                                       Agreement with irrelevant: 0
all ranked positions. That is, computing the mean P@n
value over the top n documents retrieved for N queries      Figure 1: Contour plot of the probability of (1)
is the same as computing P@Nn for a concatenation           exceeding zero (hence System A remaining superior)
of the N J[1..n] relevance vectors for each query. If       with the P@1 score, when the corpus is re-judged by a
we use the notation Ji to represent the relevance vector    judge that agrees 0 and 1 proportion of the time with
J for query i, and JS = J1[1..n]J2[1..n]..JN[1..n] to       the original judge's 0 and 1 judgements, respectively.
represent the concatenation of the first n elements of all
J 's, then:
 i                                                          be larger than 1 (approximately). To be 95% confident

      1   N                      1   N   1   n              that System A will remain superior, both agreement
            (P@n of Ji)     =                 J [j]         probabilities must be over 80%.
      N                          N       n      i
         i=1                       i=1     j=1
                                                            3.2                        The extreme case
                                  1   nN
                            =             J [k]             Just as for the P@1 case, assuming P@n=1 for Sys-
                                 Nn         S
                                      k=1                   tem A and P@n=0 for System B allows simplification
                            =    P@Nn of JS.                of Equations 1 and 2 as all of c00, c11 and c01 are 0, and
                                                            c10 = n.
Henceforth we will limit our discussions to the single
                                                                Thus
query case for notational convenience.
    Equation 2 contains cxy terms, which will alter de-                                                  E[(n)]   =   0 + 1 - 1
pending on system, query and judgements. However, if                                   Var((n))                   =   ((1 - 0 - 1 + 201)
we fix n, or assume the maximum possible separation
between systems on the corpus, the equations can be                                                                   -(0 + 1 - 1)2)/n
simplified to something immediately useful.
                                                                If we assume that 0 = 1 = , then we can
3.1    The P@1 case                                         plot E[(n)] and a 95% confidence interval as
                                                                                       
When considering P@1, the expression for Var((n))           ±1.96                                        Var((n)) for different n values.   This is
                                                            shown in Figure 2.
simplifies to something manageable. As we are inter-
                                                                To be 95% sure that System A remains superior with
ested in the case where System A is better than Sys-
                                                            new judgements, agreement must be at least 90% for
tem B on query q using the corpus judgements, then
                                                            P@1 (intersection of dark grey ellipse and the 0 line),
P@1=1 for System A and for System B, P@1=0. Hence
                                                            75% for P@5 (intersection of medium grey ellipse and
c00 = c11 = c01 = 0, c10 = 1, n = (n) = 1, and              the 0 line), and 70% for P@10 (intersection of light
          E[(n)]      =    0 + 1 - 1                        grey ellipse and the 0 line).

        Var((n))      =    0 + 1 - 0 - 1.2     2
                                                            3.3                        Other cases
    Assuming (n) is normally distributed with mean
                                                            It is possible to simplify Equations 1 and 2 for other
E[(n)] and a standard deviation of           Var((n)),      values of n where System A and System B are not sep-
then we can compute P r[(n)  0] which is shown in           arated extremely, that is, when the gap between Sys-
Figure 1. To be more than 50% confident that a new set      tem A and System B is less than one: (n) < 1. The
of judgements on the corpus will keep System A as su-       technique involves labelling each possible combination
perior with the P@1 metric, the sum of 0 and 1 must         of JA[i] and JB[i] for all i, but is omitted from this



                                                          54

        2                                                    a provided information need, and to mark documents
                                                             that were presented as relevant or not relevant for
                                                             inclusion in the report. Participants were asked to carry

         1                                                   out this task for three TREC topics (numbers 707, 770
                                                             and 771); the description and narrative fields of the
    ]                                                        topics were displayed to users as information needs.
                                                             Participants were therefore making binary decisions
     [    0
                                                             about relevance, when presented with documents that
      E                                                      had previously been judged by TREC assessors on a
                                                             three-point scale (not relevant; relevant; and highly
           -1                                                relevant).  There was no time constraint for making
                                                             decisions for the judging task.      However, it became
                                                             clear that carrying out the task for all three topics
                                                             resulted in severe fatigue effects.      The third topic
             -2
                                                             completed by each user is therefore removed from the
                                                             analysis.
                0.0    0.2   0.4      0.6     0.8     1.0
                        Agreement 0 = 1                      Searching task:       Participants also carried out a
                                                             searching task.       Here,   when presented with an
                                                             information need, users were asked to search for and
Figure 2:        Expected (n) values (black) and 95%
                                                             identify a relevant answer document as quickly as
confidence limits for n = 1 (dark grey), n = 5
                                                             possible. Users could enter a single query to a search
(medium grey) and n = 10 (light grey) assuming
                                                             system, designed to be similar in appearance to popular
P@1=1 for System A and P@1=0 for System B.
                                                             commercial search engines such as Google, Yahoo! or
                                                             Bing. Unknown to the user, for each topic they were
paper as we concentrate on the P@1 metric in our user        assigned to a system of a particular quality; that is, the
studies.                                                     system would return a ranked answer list with a pre-
                                                             determined P@1 level. For this task, 24 informational
4      Practical considerations                              topics were chosen from TREC topics 700­850 (topics
                                                             developed for use with the GOV2 collection).           To
In this section of the paper we turn our attention to
                                                             construct the P@1 controlled lists, judged documents
an investigation of the likely values 0 and 1 when           were selected from the two highest-performing runs
users conduct a web-based search task. In particular, we
                                                             submitted to the TREC terabyte track in 2004, 2005
examine data from one of our previous user studies that
                                                             and 2006. That is, all documents used in the lists could
involved both document judgements and search-and-
                                                             plausibly be returned in response to the topics by a
click judgements, and see if  values are stable across
                                                             modern information retrieval system.
different topics and tasks for a given pair of users.
                                                                 After being presented with a search results list, a
                                                             user could select a document for viewing. They could
4.1     User experiment
                                                             then take one of two actions: save the document as a
Participants      for  our user   study   were    recruited  relevant answer; or close the document, and return to
form RMIT University.             All were postgraduate      the results list. In the analysis below, these actions are
or undergraduate students studying for degrees in            taken as judgements of the relevance or non-relevance
computer science and information technology.            As   of the document, respectively.
a result, most were very familiar with searching                 Note that the user studies were not explicitly
for information on the web;          in a pre-experiment     designed to answer the questions raised in this paper;
questionnaire the average user indicated that they           rather we are retrospectively analysing the data to get
search "once or more a day". Experiments were carried        insights into likely values of 0 and 1. Full details of
out in compliance with the RMIT University Human             the user studies are available in previous papers [9, 10].
Research Ethics Committee. 40 users participated in
the study; however, three were unable to complete the        4.2    Agreement on the judging task
full experiments, and are therefore excluded from the
                                                             Figure 3 shows the distribution of agreement values be-
analysis.
                                                             tween all pairs of users for the judging task. As agree-
    Participants were asked to carry out two tasks: a
                                                             ment is not symmetrical [14], each user pair is counted
judging task, and a search task. For both, documents
                                                             twice, usually with different values. As can be seen,
and topics were sourced from the TREC GOV2 collec-
                                                             agreement varies anywhere from 100% down to 7.7%
tion, a crawl of 426 Gb of data from the .gov domain
                                                             for users 14 and 11 on 0.
from 2004 [3].
                                                                 Perhaps of more interest is the difference in agree-
Judging task:        For the first task, participants were   ment for any user pair that judged the same two topics.
asked to imagine that they are writing a report, based on    Figure 4 shows that on any two topics, both 0 and



                                                           55

                                   0.20                                                              0.20


                                       0.15                                                              0.15


                                           0.10                                                              0.10
              Proportion

                                               0.05                                                              0.05


                                                   0.00                                                              0.00
                                                                         5  25    45   65   85                                             5  25    45   65    85
                                                                            Agreement 0 (%)                                                   Agreement 1 (%)


                                   Figure 3: Distribution of agreement amongst all pairs of users on the judging task.




                                                       0.30                                                              0.30



                                                           0.20                                                              0.20


                        Proportion
                                                               0.10                                                              0.10



                                                                   0.00                                                              0.00
                                                                        -75 -35   -5   25   55                                            -75 -35   -5    25   55
                                                                            Difference 0 (%)                                                  Difference 1 (%)


         Figure 4: Distribution of the difference in agreement amongst pairs of users on the judging task.



1 can vary widely in the judging task. This makes                                                by the first user in the pair, giving 758 user pairs. Again,
it difficult to choose a representative agreement value                                          agreement is not symmetric, and so each pair of users
for any pair of judges. Note that as we had to remove                                            is counted twice, typically with different values. As can
the third topic judged for each user from the data set,                                          be seen, the distribution of agreement values is similar
not all pairs of users completed the same two topics. In                                         to those for the judging task.
total 534 of the 1369 pairs are included.
                                                                                                 4.4                                     Agreement across tasks
4.3     Agreement on the search task
                                                                                                 Figure 6 shows the distribution of the difference in 0
Figure 5 shows the distribution of agreement values be-                                          and 1 for pairs of users between the searching and
tween all pairs of users for the searching task. Here we                                         judging tasks. Again, the difference across tasks can
have taken the event where a user selected a document                                            be large, making it difficult to choose a representative
from the ranked list but did not save it as an "irrelevant"                                      agreement value for any pair of judges/users.
judgement, while the selection and explicit saving of an                                             Figure 7 plots each user pair that has an agreement
item is taken as a "relevant" judgement. For any pair of                                         value for both tasks. As is apparent, there is no guar-
users, we computed 0 and 1 over all topic-document                                               antee that if a pair of users did not agree in the judging
pairs that both users selected from the ranked lists for                                         task, they will not agree in the search task, and vice
viewing. We only included pairs where at least 6 topic-                                          versa.
document pairs were judged as relevant and irrelevant




                                                                                               56

                                   0.20                                                               0.20


                                       0.15                                                               0.15


                                           0.10                                                               0.10
              Proportion

                                               0.05                                                               0.05


                                                   0.00                                                               0.00
                                                                         5  25    45   65   85                                                 5      25      45    65       85
                                                                             Agreement 0                                                                 Agreement 1


                                  Figure 5: Distribution of agreement amongst all pairs of users on the searching task.




                                                       0.30                                                               0.30



                                                           0.20                                                               0.20


                        Proportion
                                                               0.10                                                               0.10



                                                                   0.00                                                               0.00
                                                                        -75 -35   -5   25   55                                               -75       -35    -5     25      55
                                                                            Difference 0 (%)                                                           Difference 1 (%)


Figure 6: Distribution of the difference in agreement amongst pairs of users on the searching task and judging task.



5    Conclusions                                                                                 concentrated on the P@n metrics; in future work we
                                                                                                 plan to extend the approach to other metrics.
We have presented a simple probabilistic model based
                                                                                                     Examining the agreement values in one of our user
on agreement between judges that can predict the effect
                                                                                                 studies has revealed large topic and task effects. That
that altering judges will have on system performance as
                                                                                                 is, for any pair of users, their agreement may alter on
measured through a batch evaluation experiment. When
                                                                                                 different topics or tasks by over 50%. Thus, applying
evaluating performance with P@1, for example, to be
                                                                                                 the model presented in Section 3 to predict the effect
95% confident that one system will remain superior to
                                                                                                 of changing judges on a corpus requires more sophisti-
a second after judges are changed, the agreement be-
tween relevance assessments of the judges must be at                                             cated measuring of 0 and 1 than was possible with
                                                                                                 our available user data.                                        In future work, we plan to
least 80%.
                                                                                                 investigate controlled experiments for gathering repre-
    The model can also be used to assist in selecting
                                                                                                 sentative agreement values between different users of
metrics. For example, for the P@n family of metrics,
                                                                                                 retrieval systems.
it can be seen that the larger the value of n (that is,
the more information from the result list that is con-
sidered), the lower the required level of agreement be-                                          References
tween judges to remain confident that the relative sys-                                           [1] Azzah Al-Maskari, Mark Sanderson and Paul Clough.
tem performance will not change. In this paper we have                                                                                    The relationship between IR effectiveness measures and
                                                                                                                                          user satisfaction. In Proceedings of the ACM SIGIR




                                                                                               57

                            1.0                                                                                                              
                                                                                   1.0
                                                                                                                                    
                                                                                                                                     
                                                                                                                                             
                                                                                                                                 
                                                                                                                              
                                                                                                                                             
                                                                                                                                        
                                                                                                                                             
                                                                                                                             
                                                                                                                                    
                                                                                                                                          
                                                                                                                                             
                                                     
              (%)              0.8                                
                                                                                                                                             
                                                                                      0.8                                                    
                                                                                                                                        
                                                                                                                                             
                                                                                                                                             
                                                              
                                                              
                                                                                                                       
                                                                                                                                      
                                                                                                                                        
                                                                                                                                             
                 task                           
                                                                   
                                                                                                                                      
                                                                                                                                             
                                                                                                                                  
                                                                                                                     
                                                                                                                   
                                                                                                                                         
                                                                                                                                             
                                  0.6                                                                            
                                                                                         0.6                                                 
                                                                                                                                   
                                                                                                                                             
                                                                                                                                             
                                                                            
                                                                    
                                                                                                                           
                                                                                                                                  
                                                                                                                                             
                     judge                                                                                                              
                                                                                                                                             
                                                                                                                                        
                                                                                                                                             
                          0                                                                                           
                                     0.4                                                    0.4                                              
                                                                                                                           
                                                                                                                                             
                                                                                                                                  
                                                                                                                    
                                                         
                                                    
                                        0.2                                                    0.2

                                            0.0 0.2    0.4   0.6    0.8    1.0                           0.2     0.4     0.6       0.8      1.0
                                                  0 search task (%)                                           1 search task (%)


                                                Figure 7: The agreement on each task for each pair of users.



    International Conference on Research and Development                                          national Workshop on Evaluating Information Access
    in Information Retrieval, pages 773­774, Amsterdam,                                           (EVIA 2008), pages 47­56, Tokyo, Japan, 2008.
    Netherlands, 2007.
                                                                               [11] Catherine Smith and Paul Kantor.                      User adaptation:
 [2] James Allan, Ben Carterette and Joshua Lewis. When                                           good results from poor systems. In Proceedings of the
    will information retrieval be "good enough"?                      In Pro-                     ACM SIGIR International Conference on Research and
    ceedings of the ACM SIGIR International Conference                                            Development in Information Retrieval, pages 147­154,
    on Research and Development in Information Retrieval,                                         Singapore, Singapore, 2008.
    pages 433­440, Salvador, Brazil, 2005.
                                                                               [12] Andrew Turpin and William Hersh.                       Why batch and
 [3] Stefan B¨uttcher, Charles Clarke and Ian Soboroff. The                                       user evaluations do not give the same results. In Pro-
    TREC 2006 terabyte track.                           In The Fifteenth Text                     ceedings of the ACM SIGIR International Conference
    REtrieval Conference (TREC 2006), Gaithersburg, MD,                                           on Research and Development in Information Retrieval,
    2007. National Institute of Standards and Technology.                                         pages 225­231, New Orleans, LA, 2001.

 [4] Carlos Cuadra and Robert Katter.                        The relevance of  [13] Andrew Turpin and Falk Scholer.                     User performance
    relevance assessment. In Proceedings of the American                                          versus precision measures for simple web search tasks.
    Documentation Institute, Volume 4, pages 95­99, 1967.                                         In Proceedings of the ACM SIGIR International Con-

 [5] William Hersh, Andrew Turpin, Susan Price, Benjamin                                          ference on Research and Development in Information

    Chan, Dale Kraemer, Lynetta Sacherek and Daniel                                               Retrieval, pages 11­18, Seattle, WA, 2006.

    Olson.                Do batch and user evaluations give the same          [14] Alexander von Eye and Eun Young Mun.                             Ana-
    results? In Proceedings of the ACM SIGIR International                                        lyzing Rater Agreement: Manifest Variable Methods.
    Conference on Research and Development in Informa-                                            Lawrence Erlbaum Associates, 2004.
    tion Retrieval, pages 17­24, Athens, Greece, 2000.
                                                                               [15] Ellen M. Voorhees and Chris Buckley.                       The effect
 [6] Scott B. Huffman and Michael Hochster. How well does                                         of topic set size on retrieval experiment error.     In
    result relevance predict session satisfaction?                    In Pro-                     Micheline Beaulieu, Ricardo Baeza-Yates, Sung Hyon
     ceedings of the ACM SIGIR International Conference                                           Myaeng and Karlervo J¨arvelin (editors), Proceedings of
     on Research and Development in Information Retrieval,                                        the ACM SIGIR International Conference on Research
     pages 567­574, Amsterdam, Netherlands, 2007.                                                 and Development in Information Retrieval, pages 316­
 [7] Diane Kelly, Xin Fu and Chirag Shah. Effects of rank                                         323, Tampere, Finland, 2002.
     and precision of search results on users' evaluations of                  [16] Ellen M. Voorhees and Donna K. Harman. TREC: ex-
     system performance.                         Technical Report TR-2007-02,                     periment and evaluation in information retrieval. MIT
     University of North Carolina, 2007.                                                          Press, 2005.
 [8] Arie Kruglanski and Icek Ajzen.                        Bias and error in
                                                                               [17] William Webber, Alistair Moffat and Justin Zobel.
     human judgement. European Journal of Social Psychol-
                                                                                                  Score standardization for inter-collection comparison of
     ogy, Volume 13, pages 1­44, 1983.
                                                                                                  retrieval systems. In Proceedings of the ACM SIGIR
 [9] Falk Scholer and Andrew Turpin. Metric and relevance                                         International Conference on Research and Development
    mismatch in retrieval evaluation. In The Fifth Asia In-                                       in Information Retrieval, pages 51­58, Singapore, Sin-
    formation Retrieval Symposium (AIRS 2009), Sapporo,                                           gapore, 2008.
    Japan, 2009. To appear.

[10] Falk Scholer, Andrew Turpin and Mingfang Wu. Mea-
    suring user relevance criteria.                      In The Second Inter-



                                                                             58

                                 University Student Use of the Wikipedia

                    Andrew Trotman                                               David Alexander
         Department of Computer Science                                Department of Computer Science
                 University of Otago                                           University of Otago
               Dunedin, New Zealand                                          Dunedin, New Zealand
              andrew@cs.otago.ac.nz                                         dalexand@cs.otago.ac.nz

                                                            field of research known as Link Discovery.
Abstract: The 2008 proxy log covering all student
                                                                Milne & Witten [11] use machine learning to learn
access to the Wikipedia from the University of Otago
                                                            links for documents to be added to the Wikipedia.
is analysed. The log covers 17,635 student users for
                                                            INEX has the Link-the-Wiki track [3] in which the
all 366 days in the year, amounting to over 577,973
                                                            task is to analyse a document (also from the Wikipe-
user sessions. The analysis shows the Wikipedia is
                                                            dia) and to construct an ordered list of links from
used every hour of the day, but seasonally. Use is low
                                                            which a user can choose; Geva [1] and Jenkinson et al.
between semesters, rising steadily throughout the se-
                                                            [7] provide the best solutions.
mester until it peaks at around exam time. The analy-
                                                                The recent INEX study by Huang et al. [5] raises
sis of the articles that are retrieved as well as an
                                                            questions about the validity of the methods of assess-
analysis of which links are clicked shows that the
                                                            ment that had been used with all previous solutions to
Wikipedia is used for study-related purposes. Medical
                                                            the Link Discovery problem, and therefore the validity
documents are popular reflecting the specialty of the
                                                            of the solutions themselves.
university. The mean Wikipedia session length is
                                                                The prior INEX protocol was as follows: A dump
about a minute and a half and consists of about three
                                                            of the Wikipedia is taken. From that dump a single
clicks.
                                                            document is extracted (the orphan). All links between
    The click graph the users generated is compared
                                                            the orphan and the collection are removed. The task is
to the link graph in the Wikipedia. In about 14% of the
                                                            to recommend links for the orphan. Performance is
user sessions the user has chosen a sub-optimal path
                                                            measured relative to the links that were originally in
from the start of their session to the final document
                                                            the orphan.
they view. In 33% the path is better than optimal sug-
                                                                Huang et al. introduced a new protocol to INEX,
gesting that users prefer to search than to follow the
                                                            based on the Cranfield methodology. In this protocol,
link-graph. When they do click, they click links in the
                                                            INEX participants' runs were pooled and manually
running text (93.6%) and rarely on "See Also" links
                                                            assessed. Importantly, the links in the original
(6.4%), but this bias disappears when the frequency of
                                                            Wikipedia articles were added to the pool. Most im-
these types of links' occurrence is corrected for.
                                                            portantly, the Wikipedia articles themselves were
    Several recommendations for changes to the link
                                                            scored against the pool. Unexpectedly, the Wikipedia
discovery methodology are made. These changes in-
                                                            articles performed no better than the best submitted
clude using highly viewed articles from the log as test
                                                            runs.
data and using user clicks as user judgements.
                                                                This result suggests that there are many links in the
                                                            Wikipedia that are not considered relevant to the topic
Keywords: Information Retrieval, Link Discovery.
                                                            of the articles. The nature of those non-relevant links
                                                            is not known, but could be studied by analysing the
1. Introduction
                                                            INEX assessments.

Keeping the link structure up-to-date in a large hyper-         This approach would shed light on the nature of
text collection is difficult. When a new document is        relevant and irrelevant links in the Wikipedia and
added to the collection it is necessary to link from that   could be used both to help recommend new links and

document to the collection and from the collection to       to remove bad links. But a link that is relevant to the
that document. When a document is deleted all links         content of the page may not be relevant to the infor-
from the collection to the document must be removed.        mation need of the user. To find useful links it is nec-

Finally, when a document changes, new links must be         essary to study how users use links. This raises our
added and old links deleted. Deleting links is a me-        research question: How do users use the Wikipedia

chanical process, but recommending links for new or         link structure?

changing documents is problematic and is an active              To answer this question we studied the log of the
                                                            University of Otago student web proxy, which all stu-
                                                            dent users of the University computing facilities must
  Proceedings of the 14th Australasian Document Compu-
  ting Symposium, Sydney, Australia, 4 December 2009.       pass through, for the 2008 calendar year. From the log
  Copyright for this article remains with the authors.      we extracted all references to the Wikipedia.



                                                          59

                     100000000

                      10000000
     sts
        e              1000000
         u
          q
                        100000
           Re
             f           10000
              o
               r
                e         1000
                 b
                  m
                           100
                   Nu
                            10

                             1
                                                                  it                    a                                                     tr
                               en     de ja zh fr es ar el                                   id ru cs pl fa    ia           ro       fi af hi
                                                          p    ko    nl    th     pt
                                                                        ms     sv    mi                     da           no    hu he
                                                                                                                 lg
                                  www                      m                             met                       ta
                                                            si                                                       s
                                                                                                                      o
                                                                                                                       n


    Figure 1: Frequency of use of the versions of the Wikipedia seen more than 500 times in the log. Eng-
  lish is the preferred language followed by German, Japanese, Chinese, French, Spanish and so on. The
                               subdomains for language versions of Wikipedia are ISO 639 codes.

    Before studying the link-clicking behaviour dis-                         mender task and consequently systems generate a
played in the log, we performed a number of prelimi-                         ranked list of results. Geva's solution [1] at INEX is to
nary analyses in order to better understand the data,                        match the titles of Wikipedia documents against the
and its applicability to our goal of improving the link                      text of a document, preferring longer titles if several
structure of Wikipedia. These included examining the                         overlap. The Jenkinson et al. solution [7] is based on
request frequency at different times of day and times                        Itakura & Clarke [6]. They generate a list of all an-
of year, calculating the length of user sessions, and                        chors used in the collection along with a list of all
finding the most commonly-requested pages. The re-                           documents that are targeted by each anchor text. They
sults of these analyses are presented in Sections 3.1                        rank anchor texts on the frequency with which they
and 3.2                                                                      occur as links, as a proportion of their overall fre-
    In Section 3.3, the link-clicking behaviour seen in                      quency. They then search for these in the new docu-
the log is analysed, with particular focus on the ques-                      ment and recommend links based on the above fre-
tion of whether or not the current link graph is being                       quency. The two approaches perform comparably.
used efficiently. This question is addressed in two
ways. The first is to determine the proportion of links
clicked on in each article, and to look for patterns in
                                                                             3. Analysis
the types of links clicked. The second is to determine
whether or not users are reaching their destinations by                      In this section an analysis of the proxy log is given.
following links, and if so, whether or not they are do-                      The global statistics are presented followed by an
ing so in the most efficient way possible.                                   analysis of the sessions. Finally the use of the hyper-
                                                                             text links is given.

2. Prior Work
                                                                             3.1. Global Statistics
Prior IR research on logs has focused on search engine
log analysis. Zhang & Moffat [14], for example, pre-                         The proxy log covers the period from 1st January
sent an analysis of the MSN log while Spink et al. [13]                      2008 to 31st December 2008. It covers 366 days be-
present an analysis of an Excite log.                                        cause 2008 was a leap year. The proxy configuration
    Internet use by students has previously been stud-                       at the university consists of a set of proxies each log-
ied; however such studies are typically conducted                            ging and fulfilling user requests. There were a total of
through surveys, for example Metzger et al. [9].                             6 proxy servers and so the analysis is over 2,196
    Proxy log use has been limited. Kamps et al. [8]                         source log files. One of these files (from 30 April
used a (3 month long) New Zealand high school proxy                          2008) was lost and so the analysis is short by one sixth
log to validate INEX 2007 results. Their analysis is                         on that date.
short. They state: the number of queries; the number                         All lines from the log that contained the (case insensi-
of unique queries; the number of clicks in the Wikipe-                       tive) word Wikipedia were extracted. There were a
dia; the number of queries with Wikipedia clicks; and                        total of 16,665,418 references in the extracted log, of
the number of unique queries with Wikipedia clicks.                          which 15,696,225 were to the English Wikipedia and
    There is a growing body of work in link recom-                           969,193 were to other sites. The references were made
mendation. Early work [10, 11] conducted outside                             by 17,635 students (the university had 20,752 enrolled
INEX considers the problem of generating a set of                            during 2008). Further fundamental numeric statistics
links. INEX considers link discovery to be a recom-                          are shown in Table 1.




                                                                          60

      Table 1: Fundemental statistics of the log          dren), meta.wikimedia.org (a wiki containing informa-
Duration of Log               1/Jan/2008 ­ 31/Dec/2008    tion useful to editors of the various Wikimedia pro-
Rows in Log                   16,665,418                  jects), and nostalgia (a static copy of a 2001 version
Rows for English Wikipedia    15,696,225                  of Wikipedia).
Users in Log                  17,635                          Figure 2 shows the request frequency of all sub-
Total enrolled students       20,752                      domains of wikipedia.org and wikimedia.org. It shows
Sessions in Log               577,973
                                                          that the subdomains do not completely follow a
Articles Accessed             340,477
                                                          power-law distribution.
Articles in Wikipedia         2,600,000 (approx.)
                                                              Timestamps in a search engine log are relative to
Wikipedia Subdomains          202 (inc. typos)
                                                          the search engine location. It is therefore not possible
                                                          to know the user-time at which each query was given.
                                                          In a proxy log of the type used in this study, however,
                                                          the user time is the same as the time recorded at the
                                                          proxy.
                                                              Figure 3 shows the mean number of requests per
                                                          minute at each hour of the day. At midnight there is
                                                          moderate access steadily falling to low at 5am where
                                                          access picks up and stabilizes at about 11am. A local
                                                          peak is seen at 3pm with a dip at dinner-time, picking
                                                          up at about 7pm and falling again at about 10pm. Stu-
                                                          dent use of the Wikipedia is round-the-clock.
                                                              This finding is in line with results seen by others.
                                                          Zhang & Moffat [14] found that there was no hour of
                                                          the day at which the MSN search engine was com-
     Figure 2: Frequency of use of all versions.
                                                          pletely unused from within the US. The US, however,
                                                          is a somewhat larger geographical area then the Uni-
                                                          versity of Otago (and has a larger population).
                                                              Publicly available search engine logs tend to cover
                                                          a very short period of time. The MSN log is one
                                                          month in length, the Excite logs are one day, and the
                                                          Alta Vista log is about six-weeks. From such short
                                                          logs it is not possible to make any observations about
                                                          seasonal user behaviour, analyses have been restricted
                                                          to daily patterns.
                                                              Zhang & Moffat [14] present a day-by-day analy-
                                                          sis of the MSN log, which covers May 2006. They
                                                          show a clear drop in use over weekends and a pattern

 Figure 3: Access to the Wikipedia by time of day.        of peaking early in the week and dropping towards the
                                                          end.
    The Wikipedia exists in many different languages          Shown in Figure 4 is the total number of Wikipe-
and forms. Each of these versions has its own subdo-      dia requests per day seen in the proxy log. Use is
main of wikipedia.org. In the log there are 202 refer-    clearly seasonal varying from fewer than 1,000 ac-
ences to different variants (including spelling errors).  cesses per day in December to over 14,000 accesses
The most common is the English Wikipedia while the        per day in June and October. Unsurprisingly the peak
least common (occurring only once) is spe-                is around the university's exam period.
cies.wikipedia.org, the Wikipedia Free Species Direc-         It is reasonable to conclude from this seasonal ac-
tory.                                                     cess pattern that the Wikipedia forms an important
    Figure 1 graphs the frequency of use of those va-     part of the student study regime at the University of
riants of the Wikipedia seen in the log more than 500     Otago. If this is the case then it is also reasonable to
times. The graph shows that English (subdomain en)        expect many of the most frequently requested pages to
is the primary language used at Otago, with European      be related to academic study.
and Asian languages also popular. The Mori Wikipe-            The 20 most frequently requested Wikipedia arti-
                               th
dia (subdomain mi) was the 17 most popular version,       cles are shown in Table 2. The homepage (Main Page)
accessed 2,953 times.                                     is the most viewed Wikipedia page, being requested
    All of the subdomains shown in Figure 1 are iden-     with more than 23 times the frequency as the next
tified by the ISO 639 codes for their languages, except   most popular page. This is as expected as many users
www (an entry point to Wikipedia, having links to the     will enter the Wikipedia via the homepage rather than
most popular language versions), simple (the Simple       typing an article's URL manually.
English Wikipedia, in which articles are written at a         Column 3 shows a manual classification of the
level suitable for non-native English speakers or chil-   given pages into the categories Work-Related (W),




                                                        61

Informational (I) and Entertainment (E). Of the top
20, half (10) can be considered work-related while the
other half are entertainment (2) and informational (8).
Most of the work-related pages are medical, reflecting
the importance of the medical sciences to the Univer-
sity. This provides further evidence that the Wikipedia
is, indeed, being used by students as an aid to their
study during the exam period.
    It should be noted that the classification is ad-hoc,
and was arbitrarily chosen by the two authors. In par-
ticular, all medical pages in the table are classified as
work-related on the assumption that these pages are
mostly requested by the university's large number of
medical students, rather than by people seeking medi-
                                                                   Figure 4: Access to the Wikipedia by date.
cal advice. The classification of some pages is clearly
                                                              Semester-times, breaks and examination periods
ambiguous; the Treaty of Waitangi page could be con-
                                                                                  are indicated.
sidered informational due to the treaty's relevance to
the location of the university (New Zealand), or work-
                                                                 Table 2: Top 20 most retrieved pages, classified
related due to its potential relevance to History stu-
                                                                 as Work-Related (W), Informational (I) or
dents.
                                                                               Entertainment (E)
    Plotted in Figure 5 is the number of times each of
                                                                    Page                    Requests Class
the 340,477 requested articles was retrieved (ordered
                                                                    Main Page                75583      I
by frequency). There are a small number of pages re-
                                                                    Wiki                      3256      I
quested a very large number of times. (Those articles
                                                                    New Zealand               1686      I
appear to be informational pages about the Wikipedia,
                                                                    Deaths in 2008            1315      I
Wikis, New Zealand, the University of Otago, and
                                                                    University of Otago       861       I
death!) This distribution of request frequencies sug-               Dunedin                   859       I
gests that more useful results could come from cluster-             Standard deviation        857       W
ing pages by subject area. We hypothesise that this                 Wikipedia                 806       I
would show other subject areas being looked up with                 Dopamine                  669       W
comparable frequency to the medical sciences, but that              Blood pressure            561       W
those requests would be distributed among a greater                 The Dark Knight (film)    557       E
number of pages, leading to their absence in Table 2.               Aldosterone               556       W

    It is not only reassuring that the Wikipedia is used            Glycolysis                546       W

for study purposes within the university, but also reas-            Tyrosinase                541       W
                                                                    Gossip Girl (TV series)   541       E
suring that it is not primarily used for smut. Spink et
                                                                    Treaty of Waitangi        516       I
al. [13] provide a list of the 75 most frequently seen
                                                                    Tuberculosis              514       W
search terms in the Excite query log, the top 10 of
                                                                    Meningitis                512       W
which are: and, of, sex, free, the, nude, pictures, in,
                                                                    Multiple sclerosis        511       W
university, pics. It appears as though the Wikipedia is
                                                                    HIV                       510       W
being used honourably by students.

                                                                He & Göker [2] define a web search session as a
3.2. Session Statistics                                     set of consecutive requests by a user with no longer
                                                            than some time limit from one request to the next.
Identifying a user's session in a search engine query
                                                            They conclude that for web search log analysis the
log has proven to be problematic because it is not
                                                            optimal time is between 10 and 15 minutes. There
clear what the user is doing between one log entry and
                                                            was, however, very little difference observed between
the next. The same problem exists when looking at a
                                                            the sessions produced using a time limit of 15 minutes
proxy log such as the one used in this study, because
                                                            and those produced using a time limit of 60 minutes.
only the user actions that result in an HTTP request
                                                                It is reasonable to assume that a user navigating
are recorded.
                                                            the Wikipedia will spend longer reading documents
    The proxy log used in this study distinguishes us-
                                                            than a user searching the web spends reading a results
ers, and identifies the requested page, dates, time, etc.,
                                                            list. For this reason, and for this study, a session is
but not the referrer. Therefore, although it is known
                                                            defined as a set of consecutive requests by the same
what was done, by whom, and when, it is not certain
                                                            user with a gap of no more than 60 minutes between
what a user was doing before making a particular re-
                                                            adjacent requests. Further investigation is needed to
quest. Identifying a user's session under these circum-
                                                            determine whether or not this is a suitable time limit
stances is problematic because without the referrer it
                                                            for proxy logs.
is difficult to identify the start (or end) of a session.




                                                          62

                                                           how long the user spent looking at the single page that
                                                           was requested.
                                                               In Figure 6 the sessions from the proxy log are
                                                           shown ranked from the longest to the shortest. In total
                                                           there were 577,973 sessions. When measured by time,
                                                           the longest had 26 requests over 86,441 seconds (1
                                                           day and 41 seconds), and the median had 2 requests
                                                           over 93 seconds. It is reasonable to conclude that the
                                                           longest session is not human generated (one click an
                                                           hour for a day) and so there are, in all likelihood, ro-
                                                           bots running at the university that are downloading
                                                           data from the Wikipedia each hour.
                                                               When measured by number of requests, the longest
                                                           had 2,340 requests over 8,550 seconds (a mean of one
    Figure 5: Number of times each document is
                                                           click every 3.76 seconds for 2 hours 22 minutes and
  retrieved ordered from most to least frequent.
                                                           30 seconds), and the median had 3 requests over 93
                                                           seconds. Again it is reasonable to conclude that the
                                                           longest session is not a human, but a robot.
                                                               In some cases users chose to search the Wikipedia
                                                           using a search engine. In these cases they might have
                                                           either added the word Wikipedia to their query or site-
                                                           restricted their search to a wikipedia.org site.
                                                               Table 3 shows the top 20 non-Wikipedia site ori-
                                                           gins appearing in the log. It is important to recall that
                                                           the analysed log only includes requests that contain
                                                           the substring Wikipedia ­ and so this table does not
                                                           truly reflect the number of sessions originating outside
                                                           the Wikipedia. It is surprising that Google does not
                                                           appear, but this is possibly because of Google's use of
                                                           asynchronous requests for result lists on supporting
 Figure 6: Session lengths ordered from longest to         browsers.
shortest. Sesson times in seconds and in number of             Coupling this result with the number of requests
               clicks are both shown.                      for the Wikipedia homepage leads to the conclusion
                                                           that the students tend to go directly to the Wikipedia
     Table 3: Top 20 non-wikipedia session origins         and then search, rather than using an Internet search
         Count Source                                      engines to find information in the Wikipedia.
          2030    http://rds.yahoo.com/
          1527    http://nz.wrs.yahoo.com/
                                                           3.3. Link Statistics
          1433    http://content.answers.com/
            427   http://hk.wrs.yahoo.com/                 The primary motivation for this investigation is the
            203   http://s.scribd.com/
                                                           understanding of how users navigate the Wikipedia so
            203   http://wrs.search.yahoo.co.jp/
                                                           that this knowledge may be used to improve the per-
            154   http://au.wrs.yahoo.com/
                                                           formance of link recommender systems.
            149   http://mycroft.mozdev.org/
                                                               For the purpose of this investigation a user is
            130   http://tw.wrs.yahoo.com/
                                                           deemed to have clicked a link in order to retrieve an
            129   http://sp.ask.com/
                                                           article if, within a session, there was a page requested
            110   http://uk.wrs.yahoo.com/
                                                           earlier in that session that contains a link to the re-
             82   http://www.scribd.com/
                                                           trieved page.
             81   http://digg.com/
             76   http://static.getfansub.com/                 An alternative would be to consider only the user's
             76   http://www.microsoft.com/                most recently requested page as a potential link
             68   http://www.nationmaster.com/             source, which would reduce the number of false posi-
             60   http://www.apple.com/                    tives. This was rejected because of anecdotal evidence
             57   http://wrs.yahoo.com/                    that users surfing the Wikipedia have multiple pages
             52   http://i.ixnp.com/                       open at once, meaning that the user's click sequence
             52   http://pixel.quantserve.com/             may resemble part of a breadth-first traversal of the
                                                           link graph.
   Session length can be measured in several ways              For brevity, the term click will hereafter be used
including the number of requests and the total time        without qualification to refer to a request that is be-
between the first and last request. In the case of a sin-  lieved to have been caused by a click on a particular
gle-request session, however, the session time must be     link. It is important to note that this information may
considered to be zero because it is impossible to tell



                                                         63

not be accurate, and a proxy log with referrers should
ideally be used in future research.
    Presented in Table 4 are the top 20 most clicked
links. Of particular note is the link from the homepage
to Deaths in 2008. This can be directly attributed to
the link "Recent Deaths" at the bottom of the "in the
news" section of the homepage. Of the top 20 links,
13 are clearly work-related while 5 are entertainment
and 2 are informational.
    Shown in Figure 7 is the distribution of link clicks
ordered from most popular to least popular. By in-
spection it can be seen to roughly follow a power-law
distribution. Most links are clicked only once but
some links are very popular.
                                                               Figure 7: Frequency of use of clicked links
    Figure 8 shows the distribution of clicked links on
a per document basis. It can be seen that of the links in
a document, very few were clicked even though there
are many links in the documents. This cannot be ex-
plained by the presence of "boilerplate" links such as
the What links here link because these links are not
included in the collection from which the relevant data
was extracted.




      Table 4: Source and target articles of the 20
                  most clicked links.
 Source               Target               Clicks Class
 Main Page            Deaths in 2008        3092     I
                      Nicotinamide
 NAD                  adenine dinucleo-      282    W
                      tide
 Nicotinamide
 adenine dinucleo-    FAD                    239    W
 tide
 Tyrosinase           Melanin                233    W
 Lactate              Lactic acid            219    W
 ADH                  Vasopressin            206    W
 Tyrosine             Dopamine               202    W
 Heroes               Heroes (TV series)     186     E
 Main Page            Wikipedia              181     I
 Melanin              Melanocyte             179    W
                      List of South Park
 South Park                                  176     E
                      episodes
                                                              Figure 8: Number of clicked links per document
                      Gossip Girl (TV
 Gossip Girl                                 174     E     by absolute count (above) and relative to the num-
                      series)
                                                             ber of links in the document (bottom). In most
                      Immunologic ad-
 Adjuvant                                    162    W       documents only one link was clicked despite there
                      juvant
                                                             being many links that might have been chosen.
 Thiamine pyro-       Pyruvate dehydro-
                                             161    W
 phosphate            genase
                      List of Heroes                          Huang et al. [4] present the metric used in the
 Heroes (TV series)                          151     E
                      episodes                             INEX Link-the-Wiki track. It is a mean average preci-
                      List of House                        sion (MAP) based metric which assumes that all rele-
 House (TV series)                           141     E
                      episodes                             vant links are equally relevant. This assumption may
 Systole              Systole (medicine)     133    W      not be valid; the users may show bias for certain links.
 Vitamin E            Tocopherol             133    W      In future work we will examine these potential biases
 Melanin              Melanoma               124    W
                                                           by determining the prior probability of the click fre-
                      Thoracic dia-
 Diaphragm                                   124    W      quency distributions seen in each document. Given the
                      phragm
                                                           already observed bias from the homepage to the recent
                                                           deaths page it is reasonable to believe that some links
                                                           are more popular than others. If this is the case then




                                                         64

the appropriateness of the INEX Link-the-Wiki met-               Figure 9 shows the difference between the actual
rics should be examined.                                     length and the estimated shortest path for each ses-
    6.4% of those links that are clicked are from the        sion. Positive numbers indicate that clicks would be
See Also section of the document whereas remaining           saved if the user had chosen the shortest path; nega-
93.6% are from the running text. 6.4% is also the            tive numbers are due to users arriving at their destina-
proportion of links in those documents that are See          tion by methods other than clicking links.
Also links. This suggests that there is no user prefer-          The shortest path estimation algorithm was used
ence to these links over the running text links. This is     because of the number of sessions and the magnitude
surprising because the See Also links are at the bottom      of the link-graph. It should be noted that the result is
of the page, although Fitts's Law may apply.                 always pessimistic. It computes a number that is no
    INEX offers two tasks in the Link-the-Wiki track:        smaller than the shortest path. Despite this, 83,761
file-to-file linking, and anchor-to-BEP (best entry          (14%) user sessions would be reduced in length if the
point) linking. In the former the task is to identify arti-  user had followed the shortest path. In 192,375 (33%)
cles related to a new article to be added to the Wikipe-     sessions the user found a path shorter than the esti-
dia. This is equivalent to the task of adding See Also       mated shortest path (perhaps by searching). 231,317
links to an article. In the latter task the link discovery   sessions are optimal. For the remaining 70,520 ses-
system must identify anchor-texts in the running text        sions no path could be found (the link graph is not
of the new article and targets within the Wikipedia.         strongly connected).
    The discovery that running-text links appear to be           Assuming users are doing their utmost to find the
as important as the See Also links suggests that the         information they seek, it is pertinent to ask why they
two INEX tasks are also equally important.                   waste so many clicks in their information seeking.
    Potamias et al. [12] propose an algorithm for ap-        Further investigation is needed; however it could be
proximating the shortest path between two nodes in a         due to information overload. Given the extensive
large graph. Several hubs are chosen based on an es-         interlinking between Wikipedia articles, it may simply
timate of their centrality in the graph, and a single-       be too difficult to spot which links to click. If this is
source shortest path calculation is performed from           the case then a reduction in the size of the link graph
each hub to all nodes in the graph. The shortest path        (that is, the removal of links) may result in a better
estimate for a pair of nodes is calculated by determin-      user experience. This result is in line with the manual
ing the length of the path between the nodes through         assessment experiments of Huang et al. [5], which
each hub in turn, and taking the shortest of those           suggest that many of the links in the Wikipedia are not
paths.                                                       relevant. Further, since 33% of the sessions are shorter
    The actual path taken in each session was com-           than the shortest path, it is reasonable to conclude that
puted and the lengths of the paths are shown in Figure       users' current response to viewing over-linked docu-
6. The shortest path they could have taken (from the         ments is to resort to searching.
start to the end of their session) can be estimated using        The mean number of clicks that could be avoided
the algorithm of Potamias et al. The difference is the       if a user followed the shortest path is 0.018 clicks per
slack in the session. That is, assuming the user has one     session.
information need per session and upon fulfilling it              However, it is also possible that many of the
they stop using the Wikipedia, the number of wasted          wasted clicks seen are a result of users browsing
clicks (and consequently the amount of wasted time)          Wikipedia for trivia, merely because they find it inter-
can be estimated.                                            esting. (For example, clicking links that go from the
                                                             name of a day, month or year to a list of events that
                                                             happened in that time period.) It is therefore important
                                                             not to take the link-graph reduction goal to its logical
                                                             conclusion by removing all trivial links, as this would
                                                             diminish users' enjoyment of Wikipedia, which might
                                                             in turn cause the non-trivial information content in
                                                             Wikipedia to stagnate. Therefore, it is important to
                                                             balance the removal of links that hinder navigation
                                                             with the retention of links that, while not strictly rele-
                                                             vant, are sometimes used and do not hinder naviga-
                                                             tion.
                                                                 It is pertinent to ask whether the first document the
                                                             user viewed should have been linked to the last docu-
                                                             ment they viewed. Computing this is equivalent to
     Figure 9: Number of clicks that could be saved
                                                             solving the link discovery problem, but an estimate
    if the user navigated the Wikipedia using the
                                                             might be made using one of the previously published
 shortest path from the start of their session to the
                                                             link discovery algorithms. The Itakura & Clarke [6]
                  end of their session.
                                                             algorithm as implemented by Jenkinson et al. [7] is
                                                             fast and might make a good candidate algorithm, as




                                                           65

might Geva's title matching algorithm [1]. Computing       articles were chosen and which links were clicked.
the optimal link graph for the Wikipedia is left for       This log might be used as the source of articles for the
future work.                                               INEX track. If the articles were chosen from those
                                                           accessed in the log then performance could be meas-
                                                           ured relative to those links that were clicked.
4. Discussion and Conclusions
                                                               The log might also be used in the Link-the-Wiki
The University of Otago student proxy server logged        anchor-to-BEP task in which the link discovery sys-
all accesses to the Internet for the 2008 calendar year.   tem must choose anchors and target document / best
From this log all accesses to the Wikipedia were ex-       entry point pairs. Although best entry points are not
tracted and analysed. In total 16,665,418 requests         typically linked to in the Wikipedia, the anchor text
were made by 17,635 users.                                 and target document pairs can be deduced from the
    The analysis suggests that students use the            Proxy log using the method outlined above.
Wikipedia primarily as an encyclopaedia for study-             Much of this study was devoted to understanding
related purposes. They typically use it for a very short   how university students use the Wikipedia. It is heart-
period of time (a few minutes) and search from the         ening to see the use is generally related to their study,
Wikipedia rather than via an Internet search engine.       but disheartening to see that use is driven by the ex-
They prefer to use it close to exams, and they use it at   amination schedule.
all times of the day and night.
    The analysis of the link statistics suggests that
                                                           5. References
there is some bias in the users' click pattern, as very
few of the available links are clicked, but further work   [1] Geva, S., GPX: Ad-Hoc Queries and Automated
is needed to determine the nature of this bias. Users           Link Discovery in the Wikipedia. INEX 2007 pp.
appear to click on a very small proportion of the links         404-416.
in a document, but there is no bias towards See Also or    [2] Göker, A. and D. He, Analysing Web Search Logs
running-text links. If indeed there is bias, then it may        to Determine Session Boundaries for User-
be appropriate to re-examine the metrics used to                Oriented Learning, In Adaptive Hypermedia and
measure the performance of link discovery systems.              Adaptive Web-Based Systems. 2000. pp. 319-322.
    On the assumption that a user is trying to fulfil one  [3] Huang, D.W., et al., Overview of INEX 2007 Link
information need in each session, the amount of slack           the Wiki Trac. INEX 2007 pp. 373-387.
in a user session was computed. In 14% of sessions         [4] Huang, W.C., S. Geva, and A. Trotman, Overview
the user did not choose the shortest path from the start        of INEX 2008 Link the Wiki Track, INEX. 2008p.
of their session to the end. In 33% of cases the user           314-325.
found a path shorter than the shortest path which sug-     [5] Huang, W.C., A. Trotman, and S. Geva, The Im-
gests that the link-graph of the Wikipedia is not help-         portance of Manual Assessment in Link Discov-
ing those users and they are resorting to methods other         ery, SIGIR 2009
than browsing in order to find their information.          [6] Itakura, K.Y. and C.L. Clarke, University of Wa-
    This study was conducted with the goal of improv-           terloo at INEX2007: Adhoc and Link-the-Wiki
ing link discovery systems such as those seen in the            Tracks, INEX 2007. pp. 417-425.
INEX Link-the-Wiki track. The results suggest that by      [7] Jenkinson, D., K.-C. Leung, and A. Trotman,
removing non-useful links from the Wikipedia (sim-              Wikisearching and Wikilinking, in pre-
plifying the graph) the user will find it easier to             proceedings of INEX 2008. 2008.
browse in order to fulfil their information need, but it   [8] Kamps, J., M. Koolen, and A. Trotman, Compara-
is important not to take this too extreme, and to re-           tive Analysis of Clicks and Judgments for IR
move harmless links merely because they are not rele-           Evaluation,.WSCD 2009.
vant, as this would decrease the utility of the Wikipe-    [9] Metzger, M.J., A.J. Flanagin, and L. Zwarun, Col-
dia.                                                            lege student web use, perceptions of information
    Further work might be conducted on the proxy log.           credibility, and verification behavior. Comput.
Previous studies have suggested that 4-digit year links         Educ., 2003. 41(3):271-290.
are not considered relevant by INEX assessors. The         [10] Mihalcea, R. and A. Csomai, Wikify!: linking
nature of the links the user clicked remains unknown,           documents to encyclopedic knowledge. CIKM
as does the nature of relevant links in the INEX as-            2007. pp. 233-242.
sessments.                                                 [11] Milne, D. and I.H. Witten, Learning to link with
    The INEX Link-the-Wiki track has two tasks. In              wikipedia, CIMK 2008 pp. 509-518.
the file-to-file task a set of randomly selected docu-     [12] Potamias, M., et al., Fast shortest path distance
ments are chosen from the Wikipedia. The links be-              estimation in large networks, CIKM 2009.
tween those documents and the Wikipedia are re-            [13] Spink, A., et al., Searching the Web: The public
moved and the system must predict the links that were           and their queries. JASIST 2001. 53(2):226-234.
present. As a consequence of the Wikipedia log en-         [14] Zhang, Y. and A. Moffat. Some Observations on
tries having been extracted from the full proxy log,            User Search Behavior. ADCS 2006. pp. 1-8
there now exists a complete year-long log of which




                                                         66

         Feature Selection and Weighting Methods in Sentiment Analysis

                          Tim O'Keefe                                     Irena Koprinska

           School of Information Technologies                School of Information Technologies
                    University of Sydney                               University of Sydney
                    NSW 2006, Australia                               NSW 2006, Australia
               toke9145@uni.sydney.edu.au                              irena@it.usyd.edu.au


Abstract     Sentiment analysis is the task of identifying     In fact, in the Pang et al. [9] movie review data set
whether the opinion expressed in a document is positive    that has become the de facto standard there are just
or negative about a given topic. Unfortunately, many       under 51,000 unique words and symbols.         Very few
of the potential applications of sentiment analysis are    of these features actually provide useful information
currently infeasible due to the huge number of features    to the classifier, so feature selection can be used to
found in standard corpora. In this paper we systemat-      reduce the number of features. Despite the fact that its
ically evaluate a range of feature selectors and feature   use is commonplace, there has been little research into
weights with both Na¨ive Bayes and Support Vector Ma-      the effects of different methods of feature selection
chine classifiers. This includes the introduction of two   in sentiment analysis.   In this paper we address this
new feature selection methods and three new feature        gap by comparing three feature selection methods at
weighting methods. Our results show that it is possible    a number of selection thresholds, using six feature
to maintain a state-of-the art classification accuracy of  weighting methods.      The feature selection methods
87.15% while using less than 36% of the features.          include Categorical Proportional Difference (PD), a
                                                           recently proposed method that was successfully used
Keywords      Information Retrieval, Natural Language
                                                           for topic-based text categorisation, and two methods
Techniques and Documents
                                                           based on sentiment values from SentiWordNet (SWN)
                                                           [2] that we introduce:     SWNSS and SWNPD. The
1    Introduction
                                                           feature weighting methods include Feature Frequency
The opinions of other people have always been impor-       (FF), Feature Presence (FP), TFIDF, and three other
tant to us, and in particular we are often concerned with  methods based on words grouped by their SWN values
the prevailing sentiment of those opinions. Often gov-     that we introduce: SWN-SG, SWN-PG and SWN-PS.
ernments want to know how voters feel about a policy,      All tests were conducted using both SVM and NB.
corporations want to know how customers feel about a           Our results show that PD and SWNSS were able to
product and movie goers want to know if others would       maintain or improve accuracy when used with suitable
recommend a movie. The idea behind sentiment anal-         weightings while SWNPD tended to reduce accuracy,
ysis is to provide this information by building a system   though not in all cases. SVM with PD as a feature se-
that can classify documents as positive or negative, ac-   lector achieved our highest accuracy of 87.15% which
cording to the overall sentiment expressed within those    is comparable with the state-of-the art, but uses a vastly
documents.                                                 reduced set of features.
    Early approaches to sentiment analysis tended to
focus on classifying documents according to the out-       2    Background
of-context sentiment of individual features [14]. While
                                                           While there was some early work in word-level sen-
these approaches did not require domain-specific train-
                                                           timent analysis [3] and a semi-automatic approach to
ing data, their accuracy was quite poor.      Subsequent
                                                           document-level sentiment analysis [13], the real genesis
research focused on supervised learning techniques that
                                                           of document-level sentiment analysis was the work of
are common in text categorisation tasks [9], such as
                                                           Turney [14]. The basic idea behind Turney's approach
Support Vector Machine (SVM) and Na¨ive Bayes (NB)
                                                           was to average the sentiment of the adjectives within
classifiers. Though these techniques are far more ac-
                                                           each document and then classify the document depend-
curate than the earlier text-based approaches, they are
                                                           ing on whether the average was positive or negative.
a lot more computationally expensive to run due to the
                                                           To find the sentiment of adjectives, Turney used the Al-
large number of features.
                                                           taVista search engine to determine how often individual
Proceedings of the 14th Australasian Document Comput-      adjectives co-occured with the words "excellent" and
ing Symposium, Sydney, Australia, 4 December 2009.         "poor." Words that co-occured more often with "ex-
Copyright for this article remains with the authors.       cellent" were deemed positive and words co-occuring
                                                           more often with "poor" were deemed negative.


                                                           67

  Authors                   Data      Classifier                          Cross        Feature    Baseline      Best
                            split                                         Valida-      Selec-     Accuracy      Accuracy
                                                                          tion         tion       (%)           (%)
  Pang et al. [9]           700+      NB, ME, SVM                         3-fold       No         N/A           82.9
                            700-
  Pang & Lee [8]            1000+     NB, SVM                             10-fold      Yes        87.15         87.2
                            1000-
  Mullen & Collier [7]      700+      Hybrid SVM (Turney values,          10-fold      No         83.5          86
                            700-      Osgood values, lemma models)
  K¨onig & Brill [6]        1000+     Pattern-based, SVM, Hybrid          5-fold       No         87.5          91
                            1000-
  Abbasi et al. [1]         1000+     Genetic Algorithms (GA), In-        10-fold      Yes        87.95         91.7
                            1000-     formation Gain (IG), IG + GA
  Prabowo & Thelwall        1000+     Hybrid (rule + closeness mea-       10-fold      No         87.3          87.3
  [10]                      1000-     sure + SVM)

     Table 1: Results reported in the literature on various versions of the Pang et al. [9] movie review data set.


    The first use of supervised learning in sentiment        the highest level of accuracy in sentiment analysis to
analysis was by Pang et al. [9].       Their aim was to      date of 91.7%. The drawback of this new method is
determine whether sentiment analysis could be treated        that while it can efficiently classify items, it is very
as a special case of topic-based categorisation with         computationally expensive to conduct the initial feature
two topics: positive and negative.       To achieve this     selection, since both GA and IG are expensive to run.
they tested Na¨ive Bayes (NB), Maximum Entropy
(ME), and Support Vector Machine (SVM) classifiers,          2.2     SentiWordNet
all of which have performed well in topic-based              SentiWordNet (SWN) is an extension of WordNet that
categorisation.     For features, they used the words        was developed by Esuli & Sebastiani [2], which is in-
and symbols of the documents as either a unigram             tended to augment the information in WordNet with in-
or a bigram bag-of-features, with unigrams generally         formation about the sentiment of the words in WordNet.
performing better. They tested Feature Frequency (FF)        Our research uses the information provided by senti-
and Feature Presence (FP) and found that by using a          ment in some detail, so we will describe it here. Each
SVM with unigram FP they could achieve an accuracy           synset in SWN has a positive sentiment score, a neg-
of 82.9% in a 3-fold cross validation test. Table 1 lists    ative sentiment score and an objectivity score. When
some of the best results that have been reported in the      these three scores are summed they equal one, so they
literature.                                                  give an indication of the relative strength of the posi-
                                                             tivity, negativity and objectivity of each synset. Esuli
2.1    Feature Selection
                                                             & Sebastiani [2] obtained these values by using several
Most researchers employ basic feature selection in their     semi-supervised ternary classifiers, all of which were
work in order to improve computational performance,          capable of determining whether a word was positive,
with a few using more complicated approaches [5, 8, 1].      negative, or objective. If all the classifiers agreed on a
To date there have only been two papers that have            classification then the maximum value was assigned for
entirely focused on using feature selection to improve       the associated score, otherwise the values for the posi-
sentiment analysis. The first was by Pang & Lee [8],         tive, negative and objective scores were proportional to
who used a SVM trained on subjective and objective           the number of classifiers that assigned the word to each
text to remove objective sentences from the corpus. In       class.
their initial results they found that document sentiment         The drawback in using SWN is that it requires word
classification accuracy actually declined.     They then     sense disambiguation to find the correct sense of a word
conducted some "non-obvious feature engineering"             and its associated scores. Whilst there has been sig-
by making it more likely that sentences adjacent to          nificant research into this problem, we decided that it
removed sentences would be removed as well, which            was out of scope to use any sophisticated word sense
slightly improved accuracy over their baseline.              disambiguation for this project, so we simply took the
    The other work that used sophisticated feature se-       highest positive and negative values that we could find
lection was by Abbasi et al. [1]. They found that using      for each word. This is based on the assumption that in a
either information gain (IG) or genetic algorithms (GA)      subjective document it is reasonably likely that the most
resulted in an improvement in accuracy. They also com-       subjective sense of a word is being used. Preliminary
bined the two in a new algorithm called the Entropy          testing confirmed that using the most subjective senses
Weighted Genetic Algorithm (EWGA), which achieved


                                                            68

tended to outperform the senses that are known to be        4.1.3   Term      Frequency    -   Inverse    Document
most frequent.                                                      Frequency (TF-IDF)

                                                            TF-IDF is a common metric used in text categorisation
3    Data & Evaluation                                      tasks [11], but its use in sentiment analysis has been

We use two different supervised learning approaches to      less widespread, and surprisingly it does not appear to

sentiment analysis: Support Vector Machines (SVM)           have been used as a unigram feature weight. TF-IDF

and Na¨ive Bayes (NB). SVM and NB classifiers were          is composed of two scores, term frequency and inverse

originally used in sentiment analysis by Pang et al. [9],   document frequency. Term frequency is found by sim-

who found that SVM classifiers generally outperformed       ply counting the number of times that a given term has

NB. In order to be as comparable to Pang & Lee as           occured in a given document, and inverse document

possible we use the SVM implementation developed by         frequency is found by dividing the total number of doc-

Joachims [4], called SVMLIGHT . For Na¨ive Bayes we         uments by the number of documents that a given word

use the implementation available in Weka [15].              appears in. When these values are multiplied together

    The data set we use is the set of 1000 positive and     we get a score that is highest for words that appear

1000 negative movie reviews from IMDb that was in-
                                             1              frequently in a few documents, and low for terms that

troduced in Pang et al. [9]. For all of our experiments     appear frequently in every document, allowing us to

we conduct 10-fold cross validation, and we use paired      find terms that are important in a document.

t-tests at a confidence level of 0.05 to establish signifi-
                                                            4.2    SentiWordNet Word Groups
cance.
                                                            While unigram features have emerged as the most ac-
4    Feature Weighting Methods                              curate approach to sentiment analysis, there has still
                                                            been significant work in using other types of features
4.1     Unigram Features
                                                            [14, 7, 10]. While most of this previous research has
In the domain of sentiment analysis, and more generally     shown that grouping or summing words based on their
text categorisation, it is common to use the words and      out-of-context sentiment has not performed well on its
symbols within the corpus as features in the feature        own [14, 9], some researchers have used these sorts
vectors. Though there are other ways of representing        of features to augment unigrams [7]. We add to this
the words and symbols, we will be using unigrams,           research by using SWN to put the words found in each
where each unique word or symbol is counted as one          document into groups, which we can then use as fea-
feature. Pang et al. [9] found that unigrams fairly com-    tures for classifiers.
prehensively out-performed bigrams and combinations
of unigrams and bigrams. The different feature weights      4.2.1   SWN Word Score Groups (SWN-SG)

for the unigrams are discussed below.                       One of the interesting features of SWN is that there are
                                                            only a limited number of values that the positive and
4.1.1    Feature Frequency (FF)
                                                            negative word scores can take on, due to the way those
The simplest way to represent a document with a vector      scores are calculated. We can take advantage of this
is the feature frequency method that was originally used    fact to group words with the same positive or negative
in sentiment analysis by Pang et al. [9]. The method        score, so that rather than having features that corre-
uses the term frequency, i.e. the frequency that each       spond to words, we have features that correspond to
unigram occurs within a document, as the feature values     groups of words. The value of a feature would then be
for that document. So if the word "excellent" appeared      the number of words in the document that have the same
in a document ten times, the associated feature would       positive or negative SWN score. So for example if the
have a value of ten.                                        sentence "The acting was excellent, the special effects
                                                            were amazing, and the script was terrific" appeared in
4.1.2    Feature Presence (FP)                              a document we might find that "excellent," "amazing,"
                                                            and "terrific" all had the same positive score. When
Pang et al. [9] were also the first to use feature presence
                                                            we turn that sentence into a feature vector one of the
in sentiment analysis. Feature presence is very similar
                                                            features would correspond to that positive score and
to feature frequency, except that rather than using the
                                                            would have a value of three, since there are three words
frequency of a unigram as its value, we would merely
                                                            with that score.
use a one, to indicate that the unigram exists in the doc-
ument. Multiple occurrences of the same unigram are
                                                            4.2.2   SWN Word Polarity Groups (SWN-PG)
ignored, so we get a vector of binary values, with ones
for each unique unigram that occurs in the document,        Since SWN gives words both a positive and negative
and zeros for all unigrams that appear in the corpus but    score, we can find whether a word is more positive than
not in the document.                                        negative and vice versa. This allows us to define two
                                                            features, positive and negative, which correspond to the
   1http://www.imdb.com
                                                            counts of positive and negative words respectively. So


                                                            69

words that are more positive than negative add one to      of the unigram will be close to one, whereas if it oc-
the positive feature and words that are more negative      curs in about as many positive documents as negative
add one to the negative feature. The end result is a fea-  documents then its PD will be close to zero. While
ture vector with two features, the first being the number  Simeon & Hilderman use a more general equation for
of positive words and the second being the number of       multi-class problems, we use a simplified equation for
negative words in the document.                            our two-class problem, which is as follows:

4.2.3   SWN Word Polarity Sums (SWN-PS)                                |PositiveDF - NegativeDF|
                                                                        P ositiveDF + NegativeDF
The final feature type that we introduce is similar to the
word polarity groups, except that we actually sum the      A high score from this equation indicates that the uni-
positive and negative scores, rather than just tallying    gram is telling us a lot, and a low score indicates that
the number of words with those scores. So when we          the unigram is telling us very little.    For example if
convert a document into a feature vector there are two     the word "actor" appears in exactly as many positive
features. The first one is the sum of the SWN posi-        documents as negative documents then finding the word
tive scores of all words that have a higher positive than  "actor" in a new document will tell us nothing about it
negative score. The second feature is the sum of the       and as such its PD score will be zero. Conversely, if the
SWN negative scores of all words that have a higher        word "excellent" appears in only positive documents
negative score than positive score. Any words that have    then finding the word "excellent" in a new document
no positive and no negative score, or where the positive   would give us a good clue that the document is positive,
and negative scores are equal, are ignored. The scores     and as such it would have a PD score of one. So to use
are adjusted for document length, so different length      PD as a feature selector we simply need to remove any
documents can be more accurately compared.                 features where the result of the equation is less than or
                                                           equal to some threshold value.
5    Feature Selection
                                                           5.2    SWN Subjectivity Scores (SWNSS)
When we set out to classify a document we generally
                                                           The SWN feature selector is actually able to distinguish
start off with a very large number of words that need
                                                           objective and subjective terms, which is useful since
to be considered, even though very few of the words
                                                           only subjective terms should carry sentiment. To do
in the corpus are actually expressing sentiment. These
                                                           this we use the SWN subjectivity score, which is found
extra features have two clear drawbacks that we would
                                                           by adding the positive and negative SWN scores of a
like to eliminate. The first is that they make document
                                                           unigram together. This is the opposite of the objectivity
classification slower, since there are far more words
                                                           score that is defined by Esuli & Sebastiani [2], but its
than there really needs to be. The second is that they
                                                           use is equivalent. To use it as a feature selector we
can actually reduce accuracy, since the classifier must
                                                           simply remove any unigrams whose subjective score is
consider these words when classifying a document.
                                                           less than a certain threshold. When this feature selector
    Clearly there is an advantage in using fewer
                                                           is used, unigrams that are not found in SWN, such as
features, so in order to remove some of the unnecessary
                                                           names and misspellings, are removed from the corpus
features, we use feature selection.         As the name
                                                           as well (although arguably the names of certain actors
suggests, feature selection is a process where we
                                                           could give strong clues about the quality of a movie).
run through the corpus before the classifier has been
trained and remove any features that seem unnecessary.     5.3    SWN           Proportional           Difference
This allows the classifier to fit a model to the problem          (SWNPD)
set more quickly since there is less information to
                                                           While the SWN subjectivity feature selector can find
consider, and thus allows it to classify items faster. In
                                                           words that have some a priori sentiment attached, it
this section we describe several different methods of
                                                           cannot tell us whether that sentiment is consistent or
feature selection.
                                                           meaningful. It is entirely possible that a word may have
5.1    Categorical Proportional Difference                 a SWN subjectivity score of one, indicating that it is
       (PD)                                                very subjective, but its positive and negative scores may
                                                           be 0.5 each. This may make the word uninformative as
Categorical Proportional Difference (PD), introduced
                                                           a feature so there could be value in removing it. To
by Simeon & Hilderman [12], is a metric which tells
                                                           do this we define SWN Proportional Difference, which
us how close to being equal two numbers are. We can
                                                           uses the SWN positive and negative scores in the PD
use this to find unigrams that occur mostly in one class
                                                           equation, as follows.
of documents or the other, by using the positive doc-
ument frequency and negative document frequency of                        |SWNPos - SWNNeg|
a unigram as the two numbers.         In other words if a                  SW NP os + SW NNeg
unigram occurs predominantly in positive documents
                                                           Similarly to PD, SWNPD will be high for words that are
or predominantly in negative documents then the PD
                                                           mostly positive or negative, and low for words that are


                                                           70

a mix of both. By using this score we hope to remove                  PD               SWNSS        SWNPD
subjective words that have an ambiguous polarity from          0                       14,617
the corpus.                                                                            (28.71%)
                                                             0.125    18,149           8,250        7,433
6    Results and Discussion                                           (35.64%)         (16.2%)      (14.6%)
                                                             0.25     14,860           7,094        6,870
Table 3 shows in bold the best results achieved for each
                                                                      (29.18%)         (13.93%)     (13.49%)
classifier with each feature selection method. The best
                                                             0.375    10,342           6,061        5,943
accuracy result was 87.15%, which was achieved using
                                                                      (20.31%)         (11.9%)      (11.67%)
PD feature selection with a threshold of 0.125 (which
                                                              0.5     9,180            4,919        5,750
uses 18,149 features or 36% of the total) and FP as
                                                                      (18.03%)         (9.66%)      (11.29%)
a feature weighting method.       For comparison, Table
                                                             0.625    6,716            3,607        4,868
1 shows other results reported in the literature.      All
                                                                      (13.19%)         (7.08%)      (9.56%)
approaches used the same dataset which was created by
                                                             0.75     6,034            2,302        4,485
Pang et al. [9] and is the de facto standard for sentiment
                                                                      (11.85%)         (4.52%)      (8.81%)
analysis.   Note that the evaluation methodology and
                                                             0.875    5,767            1,326        4,431
the number of instances varies between the approaches
                                                                      (11.33%)         (2.6%)       (8.7%)
which makes it difficult to compare the results. Having
                                                               1      5,758            739          4,431
said that, our best accuracy is 4.55% lower than the best
                                                                      (11.31%)         (1.45%)      (8.7%)
reported result of 91.7% by Abbasi et al.[1].
    Our    approach    offers  several    key  advantages
                                                           Table 2: Number of selected features by each feature
though. Firstly, Abbasi et al's EWGA method is quite
                                                           selector for the various selection thresholds.
computationally expensive. Our best result, though less
accurate, is much more computationally efficient, and      PD and SWNSS were successful in maintaining clas-
can make both classification and training faster. Our      sification accuracy when used with appropriate thresh-
method is also much simpler and easier to implement.       olds, and SWNPD was able to maintain accuracy in all
Furthermore we start from a baseline that is 2% lower      cases except for three. PD in particular was able to
than Abbasi et al, which reduces the significance of       statistically significantly improve accuracy for nine out
the accuracy difference.      The next best accuracy of    of 12 combinations of classifiers and feature weights,
91% was achieved by K¨onig & Brill [6], who used           while SWNSS and SWNPD were able to improve ac-
pattern matching techniques.        Their method is also   curacy in three and one cases respectively.      Table 2
very computationally expensive and has the additional      shows the number of features selected by each feature
drawback of requiring human intervention.           Other  selection method at each threshold.
approaches in the literature tend to have an accuracy          From the results in Table 3 one might conclude that
that is similar to ours [7, 5, 9, 8], though without using PD was the best feature selection method. However,
feature selection.                                         Figures 2a and 2b provide more information.         They
                                                           show that at low thresholds PD is quite successful at
6.1     Comparison of Classifiers
                                                           improving accuracy for all of the feature weights, but at
Figure 1 shows the best accuracy for the two classi-       higher thresholds accuracy drops sharply. Conversely,
fiers with all the different feature weighting methods     both SWNSS and SWNPD have relatively flat lines,
and feature selection methods. For the unigram based       indicating that they are more able to find the most
feature weights, our results confirm the findings of Pang  effective features at any threshold.
et al.[9], which is that SVM classifiers are significanly
                                                           6.3     Comparison of Feature Weights
more accurate than NB classifiers. However, for the
word group based feature weights the results are less      Figure 2 a), c) and e) show the results for SVM for
clear. In 8 of the 12 best results for the word group      the three feature selection methods respectively, while
based feature weights, there was less than 0.5% differ-    Figure 2 b), d) and f) show the same for NB. The x-
ence between the NB and SVM classifiers, though in         axis corresponds to the feature selection threshold; as
the remaining four cases the SVM clearly performed         the threshold increases, the number of selected features
better. This finding shows that while SVM classifiers      decreases. The starting point marked with a `B' cor-
are substantially more accurate than NB classifiers for    responds to the baseline where no feature selection is
unigram based feature weights, they may not necessar-      used. In general we found FP was the most accurate
ily be the best approach for other types of features.      feature weighting method, which is in agreement with
                                                           the results of Pang et al.     [9].Interestingly, the ac-
6.2     Comparison of Feature Selectors
                                                           curacy of FF increased steeply when feature selection
Table 3 compares the results between the three feature     was applied.     We speculate that this was due to the
selectors and the baseline where no feature selection      presence of stop-words, so we conducted a further test
was used for both SVM and NB. The results show that        of FF with SVM and all words appearing in 1000 or


                                                           71

               
                                  

                                  

                                                                         

                                                                                                              
                                   
                                          
                                                                         
                                     

                                                                             

                                                                             

                                                                             

                                                                             

                                                                             

                                                                             

                                                                             

                                                                           

                                                                               
                       
    
     
                          
     
    
                            
   
    




                   (a)                                    (b)                                     (c)


Figure 1: Accuracy results (%) for SVM and NB when used with different feature selectors with different
thresholds and the six feature weighting methods.


                None        PD      SWNSS      SWNPD                      None        PD       SWNSS     SWNPD
   FF           72.5      85.5        81.3      79.85        FF           68.65     77.2         72.9      71
                          t=0.25     t=0.375    t=0.125                              t=0.5      t=0.875  t=0.125
   FP           85.95      87.15      85.3      83.55        FP           80.65       81.5       81.3     79.75
                         t=0.125       t=0      t=0.125                             t=0.25      t=0.125  t=0.125
   TF-IDF       85.9       85.6       86.55     82.95        TF-IDF        75.3     77.6         74.4     73.3 
                         t=0.125       t=0      t=0.125                             t=0.25      t=0.25   t=0.125
   SWN-SG       65.5     71.75        66.75      65.45       SWN-SG        59.9     65.2         63.1     60.05
                          t=0.25      t=0.5     t=0.125                            t=0.375        t=1    t=0.125
   SWN-PG       62.2      67.1        62.2         62        SWN-PG        62      67.35          62       62.1
                           t=0.5     t=0.125     t=0.5                               t=0.5      t=0.125   t=0.25
   SWN-PS       62.85    69.35        62.85      63.2        SWN-PS        62.7    69.25         62.7      62.9
                          t=0.25     t=0.375    t=0.125                             t=0.25        t=0    t=0.125

                       (a) SVM Results                                            (b) NB Results


Table 3: Comparison between the three feature selection methods and no feature selection for SVM and NB with
all six feature weightings. The best accuracy (%) for each feature selector is shown in bold with statistically
significant gains over the baseline marked with an up arrow () and statistically significant losses marked with a
down arrow ().


more documents removed. This achieved an accuracy            7    Conclusions
of 83.95%, which indicates that the case for ignoring
                                                             In this paper we empirically and systematically evaluate
FF is not as clear cut as the results of Pang et al. [9]
                                                             the performance of a number of feature selection and
suggest.
                                                             feature weighting methods for sentiment analysis. In
    Unigram based methods consistently outperformed
                                                             particular, we introduce two new feature selection
the SWN word group methods for both SVM and NB
                                                             methods - SWNSS and SWNPD - and compare them,
with all combinations of feature weights and selectors.
                                                             at a number of selection thresholds, with PD, a recently
This finding is in agreement with the findings by Pang
                                                             proposed method, shown to be very successful for
et al. [9] and Turney [14], who both noted that summing
                                                             topic-based classification.    We also introduce three
any out-of-context sentiment scores of individual words
                                                             feature weighting methods - SWN-SG, SWN-PG and
does not seem to capture the subtleties that exist in sub-
                                                             SWN-PS - and compare their performance with the
jective writing. The features produced by SWN-SG,
                                                             standard and popular FF, FP and TF-IDF methods. The
SWN-PG, and SWN-PS illustrate this point quite effec-
                                                             experiments are conducted using two classifiers, SVM
tively since they all have approximately equal scores for
                                                             and NB, on the movie review data set that has become
positive and negative words regardless of the sentiment
                                                             the de facto standard dataset for sentiment analysis.
of the document. This is shown in Figure 3, where we
                                                                 We achieved an accuracy of 87.15% using PD as
would expect the positive bars to be higher for posi-
                                                             a feature selector, FP as a weighting mechanism and
tive documents and the negative bars to be higher for
                                                             SVM as a classifier. This is a promising result as it
negative documents. Instead the bars are approximately
                                                             is comparable with previous state-of-the-art results
equal, indicating that there are about as many positive
                                                             but is much less computationally expensive. All the
and negative words in positive documents as there are
                                                             feature selectors we tested were able to improve the
in negative documents.


                                                             72

Accuracy (%)                                                   Accuracy (%)

   90                                                             90


   80                                                             80


   70                                                FP           70                                                 FP
                                                     TFIDF
                                                     FF                                                              FF
   60                                                             60                                                 TFIDF
                                                     SWN-SG                                                          SWN-SG
                                                     SWN-PG                                                          SWN-PG
                                                     SWN-PS
   50                                                             50                                                 SWN-PS


                                                     Threshold                                                       Threshold
     B         0.25       0.5        0.75          1                B         0.25        0.5         0.75         1
         (a) SVM - Categorical Proportional Difference                   (b) NB - Categorical Proportional Difference


Accuracy (%)                                                   Accuracy (%)

   90                                                             90


   80                                                 TFIDF       80
                                                      FF                                                             FP
                                                      FP                                                             FF
   70                                                             70                                                 TFIDF
                                                      SWN-SG                                                         SWN-SG
   60                                                 SWN-PS      60                                                 SWN-PS
                                                      SWN-PG                                                         SWN-PG

   50                                                             50


                                                      Threshold                                                      Threshold
     B    0       0.25       0.5        0.75         1              B   0        0.25        0.5       0.75         1
          (c) SVM - SentiWordNet Subjectivity Scores                      (d) NB - SentiWordNet Subjectivity Scores

Accuracy (%)                                                   Accuracy (%)

   90                                                             90


   80                                                TFIDF        80
                                                     FP
                                                     FF                                                              FP
   70                                                             70                                                 FF
                                                                                                                     TFIDF
                                                     SWN-SG                                                          SWN-PS
   60                                                SWN-PS       60                                                 SWN-PG
                                                     SWN-PG                                                          SWN-SG

   50                                                             50


                                                     Threshold                                                       Threshold
     B         0.25       0.5        0.75          1                B         0.25        0.5         0.75         1
        (e) SVM - SentiWordNet Proportional Difference                  (f) NB - SentiWordNet Proportional Difference


Figure 2: Accuracy results (%) for SVM and NB when used with different feature selectors with different
thresholds and the six feature weighting methods.




                                                               73

                                                                 methods: support vector learning.     MIT Press,
                                                                 Cambridge, MA, 1999.

                                                             [5] A. Kennedy and D. Inkpen.      Sentiment classifi-
                                                                 cation of movie reviews using contextual valence
                                                                 shifters. Computational Intelligence, Volume 22,
                                                                 Number 2, pages 110­125, May 2006.

                                                             [6] A. C K¨onig and E. Brill.    Reducing the human
                                                                 overhead in text categorization. In Proceedings of
                                                                 the 12th ACM SIGKDD international conference
                                                                 on Knowledge discovery and data mining, pages
                                                                 598­603, 2006.

                                                             [7] T. Mullen and N. Collier. Sentiment analysis using
                                                                 support vector machines with diverse information
                                                                 sources. In Proc. of the Conf. on Empirical Meth-
                                                                 ods in Natural Language Processing EMNLP,
                                                                 pages 412­418, 2004.
Figure 3: Average number of words with each SWN
positive and negative score, from each class of docu-
                                                             [8] B. Pang and L. Lee.     A sentimental education:
ments.
                                                                 Sentiment analysis using subjectivity summariza-
                                                                 tion based on minimum cuts. In Proc. of the ACL,
performance over the baseline without feature selection
                                                                 pages 271­278. ACL, 2004.
when   used   with   appropriate    weighting   methods.
Overall, PD was the most successful at improving
                                                             [9] B. Pang, L. Lee and S. Vaithyanathan. Thumbs
accuracy, although SWNSS was able to achieve the
                                                                 up?: sentiment classification using machine learn-
smallest feature sets whilst maintaining accuracy. The
                                                                 ing techniques.   In EMNLP '02: Proc. of the
unigram based feature weights - FP, FF and TF-IDF
                                                                 ACL-02 conf. on Empirical methods in natural
- outperformed SWN-SG, SWN-PG and SWN-PS.
                                                                 language processing, pages 79­86. ACL, 2002.
Overall, FP was the most successful feature weighting
method for both SVM and NB.                                [10] R. Prabowo and M. Thelwall. Sentiment analysis:
   Future work will include evaluating more feature              A combined approach. Journal of Informetrics,
selection methods, particularly some of the common               2009.
ones from text categorisation, such as information gain
and 2. It would also be valuable to combine some of        [11] F. Sebastiani. Machine learning in automated text
                                                                 categorization. ACM Comput. Surv., Volume 34,
the feature selectors to see if better feature sets can be
                                                                 Number 1, pages 1­47, 2002.
produced. Lastly, there would be significant value in
repeating these tests on another data set.
                                                           [12] M. Simeon and R. Hilderman. Categorical propor-
                                                                 tional difference: A feature selection method for
References                                                       text categorization.  In AusDM, pages 201­208,
                                                                 2008.
 [1] A Abbasi, HC Chen and A Salem.            Sentiment
     analysis in multiple languages: Feature selection
                                                           [13] R. M. Tong. An operational system for detecting
     for opinion classification in web forums.       ACM
                                                                 and tracking opinions in on-line discussions. In
     Transactions On Information Systems, Volume 26,
                                                                 Working Notes of the ACM SIGIR 2001 Workshop
     Number 3, 2008.
                                                                 on Operational Text Classification, pages 1­6,
                                                                 2001.
 [2] A. Esuli and F. Sebastiani.       SentiWordNet:    a
     publicly available lexical resource for opinion
                                                           [14] P. Turney. Thumbs up or thumbs down?: semantic
     mining. In Proc. of LREC 2006 - 5th Conf. on
                                                                 orientation applied to unsupervised classification
     Language Resources and Evaluation, Volume 6,
                                                                 of reviews. In ACL '02: Proc. of the 40th Annual
     2006.
                                                                 Meeting on ACL, pages 417­424. ACL, 2002.

 [3] V. Hatzivassiloglou and K. R. McKeown.           Pre-
                                                           [15] I. H Witten and E. Frank. Data Mining: Practical
     dicting the semantic orientation of adjectives. In
                                                                 machine learning tools and techniques.        San
     Proc. of the eighth conf. on European chapter of
                                                                 Francisco, Morgan Kaufman Publishers, 2005.
     the ACL, pages 174­181, 1997.

 [4] T. Joachims. Making large-scale support vector
     machine learning practical, Advances in kernel



                                                           74

          The Use of Topic Representative Words in Text Categorization


            Su Nam Kim        and Timothy Baldwin                                   Min-Yen Kan     

          Computer Science and Software Engineering                                Computer Science
                             NICTA VRL                                 National University of Singapore
                     University of Melbourne                             Singapore, 117417, Singapore
                     Victoria, 3056, Australia
                                                                        kanmy@comp.nus.edu.sg
      {snkim,tim}@csse.unimelb.edu.au


Abstract     We present a novel way to identify the rep-   7].  While the majority of research has used simple
resentative words that are able to capture the topic of    n-grams to represent documents [4], this has been
documents for use in text categorization. Our intuition    expanded in various ways, including word clusters [2],
is that not all word n-grams equally represent the topic   complex nominals [24], words from automatically
of a document, and thus using all of them can poten-       extracted sentences [21], and title words/keyphrases(or
tially dilute the feature space. Hence, our aim is to in-  keywords) [13].      Similarly, while most research has
vestigate methods for identifying good indexing words,     used simple term weighting (TF and/or TF·IDF
and empirically evaluate their impact on text catego-      variants), some have used attributes such as mutual
rization. To this end, we experiment with five differ-     information [18], chi-square [41], and gain ratio [7] to
ent word sub-spaces: title words, first sentence words,    weight and/or select features.
keyphrases, domain-specific words, and named entities.         Our interest is in the impact of different term types
We also test TF·IDF-based unsupervised methods for         on text categorization. Our intuition is that not all word
extracting keyphrases and domain-specific words , and      n-grams equally represent the topic of a document,
empirically verify their feasibility for text categoriza-  and thus using all of them can potentially dilute the
tion. We demonstrate that using representative words       feature space. Hence, our aim is to investigate methods
outperforms a simple 1-gram model.                         for identifying good indexing words, and empirically
                                                           evaluate their impact on text categorization. To find
Natural Language Techniques and Documents, Text
                                                           representative topic words, we tested five different
Categorization
                                                           word groups:        title words,   first sentence words,
                                                           domain-specific words, named entities, and keyphrases.
1    Background and Motivation
                                                           Title words and first sentence words are based on
Automatic text categorization is the task of classifying   the notion of document zoning.            Domain-specific
documents into a set of predefined categories. It is one   words and named entities, on the other hand, are
of the more heavily researched areas in natural language   typified as occurring with markedly-high occurrence in
processing (NLP) due to its immediate applicability in     documents of particular domains. Finally, keyphrases
applications such as text filtering [1], word sense dis-   are representative words, as identified by dedicated
ambiguation [11] and automated authorship attribution      methods such as [12] and [36]. We also test combining
and genre classification [8].                              the different term types with conventional terms
    The conventional approach to text categorization       n-grams.
utilizes supervised machine learners such as support           A secondary area of interest in this research is ex-
vector machines (SVMs) and Maximum Entropy (ME)            ploration of the utility of unsupervised term extraction
models, and represents each document as a bag of           methods.    As a result, we are particularly interested
word n-grams [40, 14, 10]. Empirically, SVMs have          in the utility of unsupervised keyphrase and domain-
been shown to be superior to other machine learning        specific word extraction methods on text categorization.
techniques such as Naive Bayes (NB), Rocchio and
decision trees over a range of tasks [40, 10].             2    Zone-based Term Extraction
    While    the   predominance    of research   in  text
                                                           Our first term extraction method is based on document
categorization is on machine learning models, there
                                                           zoning, i.e. the extraction of terms based on the docu-
has also been significant research on feature extraction
                                                           ment structure. A common approach in keyphrase ex-
[4, 2, 24, 21] and feature weighting/selection [18, 41,
                                                           traction and topic detection is to use titles as a represen-

Proceedings of the 14th Australasian Document Comput-      tation of the document topic. For example, [26] showed

ing Symposium, Sydney, Australia, 4 December 2009.         that sentences in particular article sections, such as the

Copyright for this article remains with the authors.       introduction and conclusion, contain more keyphrases
                                                           in scientific articles.



                                                         75

    In our work, we drew on methods such as [21] in                Word set      T1(.02)    T2(.04)      T3(.06)
extracting important sentences from documents based                 original     7,889       5,733        4,497
on the simple heuristic that the title and first sentence            1+NP        25,343      15,257      10,679
often contains key facts about the news story. From
these observations, we select the title words and first
sentence words as candidate terms. In each case, we ex-            Table 1: Number of collected keyphrases
tract out the component 1-grams, to minimize reliance       implications both in terms of resource creation and
on parsing or manual processing. We also filter terms       domain adaptability. We are interested in minimizing
by their combined occurrence in the document set, se-       such efforts, and thus committed to using unsupervised
lecting only those terms which occur with frequency         or minimally-supervised methods.           To the best of
 1, 2 or 3. The final number of title words is 8,622,       our knowledge,      very few unsupervised keyphrase
3,878, and 2,357, for cutoffs of 1, 2 and 3, respectively,  extraction methods exist.       Therefore, we used the
and the corresponding number of first sentence words is     features used in KEA to build our own unsupervised
11,565, 5,819, and 3,905, respectively. These numbers       keyphrase extractor. That is, we use TF·IDF and first
are based on the evaluation data described in Section 6.    position, i.e. the inverse of the offset from the start of
                                                            the document, such that documents which occur earlier
3     Keyphrases                                            in the document are preferred as keyphrase candidates.
Keyphrases are simplex (i.e. 1-gram) nouns or noun          First, we calculate the score for each candidate as

phrases that represent the key ideas of the document.       shown in (1), combining TF·IDF and first position.

Keyphrases can serve as a condensed summary of the
document and also as high-quality index terms. In the                                     first position of Wi )
past, the majority of keyphrase studies have used three       Score = TF·IDF + (1 -                               (1)
                                                                                             # of total terms
types of statistics to extract keyphrases: (1) document
co-occurrence, i.e. TF·IDF-style statistics relating            We then extract the top-N candidates as keyphrases.
keyphrases to their relative co-occurrence across           In other keyphrase extraction research, N has typically
documents [12, 26]; (2) keyphrase co-occurrence,            been set to 15, but in our case, we decided to experiment
i.e. the extent to which keyphrases occur together in the   with different thresholds.    This is because the docu-
same documents [37]; and (3) term co-occurrence,            mentsusedintextcategorizationtestbedsareshort,and
i.e. local contiguity of terms in keyphrases [28].          thus result in comparatively few keyphrase candidates.
    We quickly summarize related work first.         KEA    We selected thresholds by examining the score drop.
[12] is a very simple and popular keyphrase extraction      Specifically, we set the threshold to the point at which
and indexing tool. It uses two main features: TF·IDF        the number of domain-specific terms gained at the cur-
to capture document co-occurrence, and distance to          rent similarity value is no more than a fixed proportion
signify the relative locality of keyphrase occurrences      (e.g. 2%) of keyphrases previously selected. Due to this
within documents. These features have been broadly          use of threshold, our keyphrase extractor did not assign
used in keyphrase extraction, e.g. by [37] in addition      any keyphrases for a few documents.
to keyphrase co-occurrence. [26] extended the basic             Keyphrases    can   be   either    simplex   nouns or
KEA approach by applying linguistic features such as        NPs.   [13] found that breakdown-keyphrases (i.e. all
document zones.       GenEx [36] uses more syntactic        unigrams contained within a keyphrase) performed
features, such as document positions and stemming. [3]      better for text categorization. Hence, we also convert
uses head noun-based heuristics. [35] use modelling         keyphrases into their component unigrams. However,
based on information loss between preceding and             we observed that whole keyphrases are often better
proceeding document extents. Textract [28] ranks            descriptors of the document topic (e.g. import goods
keyphrase candidates by their degree of domain-             vs. goods). Thus, we tested another set, called 1+NP,
specificity and term cohesion in a text analysis system.    which combines 1-grams with the original keyphrases.
[38] uses information from clustered documents for              Table 1 shows the number of collected keyphrases
keyphrase extraction over single documents.                 for the entire document collection (see Section 6)
                                                            at different threshold settings, for both the original
3.1     Unsupervised Keyphrase Extraction                   keyphrases and 1+NP. Figure 1 additionally shows the

As keyphrases are known to be representative of             proportion of documents containing different numbers

document topics, it is also natural to use them as terms    of keyphrases for the three thresholds.

for document categorization. Hulth and Megayesi [13]            To assess the quality of our unsupervised keyphrase

used a supervised keyphrase extraction method, seeded       extractor, we sampled 100 documents from the training

with 500 abstracts annotated with keyphrases. To avoid      data and had two human annotators manually assign

documents without keyphrases, they controlled the           keyphrases to 50 documents each. The total number of

number of keyphrases to between 3 and 12.                   manually-assigned keyphrases in the 100 sample docu-

    While supervised techniques work well,           they   ments was 1,486. Performance is shown in Table 2.

require manually-built annotated corpora, which has



                                                          76

                                                                                             T1(.02)       T2(.04)           T3(.06)
      # of documents (%)      16                                                           Method        Term set
                                          Threshold 1
                                          Threshold 2
      14                                                              D1          original    2,918          1,573             1,157
                                          Threshold 3

      12
                                                                                   1+NP       3,969          1,918             1,344
      10
       8                                                              D2          original    3,692          2,759             2,368
                                                                                   1+NP       7,169          5,021             4,215
       6

       4

       2

       0                                                           Table 3: Number of collected domain-terms words
        0         5        10    15     20          25
                                            # of Keyphrases

                                                                                    Overlap        D1              D2
                                                                              T1     1,612      55.24%          43.67%
Figure 1: Proportion of documents assigned differing
                                                                              T2       593      37.70%          21.49%
numbers of keyphrases
                                                                              T3       404      34.92%          17.06%
                       Precision   Recall      Fscore
        T1(.02)         9.76%     23.85%      13.85%
                                                                 Table 4: Overlap between domain-specific words col-
        T2(.04)         15.32%    15.62%      15.47%
                                                                 lected by D1 and D2
        T3(.06)         21.02%    10.86%      14.32%
                                                                       # of domains
                                                                      30
                                                                                                               Threshold 1
                                                                                                               Threshold 2
                                                                                                               Threshold 3
                                                                      25
     Table 2: Performance of keyphrase extraction
                                                                      20

4    Domain-Specific Terms
                                                                      15


Automatic domain-specific term extraction is a classi-                10


fication process where the terms are categorized using                 5


a set of predefined domains with supervised machine                    0
                                                                         0     5    10   15   20    25    30      35      40
                                                                                             # of domain-specific words in a domain
learning models. It has been studied for application in
areas such as keyphrase extraction [12, 38] and word
sense disambiguation [19].                                       Figure 2:      Number of domains containing differing
    Much of the work has been carried out using su-              numbers of domain-specific terms for D1
pervised machine learning techniques in the context of
                                                                     Table 3 details the number of terms and 1+NP ex-
term categorization and/or text mining. [9] focused on
                                                                 tracted by D1 and D2 over the document collection de-
simplex terms using corpus comparison, and verified
                                                                 scribed in Section 6, over three different threshold val-
the collected data using automatic and manual valida-
                                                                 ues. We also calculated the overlap in terms extracted
tion. [31] projected the categorized terms onto a pre-
                                                                 by the two methods, and report the numbers in Table 4.
defined set of semantic domains exploiting web knowl-
                                                                 The numbers in the second and third columns show the
edge, and used the context to map the terms onto do-
                                                                 portion of terms extracted by the D1 and D2, respec-
mains. [29] proposed an unsupervised method for ex-
                                                                 tively, which overlap with terms extracted by the second
tracting domain-specific terms, and used them to check
                                                                 method.
word and keyword error rates.
                                                                     The number of domains containing differing num-
    In this paper, we test two unsupervised domain-
                                                                 bers of terms is shown in Figures 2 and 3. D1 pro-
specific word extraction approaches, drawing on work
                                                                 duced less domain-specific words in total (as shown in
in the context of keyphrase extraction [16]. The first
one (D1) is based on simple TF·IDF. The second                   Table 3), but the keyphrases are better distributed across
                                                                 the domains.
method (D2) was proposed by [29], and is based on the
                                                                     In separate research, we manually evaluated the
difference in TF for a given domain relative to other
                                                                 terms extracted by the two methods, and found that D1
domains, based on:
                                                                 marginally outperformed D2 [16].
                                           cd(w)
      D2 = domain specificity(w) =           Nd             (2)
                                           cg(w)                 5    Named Entities
                                             Ng
                                                                 Named entity recognition is the task of identifying
where cd(w) and cg(w) denote the number of occur-                atomic elements in a document which belong to
rences of term w in the domain text and general docu-            predefined categories such as location, person, and
ment collection, respectively. Nd and Ng are the num-            organization. It has been applied to contexts including
bers of terms in the domain corpus and in the general            Question-Answering         (QA)     [23]       and       information
corpus, respectively. If term w does not occur in the            retrieval [34].      The standard approach is based on
general corpus, then cg(w) is set to 1; otherwise it is set      structured     classification  methods         such       as   hidden
to the highest count in the general corpus.                      Markov models (HMMs) or conditional random fields
    We use the same thresholding method for the two              (CRFs).       Recently, research has focused on semi-
methods as described in Section 3.1.




                                                               77

     # of domains
                                                                    Lewis Split, comprising 7,771 training and 3,019 test
     40                                     Threshold 1
                                                                                                       2
                                            Threshold 2
                                                                    documents across 90 domains.
                                            Threshold 3
     35

                                                                        In preprocessing,      we performed part-of-speech
     30

     25
                                                                    (POS) tagging using the Lingua POS tagger, and POS-
     20
                                                                    sensitive lemmatization using morpha [22]. Then we 3
     15

                                                                    built classifiers using SVM           ,
     10                                                                                              light 4 with TF·IDF term
       5
                                                                    weighting in an attempt to generate as competitive as
       0
        0      5   10  15  20    25    30      35      40
                          # of domain-specific words in a domain    possible a text categorization system.
                                                                        As our benchmark,           we use 1-grams with a
                                                                    frequency cutoff of 1, 2 and 3 (i.e. all terms occurring
Figure 3:      Number of domains containing differing
                                                                    less than N times are ignored), along with stopping.
numbers of domain-specific terms for D2
                                                                    The best results were achieved for a frequency cutoff
    Length       F1(f  1)     F2(f  2)            F3(f  3)          of 3, with a micro-averaged F-score of 78.54%.

    original       11,431        6,538                 4,650            Table 6 shows the text categorization performance

     1+NP          23,440        9,883                 6,234        of the various term extraction methods, organized into
                                                                    four groups: (1) individual extraction methods; (2) the
                                                                    combination of all extraction methods; (3) the combi-
     Table 5: Number of extracted named entities                    nation of individual extraction methods with 1-grams;

supervised [27] and/or unsupervised approaches [5] to               and (4) the combination of all extraction methods with

named entity recognition.                                           1-grams. In each case, we report the micro-averaged

    The relevance of named entities (NEs) to this re-               precision, recall and F-score ( = 1) for the given

search is that we expect they will be indicative of doc-            method over the test data. All values which surpass

ument domains. For example, Gulf and Kuwait often                   the benchmark performance (F3) at a level of statistical

occur in the domain of oil and not other domains. Thus,             significance (based on approximate randomisation, p <

we trial named entities as a term type in text categoriza-          0.05) are indicated in bold. In Table 6, F1, F2 and F3

tion.                                                               refer to the three frequency cutoffs used for title words,

    We experiment exclusively with the named entity                 first sentence words and named entities (f  1, 2, 3),
recognition software of the University of Illinois                  while T1, T2 and T3 refer to the three thresholds used

at  Urbana-Champaign       (UIUC NER).          1     UIUC NER      for keyphrases and domain-specific words.             We also

makes extensive use of non-local features and external              present the performance over the top-10 topics in Ta-

knowledge resources (i.e. gazetteers extracted from                 ble 7.

Wikipedia),      as well as semi-supervised learning.
It identifies four entity types (i.e. person, location,             7    Text Categorization Results
organization and miscellaneous), and is reported to
                                                                    Looking first at the individual methods (the top section
have achieved 90.80 F1-score over the CoNLL-03
                                                                    of Table 6), we notice that only keyphrases were able to
NER shared task
                                                                    surpass the performance of the benchmark, closely fol-
    Table 5 shows the number of named entities
                                                                    lowed by title and first sentence words, then named en-
extracted by UIUC NER over our document collection
                                                                    tities, and finally domain-specific terms. Almost no dif-
(see Section 6).     We used three different frequency
                                                                    ference was observed between using the original terms
cutoffs to select the candidate NEs (fNE  1, 2, 3),
                                                                    extracted by each of the methods, and combining the
and once again experimented with both the original
                                                                    original terms with their unigram components (1+NP).
NEs and the 1+NP method of breaking down the NEs.
                                                                    In general, the standalone methods tended to do bet-
                                                                    ter in terms of both precision and recall for lower cut-
6    Text Categorization                                            off/threshold values, that is larger numbers of noisier

We now describe our integrated approach for perform-                terms tended to boost performance across the board.

ing text categorization, incorporating the various ex-                  When we combine all five term extraction methods

tracted term types from the preceding sections.                     (considering D1 and D2 separately),               the results

    As our dataset, we use the Reuters newswire corpus,             exceed those of the benchmark in all cases for the

with 21,450 articles from 1987, spanning 135 topics.                lowest threshold/cutoff values, and in select cases

The number of articles with no category label, one label            for higher values.     None of these gains were found

and multiple labels are 31%, 57% and 12%, respec-                   to be statistically significant, and yet the result is

tively. This dataset has been used widely for text cat-             encouraging as the best of the combined methods

egorization research. In particular, we use the Modified            outperforms the best of the standalone methods,

                                                                       2
   1                                                                    http://www.daviddlewis.com/resources/
    http://l2r.cs.uiuc.edu/~cogcomp/asoftware.
                                                                    testcollections/reuters21578/
php?skey=FLBJNE
                                                                       3The only use we made of the POS tags was in lemmatization.
                                                                       4http://svmlight.joachims.org/svm\
                                                                    _multiclass.html




                                                                  78

                                         F1/T1                       F2/T2                       F3/T3
   Word              Length     Prec.    Recall   Fscore    Prec.    Recall   Fscore    Prec.    Recall    Fscore

   Benchmark            1      87.15%    70.26%   77.80%   87.48%   70.53%    78.09%   87.98%   70.93%    78.54%

   Title (T)            1      87.48%    70.53%   78.09%   87.58%   70.61%    78.18%   87.58%   70.61%    78.18%
   First (F)            1      87.58%    70.61%   78.18%   87.48%   70.53%    78.09%   87.35%   70.42%    77.98%
   Keyphrase (K)        1      88.01%    70.95%   78.57%   87.45%   70.50%    78.07%   87.68%   70.69%    78.27%
                      1+NP     87.78%    70.77%   78.36%   87.65%   70.66%    78.24%   87.65%   70.66%    78.24%
   Domain (D1)          1      86.26%    69.54%   77.00%   85.70%   69.08%    76.50%   83.44%   67.27%    74.49%
                      1+NP     86.26%    69.54%   77.00%   85.70%   69.08%    76.50%   83.44%   67.27%    74.49%
   Domain (D2)          1      84.67%    68.26%   75.58%   82.78%   66.73%    73.90%   81.75%   65.91%    72.98%
                      1+NP     84.67%    68.26%   75.58%   82.78%   66.73%    73.90%   81.75%   65.91%    72.98%
   NE (N)               1      86.16%    69.46%   76.91%   85.53%   68.95%    76.35%   84.87%   68.42%    75.76%
                      1+NP     86.32%    69.59%   77.06%   85.53%   68.95%    76.35%   85.17%   68.66%    76.03%
   T+F+K+D1+N           1      87.98%    70.93%   78.54%   87.91%   70.87%    78.48%   87.78%   70.77%    78.36%
                      1+NP     88.11%    71.03%   78.66%   87.72%   70.71%    78.30%   87.91%   70.87%    78.48%
   T+F+K+D2+N           1      88.05%    70.98%   78.60%   87.95%   70.90%    78.51%   88.01%   70.95%    78.57%
                      1+NP     88.15%    71.06%   78.69%   88.08%   71.01%    78.63%   88.25%   71.14%    78.77%
   B3+Title             1      87.72%    70.71%   78.30%   87.85%   70.82%    78.42%   87.55%   70.58%    78.15%
   B3+First             1      87.78%    70.77%   78.36%   87.62%   70.63%    78.21%   87.82%   70.79%    78.39%
   B3+Keyphrase         1      88.18%    71.09%   78.72%   87.85%   70.82%    78.42%   88.05%   70.98%    78.60%
                      1+NP     88.31%    71.19%   78.83%   88.38%   71.25%    78.89%   88.15%   71.06%    78.69%
   B3+D1                1      87.95%    70.90%   78.51%   88.08%   71.01%    78.63%   87.95%   70.90%    78.51%
                      1+NP     87.95%    70.90%   78.51%   88.08%   71.01%    78.63%   87.95%   70.90%    78.51%
   B3+D2                1      87.45%    70.50%   78.07%   87.32%   70.59%    77.95%   87.68%   70.69%    78.27%
                      1+NP     87.45%    70.50%   78.07%   87.32%   70.59%    77.95%   87.68%   70.69%    78.27%
   B3+NE                1      87.58%    70.61%   78.18%   87.68%   70.69%    78.27%   87.98%   70.93%    78.54%
                      1+NP     87.58%    70.61%   78.18%   87.65%   70.66%    78.24%   87.45%   70.50%    78.07%
   B3+T+F+K+D1+N        1      88.28%    71.17%   78.80%   88.31%   71.19%    78.83%   88.25%   71.14%    78.77%
                      1+NP     88.44%    71.30%   78.95%   88.15%   71.06%    78.69%   88.21%   71.11%    78.75%
   B3+T+F+K+D2+N        1      88.31%    71.19%   78.83%   88.28%   71.17%    78.80%   88.48%   71.33%    78.98%
                      1+NP     88.44%    71.30%   78.95%   88.38%   71.25%    78.89%   88.48%   71.33%    78.98%



                                    Table 6: Performance of text categorization

          Benchmark (F3)    Individual     Individual+1-grams    All candidates    All candidates+1-grams
              89.55%         89.59%              89.96%             90.02%                 90.07%


                                    Table 7: Performance over the top-10 topics


suggesting that there is complementarity between the           Looking to the results over the top-10 topics, we
term extraction methods. Comparing D1 and D2, our          find a similar trend, with keyphrases producing the best
simple TF·IDF-based unsupervised term extraction           standalone performance, and all term extraction meth-
method is marginally superior to D2 (the method of         ods combined with 1-grams producing the best overall
[16]).                                                     performance.
    Next, when we combine the individual methods
with the terms from the benchmark method, the              8    Conclusions
results improve uniformly, with the best-performing
                                                           In this work,     we evaluated the impact on text
method (keyphrases with 1+NP terms) surpassing the
                                                           categorization of five representative term extraction
benchmark method at a level of statistical significance.
                                                           methods, namely title words, first sentence words,
This indicates that keyphrases, as extracted using
                                                           keyphrases,     domain-specific   words,    and   named
our adaptation of KEA, can successfully complement
simple 1-grams in text categorization.                     entities. We used the output of the different methods,
                                                           either individually or in combination, as the source of
    Finally, when we combine the benchmark term
                                                           terms for text categorization, and verified that we were
representation with all of the term extraction methods,
                                                           able to achieve statistically significant improvements
we again achieve statistically significant gains almost
                                                           over a benchmark text categorization method using
50% of the time, once again pointing to the utility
                                                           either keyphrase extraction in combination with the
of term extraction methods in text categorization
                                                           benchmark term representation, or the combination
applications. Comparing these results with those for
                                                           of all term extraction methods, again in combination
the standalone term extraction methods combined with
                                                           with the benchmark term representation. On the basis
the benchmark system, the full set of five methods is not
                                                           of this, we concluded that keyphrases were the pick
able to improve significantly beyond the performance
                                                           of the terms experimented with, but also that there is
of keyphrase extraction with the benchmark system.
                                                           complementarity between the different term types.



                                                         79

Acknowledgements                                           [11] G. Escudero and L. Marquez and G. Rigau, Boost-
                                                               ing applied to word sense disambiguation, In Pro-
NICTA is funded by the Australian Government as rep-
                                                               ceedings of 11th European Conference on Machine
resented by the Department of Broadband, Communi-
                                                               Learning, 2000, pp. 129­141.
cations and the Digital Economy and the Australian Re-
search Council through the ICT Centre of Excellence        [12] E. Frank and G.W. Paynter and I. Witten and C.
program.                                                       Gutwin and C.G. Nevill-Manning, Domain Spe-
                                                               cific Keyphrase Extraction, In Proceedings of the
References                                                     16th IJCAI, 1999, pp. 668­673.

[1] G. Amati and D. DAloisi and V. Giannini and            [13] A. Hulth and B. Megayesi, A Study on Automati-
    F. Ubaldini, A framework for filtering news and            cally Extracted Keywords in Text Categorization,
    managing distributed data, Journal of Universal            In Proceedings of the 21st COLING/ACL, 2006,
    Computer Science, 1997, 3(8), pp. 1007­1021.               pp. 537­544.

[2] L.D. Barker and A.K. McCalluma, Distributional         [14] T. Joachims, Text categorization with support
    clustering of words for text categorization, In Pro-       vector machines:    Learning with many relevant
    ceedings of 21st ACM International Conference              features, In Proceedings of ECML, 1998, pp. 137­
    on Research and Development in Informatoin Re-             142.
    trieval, 1998, pp.96­103.
                                                           [15] M. Kida, M. Tonoike, T. Utsuro and S. Sato,
[3] K. Barker and N. Corrnacchia, Using noun phrase            Domain Classification of Technical Terms Using
    heads to extract document keyphrases, In Proceed-          the Web, Systems and Computers, 2007, 38(14),
    ings of the 13th Biennial Conference of the Cana-          pp. 2470­2482.
    dian Society on Computational Studies of Intelli-
    gence: Advances in Artificial Intelligence, 2000,      [16] S. Kim, T. Baldwin and M-Y. Kan, An Unsuper-
    pp. 40­52.                                                 vised Approach to Domain-Specific Term Extrac-
                                                               tion, In Proceedings of the Australasian Language
[4] W.B. Cavnar and J.M. Trenkle, N-gram-based text            Technology Workshop 2009, to appear.
    categorization, In Proceedings of SDAIR, 1994, pp.
    161­175.                                               [17] Y. Ko and J. Park and J. Seo, Improving text
                                                               categorization using the importance of sentences,
[5] M. Collins and Y. Singer, Unsupervised Models for          Information Processing and Management, 2004,
    Named Entity Classification, In Proceedings of the         40(1), pp. 65­79.
    Joint SIGDAT Conference on Empirical Methods
    in Natural Language Processing and Very Large          [18] D.D. Lewis, An evaluation of phrasal and clus-

    Corpora, 1999, pp. 100­110.                                tered representations on a text categorization task,
                                                               In 15th ACM International Conference on Research
[6] D. Okanohara and Y. Miyao and Y. Tsuruoka                  and Development in Informaton Retrieval, 1992,
    and J. Tsujii, Improving the Scalability of Semi-          pp. 37­50.
    Markov Conditional Random Fields for Named En-
    tity Recognition, In Proceedings of COLING/ACL,        [19] B. Magnini and C. Strapparava and G. Pezzulo

    2006, pp. 465­472.                                         and A. Gliozzo, The role of domain information
                                                               in word sense disambiguation, Natural Language
[7] F. Debole and F. Sebastiani, Supervised term               Engineering, 2002, 8(4), pp. 359­373.
    weighting for automated text categorization, In
                                                           [20] Y. Matsuo and M. Ishizuka, Keyword Extrac-
    18th ACM Symposium on Applied Computing,
                                                               tion from a Single Document using Word Co-
    2003, pp.784­788.
                                                               occurrence Statistical Information, International
[8] J. Diederich and J. Kindermann and E. Leopold and          Journal on Artificial Intelligence Tools, 2004,
    G. Paass, Authorship attribution with support vec-         13(1), pp. 157­169.
    tor machines, Applied Intelligence, 2003, 19(1/2),
                                                           [21] R. Mihalcea and S. Hassan, Using the essence
    pp.109­123.
                                                               of texts to improve document classification, In
[9] P. Drouin, Detection of Domain Specific Terminol-          Proceedings of RANLP, 2005.
    ogy Using Corpora Comparison, In Proceedings of
                                                           [22] G. Minnen and J. Carroll and D. Pearce, Applied
    the 4th LREC, 2004, pp. 79­82.
                                                               morphological processing of English, Natural Lan-
[10] S. Dumais and J. Platt and D. Heckerman and M.            guage Engineering, 2001, 7(3), pp. 207­223.
    Sahami, Inductive learning algorithms and repre-
                                                           [23] D. Molla and M. van Zaanen and D. Smith,
    sentations for text categorization, In Proceedings of
                                                               Named Entity Recognition for Question Answer-
    CIKM, 1998, pp. 148­155.
                                                               ing, In Proceedings of ALTW, 2006, pp. 51­58.



                                                         80

[24] A. Moschitti and R. Basili, Complex linguistic        [37] P. Turney, Coherent keyphrase extraction via Web
    features for text classification, In Proceedings of        mining, In Proceedings of the 18th IJCAI, 2003,
    26th European Conference on Information Re-                pp. 434­439.
    trieval Research, 2004, pp.181­196.
                                                           [38] X. Wan and J. Xiao, CollabRank: towards a col-
[25] D. Nadeau and P.D. Turney and S. Matwin, Un-              laborative approach to single-document keyphrase
    supervised Named-Entity Recognition: Generating            extraction, In Proceedings of COLING, 2008, pp.
    Gazetteers and Resolving Ambiguity, In cogprints,          969­976.
    2006, pp. 266­277.
                                                           [39] I. Witten and G. Paynter and E. Frank and C.
[26] T. Nguyen and M.Y. Kan, Key phrase Extraction             Gutwin and G. Nevill-Manning, KEA:Practical
    in Scientific Publications, In Proceeding of Interna-      Automatic Key phrase Extraction, In Proceedings
    tional Conference on Asian Digital Libraries, 2007,        of the fourth ACM conference on Digital libraries,
    pp. 317-326.                                               1999, pp.254­256.

[27] S. Pakhomov, Semi-Supervised Maximum En-              [40] Y. Yang and X. Liu, A re-examination of text
    tropy Based Approach to Acronym and Abbrevi-               categorization methods, In Proceedings of SIGIR,
    ation Normalization in Medical Texts, In Proceed-          1997, pp. 42­49.
    ings of 40th ACL, 2002, pp. 160­167.
                                                           [41] Y. Yang and J.O. Pedersen, A comparative study
[28] Y. Park and R.J. Byrd and B. Boguraev, Automatic          on feature selection in text categorization, In Pro-
    Glossary Extraction Beyond Terminology Identifi-           ceedings of 14th International Conference on Ma-
    cation, In Proceedings of COLING, 2004, pp. 48­            chine Learning, 1997, pp. 412­420.
    55.

[29] Y. Park and S. Patwardhan and K. Visweswariah
    and S.C. Gates, An Empirical Analysis of Word
    Error Rate and Keyword Error Rate, In Proceedings
    of International Conference on Spoken Language
    Processing, 2008, pp. 2070­2073.

[30] L. Ratinov and D. Roth, External Knowledge and
    Non-local Features in Named Entity Recognition,
    In Proceedings of NAACL, 2009.

[31] L. Rigutini and E. Di Iorio and M. Ernandes and
    M. Maggini, Automatic term categorization by ex-
    tracting knowledge from the Web, In Proceedings
    of 17th ECAI, 2006, pp. 531­535.

[32] G. Salton and A. Wong and C.S. Yang, A vector
    space model for automatic indexing, Communica-
    tions of the ACM, 1975, 18(11), pp. 61­620.

[33] F. Sebastiani, Machine learning in automated text
    categorization, ACM Computering Surveys, 2002,
    34(1), pp. 1­47.

[34] S. Sekine and K. Sudo and C. Nobata, Extended
    Named Entity Hierarchy, In Proceedings of LREC,
    2002.

[35] T. Tomokiyo and M. Hurst, A Langauge Model
    Approach to Keyphrase Extraction, In Proceed-
    ings of ACL Workshop on Multiword Expressions,
    2003, pp.33­40.

[36] P. Turney, Learning to Extract Keyphrases from
    Text, In National Research Council, Institute for
    Information Technology, Technical Report ERB-
    1057, 1999.




                                                         81

   
     
 
                      


             
   
     
                   1      1      1           2

                 1  
 "*
                 <*    "*
                > @*  "*
                    > \^^^ _

                
 
 "#$&
                 2 `
  
 
                     @* {
                    ` }^~\  

                     $**+


 ;  <<
  <<  <                 
    
 <<*
  =  >=
                 " 
Z[;
 *     \ *                   
**   <
   *              
 @        
>  \ ]^< *<  <<*      @ 
  
  
   
   >>   &  <<             " @        
    \ " >                       
 <<
     "< "             @   
    
* _
  >    *                  
 *  
 
   `**   Z[;               @     @ * 
<<* "  <<*    **        _     
 
 |"<   * >                  @ 
*   
 
\ ]^< **                        
                              *  @      <
                            
    
   
 
 

                              @     
  
 

 
 
 * 
                             
       @  
                            *          
                               
       *

 
                                         

                            _       
 
 
     
   
                
   

 <                             

        <                   
  

   @                      
  
 
     "                    
 
  @

   * <    
                                  
 
  

                                 @
   
    
                                    
    ! "##$%   
                              {  @* 
    %




                          82

" 
*               *   " 
  
 
    
   *      
 
  
  
           
  
 
 
   
  
     
         *  @
  

                           
 
   
                              
  *
   @ 
    
   
          
   
*  
   
*         " & 
    
 
             
         * 
            
   
   
    < ^                         
  "*  
   @ 
    
              
    
 
   

*  * 
   @ 
   
   *  "     *   
       
  
 <         *  
 

    
   
 
         *   
    

   
      *   
           

    
                  "*  *  
 
 _* 
                            *   { " *   

"    @ 
  @ " 
                             

* @   
   
    
* 
   

                            

   @ _    
* 


    
* * @  
  
    @*{{ 
                                          c         c
 
  *  
 `                    R = , and P =
                                          N         n
   @     
 
   
  <
                    

       
  _       

  

*        *   
  *  
 
    @*   
*  Z   
        
   @ 
    
    
  
     
*
  
 
   @ " 
           *  * @*_ 
  
    *
  
     * <   \  
    @
 * @       
 
 
 _* @* 
  *   
   
  @         _    
  

 
*        
 @   *        

     *  @   * <   *   @*    

 
      
 
       _ _
  @  
  
    @       

 
  
@        *      "   
 
    
  
 * 
    

 _  
    
  
        
 *      
                      ¡ ¢ £   £   * 
 
 *
                      
 
 _    
  " @      
 
     @ *       
         *  @    * * 
   *  
                  
 * "  ¡ \  
  
 @  
  
         

 
              " 

  
        * @      @  *
        
               

   
     
     * @      
   
 " @                    

 
 *         @ 
  
*    


  ~   
 " 
             




                          83

 * 
 
    
     
 "     
        " *        @ 
   
 *    * 
   * 
  
    
" 
                                                  "   *    } "   
                                                                  
                                                              
    @  
 

                                                                      
                                      freq(xy)
        A(xy) = MI(x, y) = log2(         N       )          @@  
 
     
                                  freq(x) freq(y)
                                     N      N                   " * |   
                       p(xy)
               log2(          )                             L|R      ¡     
                                                         
                      p(x)p(y)                               
*

                                                               @       "
  A(xy)      
 
                                                            *   *  
   
xy freq(x)   *      
@  freq(xy)   *                                                             BC(L|R) = MI(L, R)
    * *     Z                                                                            = sgn  (A(LR))2                  \
    @     
  
*   x                                                                         8
freq(x)/N                                                                              <-  1,  if A(LR) < 0
                                                                               sgn =
    ¤   `                                                                       :1
                                                                                          ,    if A(LR) >= 0
@  
@ 
 
 
  

 
    *                                                         
 S  

                                                                                                                  i
                                                            Si+1 "    
 
      
 Improved MI(xy) = 0.39  log2(p(xy)) - 0.28  log2(p(x))     
      * * 

                                -0.23  log2(p(y)) - 0.32         * @   

                                                         
                                                             
 @      
 " 
     
  
                                        
    *      *  
      
 
 
                                      
     
   
"*  @  
                                                    

 
    *                                                < *    @
 
 
@     vxyz    
                                                                                   s = c1c2c3 ··· cici   ···cn              ~

 xy   
                                                                                          +1


                                                               s          
     CI(xy) = 0.35  log2(p(xy)) + 0.37  log2(p(v))           " 
*   *   *   
                                                             c c             @        
           + 0.32  log2(p(z)) - 0.36  log2(pdocwt(vx))                  i i+1
                                                            @  
     
   
           - 0.29  log2(pdocwt(yz)) + 5.91               
                                                             
 S  @

  p              *  @  
        docwt
  
   *  *                                                 S = [c1c2 ··· ci] | [ci+1 i+2
                                                                                       c   ···ci +k ] |···| [cn
                                                                                                              -m n-m+1
                                                                                                                c     ···cn]

     
  " 
                                                                = S1S2 ··· Sx                                               ¦

 
   @   
  
 "   ¢¥ 
@
  @ 
                                        [c1c2c3 · · · cl]  Si,   
 
  
 
                                                         s       

                                                                     *  

                                                            

' *+ /                                                   
 "    * 
                                                            
  *  @ " 
 

" @
  
   
 
  
 
    
        
      *     
                                          NGMI(S)        =     [BC(S1|S2), BC(S2|S3), ··· , BC(Sn -1 |Sn)]
                                                                                   n
@ 
  
 
 
                                                           =       BC(S |Si      )
                                                                                           i   +1

 
   
 
  @                                                               i=1
                                                                                   n
   
 
  
 *                                                          =       MI(S , S       )
                                                                                           i
                                                                                  i=1          i+1                          ¢




                                                          84

   @  *  ¤    `                                             
 
 
  *                                                                       n-1
                                                                        NGMI       (S) =   MI      (S , S   )
                                                                               pair            pair  i   i+1      
   
 " 
  
 @
                                                                i=1

 
     
 
   
                                                               n-1
                                                                        NGMIsum(S) =       MIsum(Si, Si   +1)     \
       
  <* *                                                                    i=1

                                                                               n-1
                                                                        NGMI       (S) =   MI      (S , S   )
                                                                               min             min   i   i+1      ~
   @   * *                                                                         i=1

  
  
                                                                      n-1
                                                                        NGMImax(S) =       MImax(Si, Si   +1)     ¦
     
    *                                                                    i=1

   @                                                                                  n-1
                                                                        NGMImean(S) =      MImean(Si, Si      )   ¢
                                                                                         i=1               +1
  MI     , MI      , MI     , MI      and MI         
     pair      sum      min        max          mean
                                                                  @ @       

  @      S     
                                          i
                                                                        
  
 * 
  S          i+1    @
                                                                  @    < *     
                                                                *   @  @   * 
    MI    (S , S   ) = MI(C          (S ), C        (S    ))
      pair  i  i+1          rightmost  i    leftmost  i+1               
 _  

                     = MI(Cn(Si), C1(Si +1 ))                £   
*    @   


   C (S )  C                   (S )    
 
          n   i      rightmost    i                                         S1(ab|cdef) = [MI(ab, cdef)]
 S  C1(Si      )  C           (S    )    
 
    i       +1       leftmost    i+1                                    S2(ab|c|def) = [MI(ab, c), MI(c, def)]
 S  i+1   *  
     
                                                                    NGMI
                                                                                    min (S1)  NGMImin(S2)
  *  @
                                                                       NGMI
                                                                           min  (S1) = min(MI(b, c), MI(ab, c),
  MIsum(Si, Si +1) = MI(Cn(Si), C1(Si +1 )C2(Si  +1))                               MI(b, cd), MI(ab, cd))

                   + MI(Cn(Si), C1(Si +1 ))

               + MI(Cn                                                     min  (S2) = min(MI(b, c), MI(ab, c),
                        -1(S )Cn(Si), C1(Si    ))                      NGMI
                            i              +1
                                                                                    MI(b, cd), MI(ab, cd))
               + MI(Cn  -1 (S )Cn(Si), C1(Si             ))
                             i               +1 )C2(Si+1     }
                                                                                     + min(MI(c, d), MI(bc, d),
                                                                                    MI(c, de), MI(bc, de))
  MI    (S , S                                          )),
     min  i   i+1) = min(MI(Cn(Si), C1(Si   +1 )C2(Si+1

                   MI(Cn(Si), C1(Si  +1)),

               MI(Cn                                             ; 
                     -1 (S )Cn(Si), C1(Si    )),
                          i               +1

                MI(Cn  -1(S )Cn(Si), C1(Si             )))
                           i               +1 )C2(Si+1      ^
                                                                %
  ; 

  MImax(Si, Si +1) = max(MI(Cn(Si), C1(Si    +1 )C2(Si+1)),     "      
   @

                   MI(Cn(Si), C1(Si                                  
 g
                                     +1)),
                                                                     §
               MI(Cn -1 (S )Cn(Si), C1(Si    )),
                          i               +1                    
   ~^  *  
                MI(Cn  -1(S )Cn(Si), C1(Si             )))
                           i               +1 )C2(Si+1           <  _  
  @ 
   
  
                                                                  
     
  MImean(Si, Si                                                  @ "   *  
*
                +1 ) = (MI(Cn(Si), C1(Si +1)C2(Si +1 ))
                                                                  
                   + MI(Cn(Si), C1(Si +1 ))

               + MI(Cn  -1(S )Cn(Si), C1(Si    ))
                            i              +1

           + MI(Cn                                              %" <> "##? ; 
                     -1(S )Cn(Si), C1(Si            )))/k
                         i              +1 )C2(Si +1        

   C                                                                  
 
         n-1  (S )        
                i
   
 *    
                                                  @ 
      S  C2(Si                         )                     @ * _
  *
                                i      +1
        
 *                                          @*     @*  
  
        S                                           @*  "    
                                                           i+1
     8
     <2                                                         
        * 
        ,  length(S or S   ) <= 2 and at begining or end of S      ¨     
  
  k =:4            i    i+1

        ,  length(S and S    ) > 2                                 
  
                   i     i+1




                                                              85

           _                          `  
 ¡    £¢¥ ¢¦~^¥ ¢}~£¥ ¢£¦^¥ ¢\¥

      £¢¥ ~^¥   ^\¥   \^¥   ¦£¦¥      `%
 / 

"     
  
   " 
      
 
   
                                      _ *   
 *
                                            *   \ 
                                           ~ @     
%' *+ @  Q ;                             
                                         "    @    
>
                                              
 @  
Y   
  
 
                                      
    " 
     @
[   
   © 
                                      
    *      
                                          " 
   
  "     ^¥  
   
               
  
      
 ª£^¥ 
 
        
 
                                      `%" / 
        
 
* ¢\¥ "     
*
                                       `    
      

  
    *    
                                      
@ <  <  <  <

  

                                         

  
  
  " 
                                      
    
  
  
                                               < 
? \]                               * 
   <    
  
                                           < 
  
 
 
                                      " 
 *  *    *
?%
   Q^ ;!
                                        \       ~    
                                      
  * `   @  
  
"  
      
                                                  
  
     «¡  
                                      
 @   _`¨  
  
"  ~¦¦¦ 
 ¢¦^}}  
                                       
   < 
 
 \¦\      < * 
                                        < *  
 _     
         
                                               _    
       *  
                                         "        @   
     
   @*  *  
                                       _        
 _ 
     
  ¦ 
  
                                               @  
¦  *      *  
                                       _   _       {  
  
 

*
                                           @ _   
   
                                          `        
                                         
  "      
?%"  @
                                      


          
    
     `%' *+/ 
     <
   
                                      @                              s   =
*    ^                     
                                                                     
 
 * @ ^^^^^ 
                     c1c2c3 · · · cici+1· · · cn 
                                            
 

                                         "  "    
 "    
                                           c1c2c3 ···cx  @ 
  
  
?%'  _
                                           

                                               
  Slist   "
"  
 
  
 
                                           "    Slist 2x -1 < 
 
 
    "  
                      ^ 
  

     "                                  
@ 
    *  *




                                    86

 
                 `

"¡_                  "¡_   
 *
  
 @ ¦ 
  _
*  

                           @   
*  @ 
 ~

                      
 
 
     

                      
@ 
 
 
  * `       

¬_                   NGMIpair
     

¬                    NGMIsum
     

¬                    NGMImin
     

¬_«                  NGMImax
     

¬¨_                  NGMImean
     

¬¬                   NGMImin
 
   
      " 
  

                     * 
    
              

                      
     ¦   
 


¬¬¬_                
  ¬¬     _
   

¬¬¬                 
  ¬¬     * @*     

¬¬¬                 
  ¬¬      @*  

¬¬¬                 
  ¬¬        


                                        "  "    
 


      *        8                                  
  ~  

                      >c
                      >
                      <c  1  |c2c3 ···cx                       S
                                                                best= W1[w22w23 . . . w2z] ··· Wy
                             |c2|c3 ···cx
                S    =    1
                      >
                 list >c
                      :   1  |c2c3|···cx
                         . . .
                                                       ¢ _   
  Sbest                      
                                                             Wy    
 
 <  *    
 *
                                                         "  
      

  *    
   
                                                               "   * 
    
                                                  
 Sbest
* @     
                                                          

\  Slist    
   
                                                       £   
     

   
       
 *  
                                                          
  ¢    
 
  @                                                  


~    
 @                                    "  @   
  
 *
  
                                               " 
 
  
* ¦^^^ 
                                                       
           S
            best= c1c2c3 ··· cx = W1W2 ··· Wy


¦ "   *         
                                j \&  
    ¬¬  ¬¬¬_
   < * W c1c2 ···ck    
     
  
   
                      i
   Sbest    
       
          
        @  
    " 
      
     ~                                                
 
 

  
                                                      *    @ _  
  
                  W = w w
                     i    i1   i2···wiz
                                                      
       
 * @ 
  "  
    *   
                               
         
 w                             i1
    
 w   Wi    
                   iz                                 j%
 _ {   ; 
     w     i1  w        
                    iz
    < 
  W2    "  @    
  
  w21       
 Sbest                          *          




                                                    87

                                                                                                                                                                                                                                                                                                                                                                                           
@  "  "        
   @  " \
                                                                                                                                                                                                                                                                                                                                                                                           ¯®
   "     
 
   *

 *  * 
                                                                                                                                                                                                                                                                                                                                                                                
             @  @
                                                                                                                                                                                                    £                           ^                                ^                              ^
                                                      ¦£                                                                                                          }¦
    @ *  ~^¥                                                                                             ~^                                                                                                                                                                                                              }}

@ " 
  *    

                                                                                                                                                                                                                                                                                                                                                                                             
         " 
 

  *   
  
  *
  ¬¬   
                                          \}\^¥                                                    ¢¢¦¥                                               }}¥                              ¥                           ^^^¥                                                                                            ¦¦\¥


 
  *  "                                                                                                                                                                                                                                                                                                                                                                                 *
                                    ¬¬                                                                                                                                                                                                                                                                                                                                                          
 ¬¬  
     @
    ¦¦\¥ 
 ~~¥   
                                                                                                                                                                                                                                          ^                              ^
                                                             \                                                       £                                                                                }
                                                                                                                                                                                                                                                                                                                                     £\
                                                                                                                                                                                                                                                                                                                                                                                                ®
  *  
   
 
  
         

                                  ¬¨_
                                                              ~^^^¥                                                   ¦\}¥                                             }¦¥                             ¦¥                            ¦^¦¥                          ^^^¥                           ^^^¥                                 \}}¥                                                      
   `    *  
  
¬¬                                                                                                                                                                                                                                                             ^                              ^
                                                                   \                                                      ¢}                                                                             }
                                                                                                                                                                                                                                                                                                                                           ¢¢¦
 "    
 
  ¦}¢¥                                                                                                                                                                                                                                                                                                                                                                          {@
¦£¦¥ @* 
    { 
                                         ¬_«
 @ * @ ¦^¥   "    "                                         \£¦~¥                                                   ¦£¥                                           £~^¥                            ^¥                             ¦^¦¥                           ^^^¥                           ^^^¥                                   \£^^¥                                                 
¡_~¦~£¥     @*  
                                                                                                                                                                                                                                                                                                                                                                                                       
_
                                                                                                                                                                                                                                                                     ^                              
                                                                         ¦                                                     ^¢                                             ^\                            ¢
                                                                                                                                                                                                                                                                                                                                                   £^
 *  "¡_        
*           
                                            ¬                                                                                                                                                                                                                                                                                                                                                            
    "¡_   
 *                                                    \£¢}¥                                                  ¦}¢\¥                                          }~¥                          \¥                              \¢¦¥                            ^^^¥                          ~^^^¥                                     ~~¥
                                                                                                                                                                                                                                                                                                                                                                                                         

      
   
                                                                                                                                                                                                                                                                                                                                                                                                          
 
          
                                                                                                                                                                                                                                                          ^                              ^
                                                                               ~^                                                     ¢}                                           \                           ^
                                                                                                                                                                                                                                                                                                                                                        ¢£~

 
    "¡_   

                                                                                                                                                                                                                                                                                                                                                                                                          
   {@  @ 
 
*  

                                             ¬
@    
   *   
                                                                \£}~¥                                                  ¦~¥                                         £¦~¥                        ~¥                               ¦~¥                              ^^^¥                           ^^^¥                                      \£¢¥

  @ @*  
 
   
                                                                                                                                                                                                                                                                                      ^                              ^
"  * "¡_                                                                         ¦~                                                   }                                                                      }
                                                                                                                                                                                                                                                                                                                                                               ££                                         
                                              _
           
   
      _
                                               ¬

                                                                            \}}¥                                                ¢¦\¥                                        £\}¥                       ¦}¥                              }^}¥                               ^^^¥                           ^^^¥                                       ~££¥
                                                                                                                                                                                                                                                                                                                                                                                                           *


                                                                                                                                                                                                                                                                                                                                                                                                             
                                                                                                                                                                                            ^                         ^                                 ^                                  ^                              ^
                                                                                            \^¦                                                 \}                                                                                                                                                                                                                   £~

j%" _ {  <>  ;                             
                                                                                                                                                                                                                                                                                                                                                                                                             

                                                                                               \£¢¥                                               ¢^¦¥                                                                                                                                                                                                                 ¦^\¥
        
    
     *    * 
                                                                                                                                                                                             ^                         ^                                 ^                                  ^                              ^
                                                                                                   \^
  
      *                                                                                                                                     £                                                                                                                                                                                                                    ¢£

       "                             

  
*      "                                                                                                                                                                                                                                                                                                                                                                                           
                                                                                                     \^~¥                                              ¢}^¥                                                                                                                                                                                                                  ¦~£¥
      @  " ~                                                                                                                                                                                                                                                                                                                                                                                              
   " ~           
                                                                                                         \\                                                                                   }¢                        £                                                                    ^                              ^
 ¢^¥          *                                                                                                                                     ^\                                                                                                                                                                                                                    ¦^\

   
 
             {   
                                                                                                                                                                                                                                                                                                                                                                                                               
                                   "¡_                                                                                                                                                                                                                                                                                                                                                                
                                                                                                           ¦~¥                                               £¢^¥                               £¦¥                      ¢~¦£¥                                                                                                                                                      ¦^¥
 
      @                                                                                                                                                                                                                                         ^^^^¥                                                                                                                                                "
   " \   ~                                                                                                                                                                                                                                                                                                                                                                                         
    
  NGMI         
                                                                                                                                                                                                  \                                ~                              ¦                              ¢                                                                                     
                     min
                                                   
                                                                                                                                                                                                                                                                                                                                  {@                      
  
                                                                                                                                                                                                                                                                                                                                                                                                     "   




                              88

   "¡_                          ¬_        ¬         ¬        ¬_«        ¬¨_ ¬¬

   ~¦~£¥     ¦^}~¥     ¦\¢¥       ¦\£¥      ¦}¦¥     ¦~¦¥      ¦¦~¥      ¦}}¥       ¢^¦¥


                      " \    
     



  ¬¬¬_       ¬¬¬       ¬¬¬        ¬¬¬           \  <     « ` < 
    ¦£}¦¥     ¢£~¥       ¢¦¥        ¦}£¦¥          > \ \<  ** 
                                                   ¯  ^^~  ¦}\¢^
" ~   
      
                                                ~   
 "*  _
*
                                                                * *
 "
¢^¥                                            ¡_ http://www.ict.ac.cn/jszy/jsxk_zlxk/
                                                  mfxk/200706/t20070628_2121143.html

|                                             ¦   
 "*  _
* 
                                                   "¡_  
 "*
    @   
 @ 
                                ¡ _* *
 http://ictclas.
  *       
                              org
   
 
     
                                                ¢ ¡  <
     _   

 
 *  @
  
 
                                                  
 * \< _* > \  
  
 
  
  
                                                     \ }}^ ^\
* 
  *   
  *
    

                                                £ ¡     _   
 
  " 
  *  
  
 
                         *    
 \< _*
 
      
 
 
                         > \      }}\ ~\
 
    
     
    
 *
 "¡_                         }   `*   "    
     
 
 
                                
            
  @    
 
 
                                   _* >   
 @ NGMI         NGMI    NGMI     NGMI         *>*  \< *  
               pair     sum      min      max
 NGMI          
 
                             ± _ }}£ _  
 ¡
        mean
{ 
              NGMI     
 
                    ¦~¢
                       min

  @ "  
  * 
                                                ^  < < <  
 _  

   @     
 
                                                               

    
 
   
                                                   ~¦
  
 _    
 @ 
                                  _              
 
        *                                
  ` http://www.sighan.org/
    
                                    bakeoff2005/
     *
 * @ 
 
  
                                            _  
  
                                                         \< _*
                                                  > \     \ }}^ ¦~
_
                                                 "  ±    °     _
                                                  
 
    

                        
 
                                                   \<  `  ^^^ ¢~}
      

*             http://www.sighan.org/
   bakeoff2005/data/results.php.htm
                                                \ "   ¡ ±    
                                                    
   \< _* > \
  ` ° ¡ " ¨      _  
                                                       }}\ ~\
    
    
 
    
 ££}

  `* ¡    "  «¡ 
    " 




                                              89

   An Automatic Question Generation Tool for Supporting Sourcing and
                                   Integration in Students' Essays

                            Ming Liu                                     Rafael A. Calvo

           School of Elec. & Inf. Engineering                School of Elec. & Inf. Engineering
                    University of Sydney                             University of Sydney
                              NSW                                              NSW
                  liuming@ee.usyd.edu.au                             rafa@ee.usyd.edu.au


Abstract     This paper presents a domain independent     medical or security domain, where a system suggest
Automatic Question Generation (AQG) tool that gener-      questions to a practitioner based on a the case file. The
ates questions which can be used as a form of support     second type of AQG systems is useful in a growing
for students to revise their essay. The focus here is on  number of tutoring systems that have natural dialogue
generating questions based on semantic and syntactic      capabilities (e.g. Autotutor discussed later).
information acquired from citations. The semantic in-         In this study we are concerned with building
formation includes the author's name, the citation type   an AQG component for a third type of pedagogical
(describing the aim of the cited study, its results or an applications:   supporting students in their academic
opinion), the author's expressed sentiment, and the syn-  writing. In this context the common way of addressing
tactic information of the citation. Pedagogically, the    the AQG problem is substantially changed:
question templates are designed using Bloom's learn-
ing taxonomy where the questions reach the Analysis           The driver for the technology is pedagogical so
Level. We used 40 undergraduate students essays for             the questions should be framed in a pedagogical
our experiment and the Name Entity Recognition com-             theoretical framework.
ponent is trained on 20 essays. The result of our ex-
                                                              The domain may be very general and a corpora for
periment shows that the question coverage is 96% and
                                                                background knowledge might not be available.
accuracy of generated questions can reach 78%. This
AQG tool will be integrated into our peer review system       The questions must be generated from a single
to scaffold feedback from peers.                                document, instead of a whole corpora

Keywords      Question       Generation,       Electronic     The target audience of the questions is the same
Feedback System for Sourcing and Integration in
                                                                author of the document. The author should know
Students'Essay
                                                                the answers, so the goal here is to trigger reflection
                                                                or get the student to expand on a topic.
1    Introduction
                                                              Most different genres of academic writing contain
Progress made in question answering systems has mo-
                                                          citations of third party work on which the student is
tivated a recent growth in automatic question genera-
                                                          expected to comment (as in a literature review) or which
tion systems. Two types of question generation tasks
                                                          is being used as evidence in an argument. When writing
are normally considered. The first is text-to-question,
                                                          an essay or literature review, students are expected to
where a document is provided to an AQG system that
                                                          learn and reason from multiple documents which re-
generates a question for which the answer is contained
                                                          quire the skill of sourcing (i.e., citing sources as evi-
in the text. The second type is as a component of an
                                                          dence to support their arguments) and Information In-
Intelligent Tutoring System where a dialogue between
                                                          tegration (i.e., presenting the evidences in a cohesive
the student and the ITS, and a set of propositions, is
                                                          and persuasive way).
used as the input to the AQG component. In this case
                                                              The development of student's sourcing and integra-
the question is aimed at helping the student elicit an
                                                          tion skills can be supported by using trigger questions
answer containing the propositions.
                                                          such as Does the essay provide evidence for the claims
    The former AQG systems can support reading
                                                          it makes? or Does the conclusion follow from the argu-
comprehension      tasks,    automatically     suggesting
                                                          ment? But such questions are too general and not likely
questions that tutors can use in their teaching. Similar
                                                          to provide strong support in the process of writing on a
systems can be used to generate questions in the
                                                          specific topic. More specific questions need to be asked.
Proceedings of the 14th Australasian Document Comput-         Most of the current AQG systems rely on shallow
ing Symposium, Sydney, Australia, 4 December 2009.        semantic parsing with entity recognizers. For example,
Copyright for this article remains with the authors.      Name Entity Recognizer,Verbnet [14] and Framenet [1]


                                                          90

can only `understand' the semantic role of the entities    (LSA) to calculate the average distance between con-
such as agent, time, location and object in a sentence     secutive sentences and provide feedback on the overall
and generate factual questions. To generate deep ques-     coherence of the text. LSA is a technique used to mea-
tions related to a student's essay, AQG systems depend     sure the semantic similarity between texts and has been
on some type of domain knowledge. AutoTutor [8] can        described thoroughly elsewhere [11]. SaK can also an-
generate deep questions, using domain specific knowl-      alyze the purpose of a sentence, identifying clusters of
edge in Computer Literacy or Physics.                      topics amongst the students so when the topic of a new
    This paper describes a new AQG system that in-         composition is not identified the student can be asked
cludes a name entity recognizer for citation extraction, a for an explanation or reformulation.
pattern-matching based classifier for citation type clas-       Sourcer's           Apprentice      Intelligent           Feedback
sification and a sentiment analysis component for de-      (SAIF) [3] is an automated feedback tool for writing
tecting the author's opinion polarity. These pieces of     essays which can be used to detect plagiarism,
information are used to generate template-based ques-      uncited quotation, lack of citations and limited content
tions during student's academic writing activities and     integration problems.            Once a problem is detected,
targeting specific levels of Bloom's learning objectives   SAIF can give helpful feedback to the student as shown
taxonomy.    Section 2 provides a brief review of the      in Table 1.
extensive literature focusing on approaches and systems        Problem                        Feedback prompts student to:
that support learning experiences with sourcing and in-        1a. Unsourced copied material  Reword     plagiarism    and   model
                                                               (plagiarism)                   proper format.
tegration as learning goals. Section 3 describes the sys-      1b. Unsourced copied material  Explicitly credit source and model
tem's architecture while Section 4 its evaluation, in-         (quotation)                    proper format.
                                                               2. Explicit citations          Explicitly make a minimum of 3
cluding coverage and correctness. Section 5 concludes.                                        citations.
                                                               3. Distinct sources mentioned  Cite at least 2 different sources.
                                                               4. Excessive quoting           Paraphrase more instead of relying
2    Related Work                                                                             on quotations too heavily.
                                                               5.  Integration from multiple  Include a more complete coverage
Natural Language Processing techniques have been               sources                        of the documents in set.

used to develop a number of tutoring and feedback
systems.    Section 2.1 reviews some of the projects
                                                           Table 1: Types of Problems SAIF addresses and the
developing writing support tools, and Section 2.2
                                                           intended goal of feedback
systems that generate questions automatically.
                                                                SAIF also uses Latent Semantic Analysis (LSA)
2.1     Electronic      Feedback        System       for
                                                           techniques for plagiarism detection, computing the
        Sourcing and Integration
                                                           similarity between each essay sentence and the source
Numerous       projects   have     used   computational    sentences in LSA semantic space.                     For finding the
approaches to assessing and providing automatic            explicit citations, SAIF uses a Regular Expression
feedback on writing, most of the focus being on the        Pattern Matching technique to detect the explicit
assessment [15].     Despite a variety of initiatives to   citations      by      recognizing    phrases        containing       the
improve the quality of automatic feedback the efficacy     author's name (e.g. According to, As stated in, State).
of the systems remains to be proven and more research      Evaluations showed             [3] that SAIF provides helpful
is needed. Meanwhile providing timely and appropriate      feedback for students to use more explicit citations in
feedback at key stages of the writing process remains      their essays. However, this tool only addressed some
a manual task, and a serious challenge for university      basic problems for sourcing and integration. Moreover,
lecturers.                                                 it required a large number of source documents to build
    Some of the early systems include Writers Work-        the LSA semantic space and a large number of pattern
shop a system developed by Bell Laboratories, and Ed-      matching rules had to be predefined.
itor [16] both focused on grammar and style. Studies on         Glosser is an automated feedback system for
the impact of Editor [2] concluded that the pedagogical    student's writing [17].           It uses textual data mining
benefits of grammar and style checking are limited. It     and computational linguistics algorithms to quantify
could also be argued that these systems only aimed at      features of the text, and produce feedback for the
supporting writing to communicate and did not address      student. This feedback is in the form of generic trigger
the issue of supporting writing to learn, important in     questions (adapted to each course) and document
today's curriculum design.                                 features that relate to each set of questions.                        For
    SaK, a writing tutoring system developed at the Uni-   example, by analyzing the words contained in each
versity of Memphis [18] is based on the notion of voices   paragraph, it can measure how close two adjoining
that speak to the writer during the process of composi-    paragraphs are. If the paragraphs are too far this can
tion. SaK uses avatars to give the impression of giving    be a sign of what is called lexical cohesiveness and
each voice a face and a personality [18]. Each avatar      Glosser flags a small warning sign.                      Glosser (1.0)
provides feedback on a different aspect of the composi-    provides feedback on four aspects of the writing:
tion, saying what is good or bad about the text but with-  structure, coherence, topics, and concept visualization.
out correcting it. SaK uses Latent Semantic Analysis


                                                           91

    Glosser does not address sourcing directly, but four   physics through an animated agent asking a series of
trigger questions (and the text features above) are pro-   deep reasoning questions that follow Graesser-Person
vided:                                                     taxonomy [7]. In each of these themes a set of top-
                                                           ics have been identified. Each topic contains a focal
  1. Are the ideas used in the essay relevant to the ques-
                                                           question, a set of good answer aspects, a set of hints,
     tion?
                                                           prompts or elaborations which used to elicit each good
                                                           answer aspect, a set of anticipated bad answers and so
  2. Are the ideas developed correctly?
                                                           on. The system initiates a session by asking a focal
  3. Does this essay simply present the academic refer-    question about a topic and the student are expected to
     ences as facts, or does it analyse their importance   write an answer containing 5-10 sentences. The system
     and critically discuss their usefulness?              can generate hints or prompts for the student to elicit the
                                                           correct and complete answer. The authors showed that
  4. Does this essay simply present ideas or facts, or
                                                           AutoTutor's questioning approach had a positive im-
     does it analyse their importance?
                                                           pact on learning with an effect size on a pretest post-test
                                                           study of approximately 0.8 standard deviation units in
    The AQG algorithms described here are designed
                                                           the areas of computer literacy and Newtonian physics.
to be integrated into Glosser and provide support for
                                                           However, the system is domain dependent and requires
sourcing an integration of citation sentences. The stu-
                                                           a large number of human resources to predefine the con-
dents upload a composition and Glosser provides the
                                                           tent of each topic.
different forms of feedback. Other approaches for in-
cluding the automatically generated questions include
embedding them within an email, or using them as part      3    System Design and Architecture
of a peer-review process.
                                                           The AQG tool described here is designed to generate
                                                           questions from a student's essay and a set of templates
2.2    Question Generation
                                                           designed by the instructor. The system was evaluated
One of the first automatic question generation systems     using a corpus of student essays discussed in Section
proposed for supporting learning activities was AUTO-      4. Sentences from that corpus are used here as exam-
QUEST [19]. In this case, as in most of the current        ples on how the questions are generated. The corpus
research questions are generated from external sources     contains essays on the topic "English as a Global lan-
that the student reads (as opposed to writes).             guage".
    The approach used here is similar to that of               In this section we provide an overview of the sys-
Kunichika et. al. [10] who proposed an AQG approach        tem's architecture shown in Figure 1 and describe each
based on both the syntactic and semantic information       step in a pipeline process. The input to the system is an
extracted from the original text based on DCG (Definite    essay and the output is the generated questions.
Clause Grammar). Their educational context was the             Table 2 shows an example of questions generated by
assessment of grammar and reading comprehension            the AQG tool and their mapping to cognitive levels in
around a story.      The extracted syntactic features      Bloom's Taxonomy. In this example, the questions are
include subject, predicate verb, object, voice, tense      generated from the raw sentence written by a student as
and sub clause.    The semantic information contains       part of an essay.
three semantic categories: noun, verb and preposition,         The question generation process follows 3 steps
used to determine the interrogative pronoun for            shown in Figure 1:
the generated question.     For example, in the noun           Step 1. Pre-processing. This includes citation ex-
category, several noun entities can be recognized          traction, filtering `noisy' segments, splitting complex
including the Person, Time, Location, Organization,        sentences and sentence transformation if it uses a noun
Country, City, Furniture.     In the verb category, the    or passive voice to refer to resources. There are two
bodily actions, emotional verbs, thought verbs and         major components to perform these tasks: 1 Sentence
transfer verbs can be identified.     It also builds the   Extractor, performs citation sentence extraction using
semantic links among the time, location and other          the combination of trained Stanford Name Entity Rec-
semantic categories when an event occurs.        Because   ognizer [5], and a Pronoun Resolver, which is imple-
this technique extracts substantial syntactic and time     mented by finding the nearest Name Entity appeared
/ space semantic information from sentences, the           before the pronoun, and 2 Filter performs the rest of
generated questions can be more sophisticated and          tasks which involved to clean up "noisy" segment, split
provide better support.     The empirical result shows     complex sentences, transform other types of citation
that 80% questions were considered by experts as           form to reporting verb type by using Tregex Pattern
appropriate for novices learning English and 93% of        Match Techniques[12].
the questions were semantically correct.                       Examples of students' compositions include:
    AutoTutor, developed by the Graesser et al [8] at
the University of Memphis, is an ITS that improves stu-      1. According to Crystal, more people in the world

dent's knowledge in computer literacy and Newtonian             speak Chinese than any other language.


                                                           92

 Level         Description                    Example                     tence 3 uses the noun `opinion' to refer to the reference
                                              1.Who      is     Graddol?
                                                                          and the system will convert it into a reporting verb type
               Ability to identify the spe-   2.What     does    Graddol
 Recognition
               cific content.                 point     to      in    his (explained later). The new reported verb type version
                                              study?(Sourcing)
                                                                          for Sentence 3 is:
               Ability to retrieve the spe-
 Recall                                       The same to Recognition
               cific content from memory.                                     Wallraff states that there is a rate of growth of other
                                              Why would Graddol point
               Ability to understand the                                  languages in the USA which is higher than the rate of
                                              to the social and economic
               learning material in terms
                                              inequality that the dom-    growth of English.
               of generation inferences,
 Comprehension                                inance of English could
               interpretation information,                                    To achieve these, the input sentence is parsed into a
                                              lead to?   (What evidence
               explanation    and  summa-
                                              does Graddol provide to     Phrase Structure tree, and then the Tregex Expressions
               rization information.
                                              prove that?) (Sourcing)
                                                                          are used to detect the syntactic patterns, and finally we
                                              How     did   you   present
               Ability to apply the knowl-
                                              Graddol      opinion     as use Tsurgeon to perform required opersions.
               edge from the learning ma-
 Application                                  evidence to confirm the
               terial to a problem or situa-                                  Tregex, developed by Stanford NLP group, is a
                                              thesis  in   your   essay?(
               tion.
                                              Integration)                powerful pattern matching technique which can match
                                              1. Is Crystal against Grad-
                                                                          an individual word, regular expression, a POS tag or
                                              dol's opinion?    2.  Since
               Ability to disassemble the     you say Crystal's opinion   group of POS tags such as a Noun Phrase. Once the
 Analysis      elements and find the rela-    is against Graddol,    can
                                                                          matched node is found by the Tregex, the Tsurgeon
               tionship between elements.     you find the contradic-
                                              tive evidence provided by   tool can perform delete, add, remove the node from the
                                              Crystal? (Integration)
                                                                          syntactic tree as shown in Figure 2 .
                                                                              According    to   a  study   by   Hyland   [9],   there
Table 2: An example of questions generated from the                       are   mainly   three   grammatical    ways    to refer   to
sentence "Graddol on the other hand points to the                         sources, which use Reporting Verb, Noun and Passive
social and economic inequality that the dominance of                      construction. Here, we call this as three grammatical
English could lead to".                                                   patterns for citation. In our implementation, the citation
                                                                          sentence which is either Noun or Passive construction
                                                                          patterns would be transformed into reporting verb
                                                                          pattern because it would be easier to transform
                                                                          the citation sentence with reporting verb pattern
                                                                          into questions in later stage.     Therefore, the Tregex
                                                                          Expression are defined to detect the three grammatical
                                                                          patterns and extract right Subject, Predicate Verb,
                                                                          Predicate, Auxiliary Verb for processing in later stage.
                                                                          The code segment in Figure 2 is used to split the
                                                                          complex sentence 2.


                                                                               find_adv=TregexPattern.compile
                                                                                         ("ADVP =rb >>,(NP >(S > ROOT)) | > S");
                                                                               find_clause=TregexPattern.compile
                                                                                         ("SBAR=sbar<(IN<Although|though)<S");
                                                                               find_comma=TregexPattern.compile
                                                                                         ("/,/=comma \$ (NP >( S > ROOT ))");
                                                                               Tsurgeon.parseOperation("delete rb");
                                                                               Tsurgeon.parseOperation("delete sbar");
                                                                               Tsurgeon.parseOperation("delete comma");




             Figure 1: System Architecture                                Figure 2: An example of code segment using Tregex-
                                                                          Pattern and Tsurgeon for splitting a complex sentence
  2. Although Crystal and Graddol use many statisti-
     cal evidence to discuss the spread of English as a
     Global language and the resulting consequences                           Step 2. Syntactic and Semantic features. The pur-

     of this,Wallraff actually challenges the notion                      pose of this step is to extract the Syntactic feature and

     that English has the global status most people                       Semantic feature, such as the citation type (Study Re-

     believe it to have.                                                  sult, Author's Opinion, Aim of Study) and the Author's
                                                                          Opinion Polarity. AQG then inserts the Semantic fea-
  3. Wallraff's opinion is that there is a rate of growth                 tures as facts into a prolog knowledge base to be used
     of other languages in the USA which is higher than                   in Step 3. There are two components to perform these
     the rate of growth of English.                                       tasks: a Sentence Feature Extractor which performs
                                                                          Syntactic Feature and Semantic feature extraction, and
In sentence 1 the noisy segment is shown in Bold. Sen-
                                                                          a Sentiment Classifier which detects the Author's Opin-
tence 2 is a complex sentence divided into two simple
                                                                          ion Polarity.
sentences shown in Bold and Italics respectively. Sen-


                                                                          93

   Sentence Feature Extractor uses Tregex Expression      defined three categories for a word sentiment with some
on the Syntactic Tree for pattern match to extract        magnitude: positive, negative and neutral.
syntactic features: Subject, Predicate Verb, Link Verb,
                                                            Sentence Opinion  Topic   Polarity   Sentiment words list
Modal Verb and Predicate which are essential elements                Holder
                                                            S1       Crystal  English Negative   (negative=-1.0), gain=0.5, in-
for question generation.     In addition to Syntactic
                                                                                                 crease=0.5
Features Extraction, Sentence Feature Extractor also        S2       Graddol  English Negative   Inequality=-1.0

uses predefined Reporting verb to define the Citation
Type by matching the predicate verb in a sentence. In
                                                          Table 3: an example of Author's Sentiment Classifica-
our database, Reporting Verb have been classified into
                                                          tion
three categories which correspond to different citation
types.                                                        Table 3 shows the result of Sentiment Classifica-
   Sentiment Classifier is used to detect the Author's    tion from the two citation sentences in the above ex-

opinion polarity about a topic.     For the sentiment     ample. Crystal is the Opinion Holder for Sentence 1,

analysis AQG defines three elements: Opinion Holder,      the English is chose as the Topic and the Opinion Po-

Topic and Opinion Polarity.     At the moment, AQG        larity is Negative because AQG calculates the sum of

only handles one Author appearing in a sentence and       the two nearest sentiment words:       Negative=-1.0 and

the opinion holder is the Author mentioned in the         increase=0.5 which is negative. It is similar to sentence

citation sentence. The topic is detected by choosing      2. Once finishing the sentiment analysis AQG will in-

the most frequent noun or noun phrases among              sert the extracted facts including Opinion Holder,Topic

citation sentences expressed as a Sentence-Term matrix    and Opinion Polarity into our prolog knowledge base

containing rows corresponding to the citation sentences   showed in Figure 3 which will be used to infer if the

and columns corresponding to the terms appeared in the    Author's opinion is against/support each other.

sentence. Because AQG doesn't consider the number
of times a word appears, a Binary Weighting schema
                                                             #Facts
is used. The topic is chosen by finding the term with        author(crystal).
maximum value and the Equation 1 is defined below,           author(graddol).

where  = 1 if the term j appears in the citation             against(graddol,english).
                                                             against(crystal,english).
sentence i, n is the number of citation sentences in an      opinion(english).
                                                             #Inference rules
essay and the m is the number of terms appearing in          support(person1,noun1).
these sentences.                                             against(person2,noun2).
                                                             ally(X,Y):-support(X,Z),support(Y,Z),opinion(Z),X\=Y.

                                                             ally(X,Y):-against(X,Z),against(Y,Z),opinion(Z),X\=Y.
                                                             enemy(X,Y):-support(X,Z),against(Y,Z),opinion(Z),X\=Y.
                  max       {    }                   (1)     enemy(X,Y):-against(X,Z),support(Y,Z),opinion(Z),X\=Y.

                              

For example, two citation sentences are extracted from
an essay.
                                                          Figure 3: An example of Author's Opinion Polarity in
                                                          Prolog knowledge base
  1. "The increasing use of English is also negative
     in respect to the advantage gained by its native-        Step 3. Generation This is the final step to generate
     speakers, not to mention the "threat to the identity template-based questions where the Question Genera-
     of nations" through the inevitable increase of use   tor uses the extracted syntactic features and the knowl-
     of minority languages (Crystal, 1992)."              edge base, and then matches the predefined patterns
                                                          in our Rule Repository, and finally generates template-
  2. "Graddol on the other hand points to the social
                                                          based questions.     In our current implementation, we
     and economic inequality that the dominance of En-
                                                          have defined 5 rules and each rule defines the pattern
     glish could lead to."
                                                          for matching and 5 question templates. Each citation
                                                          sentence would be applied by only one of the five rules.
   The word `English' has been chosen as Topic be-
                                                          If a citation sentence matches both reporting verb and
cause it has the largest value 2 according to Equation
                                                          sentiment words, we would consider the rule for re-
1. After the Opinion Holder and a Topic are detected,
                                                          porting verb because sentiment words have higher error
AQG detects the Opinion Polarity about the topic. The
                                                          rate to determine the citation type. In the future, we
Opinion Polarity is decided by the Sentiment Region
                                                          will use Machine Learning techniques to train a cita-
containing sentiment words in a sentence. The size of
                                                          tion type classifier which will use the weight of se-
Sentiment Region is very important and AQG defines it
                                                          lected features (reporting verb, sentiment words, num-
as the set of nearest sentiment words around the topic
                                                          bers and etc) rather than current fixed pattern matching
in a sentence, and use the SENTIWORDNET [13] to
                                                          technique. Table 4 shows that the five rules are defined
determine the sentiment of a word. The SENTIWORD-
                                                          in our Rule Repository.
NET, a publicly available lexical resource for opinion
                                                              The Pattern Matching is based on the Reporting
mining, is an extension of WORDNET2 [4] and has
                                                          Verb and Word Sentiment in the citation sentence. In


                                                          94

  Rules   Pattern    Citation    The Purpose of Generated Question        Pattern     The predicate verb matches reporting verb for express-
                     Type                                                             ing Authors opinion purpose.
  Rule 1  Reporting  Opinion     Ask the student to provide evidence      Template
          Verb                   which support the Opinion (Sourc-
                                 ing), to provide other Author's con-                      Who is [Author Name]?
                                 tradictive opinion or result about
                                 the topic(Integration) if applicable                      What does [Author Name] [predicate verb
  Rule 2  Reporting  Aim     of  Ask the student to identify the mo-                        Lemma]?

          Verb       Study       tivation for this Author's study and
                                 the outcome of the study (Sourc-                          In the [Author Name]s study, do you agree that
                                 ing).                                                      [Author Name] [Predicate]? Have you evaluated

  Rule 3  Reporting  Result      Ask the student to identify if the                         [Author Name]s opinion?

          Verb                   Author's Result is objective and
                                 what opinion does the result sup-                         Why would [Author Name] [Predicate]? (What
                                 port (Sourcing)                                            evidence does [Author Name] provide to prove

  Rule 4  Sentiment  Opinion     The same to Rule 1                                         that?)

          Word
  Rule 5  Sentiment  Result      The same to Rule 3                                        How did you present [Author]'opinion as evi-
          Word                                                                              dence to confirm the thesis in your essay?

                                                                                           Is [other Author Name] against [Author Name]'s
                                                                                            opinion? Since you say [Other Author Name]s
Table 4: The Rule Definition for Patterns and Templates                                     opinion is against [Author Name], can you find
                                                                                            the contradictive evidence provided by [Other
                                                                                            Author Name]?

our database, the reporting verb has been classified
under one of three citation types and matches the
predicate verb extracted from Step 2. If they are not                   Table 5: A Example of Question Template in Rule 1
matched, the sentiment words is used to detect the
                                                                      4     Evaluation
citation type.    In our Rule repository, the question
templates are designed according to the citation type.                 This section describes a preliminary evaluation of the
For example,                                                           technique focused on two aspects : 1) The Question
   Graddol on the other hand points to the social and                  Coverage. 2) The Semantic Correctness of generated
economic inequality that the dominance of English                      questions. In the last section we comment on planned
could lead to.                                                         evaluations that will study the learning impact of such
   The predicate verb is point to and it matches a                     a system, and self (the writer's view) and 3rd person re-
reporting verb under Opinion Type in our repository,                   ports on the quality features of the questions generated.
then we apply Rule 1 shown in Table 4 to generate the                     The evaluation was performed using 40 essays
template-based questions. Table 5 gives an example of                  written by students at the University of Sydney.
question templates defined in Rule 1 and Table 2 shows                 Students gave informed consent as approved by the
an example of generated template questions defined in                  Human Ethics Committee of the University of Sydney.
Rule 1. As you noticed, the following questions are
generated by using prolog inference engine described                  4.1      Question Coverage
in Step 2.
                                                                       The citation sentence extraction approach is based
   1. Is Crystal against Graddols opinion? 2. Since
                                                                       on the Author Name Recognition.                      The Expected
you say Crystals opinion is against Graddol, can you
                                                                       Number of Questions depends on the total number
find the contradictive evidence provided by Crystal?
                                                                       of citation sentences.        Table 6 shows that AQG can
(Integration)
                                                                       reach 96% coverage.              This dataset contains 127
   If the sentence does not contain any reporting verb
                                                                       citation   sentences(127*2=254             questions)        and   123
but some sentiment words, then it is also considered as
                                                                       citation sentences (123*2=246 questions) are extracted
the Author's Opinion. For example,
                                                                       by AQG. We only evaluate 2 generated questions
   The increasing use of English is also negative in
                                                                       per citation sentence because some template-based
respect to the advantage gained by its native-speakers,
                                                                       questions only require Author Name, a relatively easy
not to mention the "threat to the identity of nations"
                                                                       task, the evaluation does not include these questions.In
through the inevitable increase of use of minority lan-
                                                                       other words, two questions are evaluated per citation
guages (Crystal, 1992).
                                                                       sentence. For example, in Rule 1 question 3 and 4 are
   As the word Negative has been detected as a senti-
                                                                       evaluated which is shown in Table 5. The problem for
ment word, the sentence is consider as expressing Au-
                                                                       missing these citation sentence extraction is that some
thor's Opinion, and then AQG applies Rule 4 to gen-
                                                                       Author Names are not identified by the Name Entity
erate questions. Rule 5 is similar to Rule 4 for pattern
                                                                       Recognizer which cause these citation sentences can
matching except the sentence does not contain the sen-
                                                                       not be detected by AQG.
timent words and the citation are expressed as Study
                                                                          Expected   Number    Number of Gener-         The Question Cov-
Result.
                                                                          of Questions         ated Questions           erage
                                                                          254                  246                      96%



                                                                                      Table 6: Question Coverage


                                                                      95

4.2     The Correctness of Generated Ques-                 major forms of citation and as shown to have excellent
        tions                                              accuracy.
                                                               Reasoning techniques were implemented in Prolog
123 citation sentences were extracted from the 40 es-
                                                           to detect when two authors are against each other and
says. Of these, 5 citation sentences had serious gram-
                                                           the generated question can reach to Analysis Level in
matical errors which caused the sentence Parser to fail.
                                                           Bloom's Taxonomy.
Therefore only the 118 remaining sentences were con-
                                                               The tool can not only detect how many citations the
sidered for evaluation. Because we only evaluate two
                                                           writer has used in their essay but also generate spe-
questions per rule, the total number of evaluated ques-
                                                           cific or content related questions.      Compared to cur-
tions is 236.
                                                           rent question generation systems, our tool can generate
   Table 7 shows that the semantic correctness of ques-
                                                           pedagogically deep questions in a somewhat domain
tion reach to 78%. One of the main problems is that
                                                           independent form (it still requires templates that may
the rules used are too rough to handle multiple Authors
                                                           required adaptation). It also presents novel results for
appeared in a sentence. For example, the sentence
                                                           using the authors' sentiment to generate questions.
   "Wallraff suggests that the number of Spanish
                                                               Some limitations of this early work are obvious as
speakers in the USA has grown by 50% in the 1980-
                                                           the need to handle multiple authors in a sentence and to
1990 census, thus refuting Crystal and Graddol's
                                                           improve the classification of the citation type.
arguements for English being a global language."
                                                               In future work, we will integrate the AQG tool into
   Another major problem is the misclassification for
                                                           Glosser and to our peer review system so it provides
the citation type: Opinion and Result. For example, the
                                                           extra information to support students' engagement with
sentence
                                                           the writing (or peer-reviewing) process. for example,
   "Many Chinese-speakers (four out of five of about
                                                           in a peer-reviewing scenario, the peer could not only
2.4 millions) in America prefer to speak Chinese at
                                                           evaluate the essay but also the author's answers to
home rather than English (Wallraff, 1999)."
                                                           these automatically generated questions and provide
   In this case, although it contains prefer as a senti-
                                                           better feedback. We will also improve the technique by
ment word with a positive term, the citation sentence
                                                           adding ways for extracting multiple authors' arguments
should be considered as Study Result.
                                                           in a sentence and use other machine learning techniques
   Rules             Number of Gener-  Number of Seman-
                                                           to improve the Citation Type classification accuracy.
                     ated Questions    tic Correct Ques-
                                       tions
   Rule 1            82                72
   Rule 2            12                12                 Acknowledgements
   Rule 3            40                36
   Rule 4            64                34                  The authors would like to thank Jorge Villalon and
   Rule 5            38                30                  Setphen O'Rourke for the development of TML and
   Total             236               184
                                                           Glosser. Ming is partially supported by a N.I. Price
                                                           scholarship.   This project was partially supported by
          Table 7: Question Generation Result              an Australian Research Council Discovery Project
                                                           DP0986873.

5    Conclusion and Discussion
                                                          References
Sourcing and Integration are important quality features
                                                            [1] C. F. Baker, C. J. Fillmore and J. B. Lowe.          The
in writing, and are part of the skills that college stu-
                                                                berkeley framenet project. In Proceedings of the 17th
dents must learn to master. The importance of asking
                                                                international conference on Computational linguistics,
questions has been shown to be an important part of
                                                                pages 86­90, Morristown, NJ, USA, 1998. Association
teaching and learning experiences, so we designed an            for Computational Linguistics.
implemented a tool for automatically generating ques-
                                                            [2] T. J. Beals. Between Teachers and Computers: Does
tions from an essay.
                                                                Text-Checking Software Rea lly Improve Student Writ-
   This domain independent AQG tool supports stu-
                                                                ing? English Journal, pages 67­72, 1998.
dent's essay writing in the areas of sourcing and inte-
                                                            [3] M. A. Britt, P. Wiemer-Hastings, A. A. Larson and C. A.
gration. Although we have not yet been able to assess
                                                                Perfetti. Using intelligent feedback to improve sourcing
the impact on student learning, the system was evalu-
                                                                and integration in students' essays. Int. J. Artif. Intell.
ated using real student essays.                                 Ed., Volume 14, Number 3,4, pages 359­374, 2004.
   Both the question coverage and the semantic
                                                            [4] C. Fellbaum and G. Miller.      WordNet: An Electronic
correctness of generated questions were evaluated.
                                                                Lexical Database. The MIT Press, 1998.
Although the performance of Name Entity Recognizer
                                                            [5] J. R. Finkel, T. Grenager and M. Christopher. Incorpo-
would be different under different domain, the focus
                                                                rating non-local information into information extraction
of current work is on interesting question generation.
                                                                systems by gibbs sampling. In ACL '05: Proceedings
The pattern matching algorithm is based on Hyland's
                                                                of the 43rd Annual Meeting on Association for Com-
citation study that describes the most common ways              putational Linguistics, pages 363­370, Morristown, NJ,
of citing third party work. The algorithm captures the          USA, 2005. Association for Computational Linguistics.


                                                          96

 [6] X. Gong, Y. H.and Liu.      Generic text summarization
    using relevance measure and latent semantic analysis.
    In SIGIR '01:      Proceedings of the 24th annual in-
    ternational ACM SIGIR conference on Research and
    development in information retrieval, pages 19­25, New
    York, NY, USA, 2001. ACM.

 [7] A. C. Graesser and N. K. Person. Question asking dur-
    ing tutoring. American Educational Research Journal,
    Volume 31, pages 104­137, 1994.

 [8] A. C. Graesser, K. VanLehn, C. P. Ros´e, P. W. Jordan
     and D. Harter.   Intelligent tutoring systems with con-
     versational dialogue. AI Mag., Volume 22, Number 4,
     pages 39­51, 2001.

 [9] K. Hyland.     Academic attribution:   citation and the
     construction of disciplinary knowledge.    Applied Lin-
     guistics, Volume 20, pages 341­367, 1994.

[10] H.   Kunichika,   T.   Katayama,    T.  Hirashima   and
     A. Takeuchi. Automated question generation methods
     for intelligent english learning systems and its evalua-
     tion. pages 1117­1124. Proc. of ICCE01, 2001.

[11] T. K. Landauer, D. S. McNamara, S. Dennis and
     W. Kintsch.    Handbook of Latent Semantic Analysis.
     Lawrence Erlbaum, 2007.

[12] R. Levy and A. Galen.       Tregex and tsurgeon: tools
     for querying and manipulating tree data structures. In
     Proceedings of the Fifth International Conference on
     Language Resources and Evaluation., 2006.

[13] S. Osinski and D. Weiss. Conceptual clustering using
     lingo algorithm: Evaluation on open directory project
     data. In In IIPWM04, pages 369­377, 2004.

[14] K. K. Schuler.     VerbNet:    A broad-coverage, com-
     prehensive verb lexicon.    Ph.D. thesis, University of
     Pennsylyania, 2005.

[15] M. D. Shermis and J. Burstein.         Automated essay
     scoring: A cross-disciplinary perspective, Volume 16.
     The MIT Press, 2003.

[16] E. C. Thiesmeyer and J. E. Thiesmeyer. Editor:A Sys-
     tem for Checking Usage, Mechanics, Vocabulary, and
     Structure. New York: Modern Language Association,
     1990.

[17] J. Villalon, P. Kearney, R. A. Calvo and P. Reimann.
     Glosser: Enhanced feedback for student writing tasks.
     In Proc. Eighth IEEE International Conference on Ad-
     vanced Learning Technologies ICALT '08, pages 454­
     458, July 1­5, 2008.

[18] P. Wiemer-Hastings and A. C. Graesser.         Select-a-
     Kibitzer: A computer tool that gives meaningful feed-
     back on student compositions.      Interactive Learning
     Environments, Volume 8, Number 2, pages 149­169,
     2000.

[19] J. H. Wolfe. Automatic question generation from text
     - an aid to independent study.       SIGCUE Outlook,
     Volume 10, Number SI, pages 104­112, 1976.




                                                              97

      You Are What You Post: User-level Features in Threaded Discourse

                                           Marco Lui and Timothy Baldwin

                                               University of Melbourne
                                                   VIC 3010 Australia

                                       saffsd@gmail.com, tb@ldwin.net


Abstract      We develop methods for describing users             One problem with current approaches to accessing
based on their posts to an online discussion forum.            threaded discourse data is that they do not take into
These methods build on existing techniques to describe         account the structure of the discourse itself. The bag-
other aspects of online discussions communities, but           of-words (BOW) model standardly used in text classi-
the application of these techniques to describing users        fication and information retrieval (IR) discards all con-
is novel. We demonstrate the utility of our proposed           textual information. However, even in IR it has long
methods by showing that they are superior to existing          been known that much more information than simple
methods over a post-level classification task over a           term occurrence is available. In the modern era of web
published real-world dataset.                                  search, for example, extensive use is made of link struc-
                                                               ture, anchor text, document zones, and a plethora of
Keywords        Document Management, Information Re-
                                                               other document (and query, click stream and user) fea-
trieval, Web Documents
                                                               tures [15].
                                                                  The natural question to ask at this point is, "What
1    Introduction                                              additional structure can we extract from threaded dis-
                                                               course?" Previous work has been done in extracting
People like to talk. In particular, people like to talk to     useful information from various implicit relationships
other people that share their interests, resulting in ev-      between chunks of data in threaded discourse; we de-
erything from hobby groups to clubs to professional as-        scribe this in more detail in Section 2. However, one
sociations. The internet gives people the ability to talk      dimension that has not yet been explored is how we can
to each other on an unprecedented scale, and this has          use information about the identity of the participants
fostered the growth of publicly-accessible communities         to extract useful information from the structure of the
around a gamut of topics, from technology (Slashdot )      1
                                                               discourse. In this paper we will examine how we can
to knitting (Ravelry ), to social interaction for its own
                         2
                                                               extract such user-level features, and how we can use
sake (Facebook ).  3
                                                               them to improve performance over established tasks.
     The most natural form of communication is through            We use the term threaded discourse to describe
dialogue, and in the internet age this manifests itself via    online data that represents a record of messages
modalities such as forums and mailing lists. What these        exchanged between a group of participants.           The
systems have in common is that they are a textual rep-         two most common examples of this are forums and
resentation of a threaded discourse. The Internet is full      mailing lists. In this paper, the data that we examine
of publicly-accessible communities which engage in in-         in Section 5 originates from a site which bridges both.
numerable discourses, generating massive quantities of         Indeed, the techniques we describe should generalize to
data in the process. This data is rich in information,         any data which can be mapped into a similar structure.
and with the help of computers we are able to archive             There are several dimensions to the structure of
it, index it, query it and retrieve it. In theory, this would  threaded discourse that can be useful. For this paper,
allow people to take a question to an online community,        we will focus on the relationships between participants,
search its archives for the same or similar questions,         which we refer to as the user-level structure. However,
follow up on the contents of prior discussion and find         most instances of threaded discourse do not encode
an answer. However, anyone with any experience in              relationships between users explicitly.       Therefore,
searching for an answer to a technical question online         we   must    infer  the user-level  relationships   from
would agree that the situation is seldom that simple.          relationships in other dimensions of the data.         In
                                                               particular, we focus on the following levels of threaded
    1http://www.slashdot.org
                                                               discourse structure:
    2http://www.ravelry.com
    3http://www.facebook.com
                                                               Post-level: The individual unit contributions submitted
                                                                     by participants
Proceedings of the 14th Australasian Document Comput-
ing Symposium, Sydney, Australia, 4 December 2009.             Thread-level: Groupings of posts into a discussion on
Copyright for this article remains with the authors.                 a particular topic



                                                             98

    Our contribution in this paper is to develop methods     Task Orientation: Is the thread about a specific prob-
for describing users based on their posts to an online            lem?
discussion forum. We demonstrate the utility of our
                                                             Completeness: Is the problem described in adequate
proposed methods by showing that they are superior
                                                                  detail?
to existing methods over a post-level classification task
over a dataset from Nabble.   4
                                                             Solvedness: Has a solution been provided?
    The research presented in this paper forms a compo-
nent of a larger research agenda on the utility of user-     They manually annotated a set of 250 threads for these
level characteristics in a variety of user forum tasks [?].  qualities, and extracted a set of features to describe each
                                                             thread based on the aggregation of features from posts
                                                             in different sections of the thread.    We apply a sim-
2    Related Work
                                                             ilar idea, but instead of aggregating over sections of
                                                             the thread, we aggregate posts from a given user. The
This section provides a first-gloss overview of related
                                                             results from [2] were inconclusive, but we have found
work on thread- and post-level text classification, and
                                                             that their feature set can be effective when aggregated
feature-based approaches to capturing user characteris-
                                                             by user. Full details of the feature set are presented in
tics. We return to present the aspects of this work that
                                                             Section 5.3.
are most relevant to our research in greater detail, as
detailed below.
                                                             3    Applications
    Wanas et al. [16] detail a set of post-level features
extracted based on a more structured approach. They          While our experiments in this paper focus exclusively
evaluate their feature set over a classification task        on a post-level classification task, this research has po-
involving post and rating data derived from Slashdot.        tential impact in a much wider range of settings, as
Their task involves classifying discrete posts into one      outlined in this section.
of three quality levels (High, Medium or Low) where
the gold-standard is provided by annotations from the        3.1    Information Access
community itself. We implement part of their feature
                                                             A key application of this paper is to support improved
set for experiments conducted in this paper; more detail
                                                             information access over internet forums, building on the
is provided in Section 4.
                                                             work of Baldwin et al. [2]. The underlying intuition
    Agrawal et al. [1] describe a technique for partition-   here is that not all contributions on a forum are equal in
ing the users in an online community based on their          their usefulness, and that we often find that certain users
opinion on a given topic. They find that basic text clas-    are consistently outstanding in their contribution. Note
sification techniques are unable to do better than the       that this is not the same as the user being an expert --
majority-class baseline for this particular task. They       other qualities come into play, such as how clear their
then describe a technique based on modeling the com-         explanations are, as well as how much effort they put
munity as a reply-to network, with users as individual       into responding. Indeed, a relatively inexperienced user
nodes, and edges indicating that a user has replied to a     may post a detailed description of how he or she tackled
post by another user. They find that using this represen-    a particular problem, which could be extremely valu-
tation, they are able to do much better than the baseline.   able to a similar inexperienced user tacking a similar
Fortuna et al. [5] build on the work done by [1], defining   problem.
additional classes of networks that represent some of
the relationships present in an online community. We         3.2    User Profiling
describe these networks in detail in Section 4, and adapt
                                                             In some situations, we may wish to identify users
them to generate user-level features.
                                                             with particular characteristics.    For example, Kim et
    Weiner et al. [17, 18] propose a set of heuristic
                                                             al. [9] use Speech Act Analysis to classify student
post-level features to predict the perceived quality of
                                                             contributions according to Speech Act categories,
posts using a supervised machine learning approach.
                                                             thereby identifying roles that participants play, using
The data they evaluate over is extracted from Nabble,
                                                             this information to identify when participants require
and they use the ratings provided by users as the gold-
                                                             assistance.    This approach can be enhanced with
standard for a correct classification. They conclude that
                                                             user-level features.
post-level classification using their feature set provides
a substantial improvement over the majority-class            3.3    User Karma
baseline.   We describe the dataset in greater detail
                                                             Karma is formalization of the notion of how influential
Section 5.1, and use it as the basis of our evaluation.
                                                             a user is in an online community. It is the subject of
    In work on thread classification, Baldwin et al.
                                                             much discussion in web communities as it is critical
[2] attempted to classify forum threads scraped from
                                                             to the self-organizing structure of some communities,
Linux-related newsgroups according to three qualities:
                                                             such as Reddit.   5  It is even more influential in other

   4http://www.nabble.com                                       5http://www.reddit.com




                                                           99

  Feature name          Description                                                                           Type

  distribution          Mention of the name of a Linux distribution                                           Boolean
  beginner              Mention of terms suggesting the posted is inexperienced                               Boolean
  emoticons             Presence of "smiley faces"                                                            Boolean
  version numbers       Presence of version numbers                                                           Boolean
  URLs                  Presence of hyperlinks                                                                Boolean
  words                 Number of words in post                                                               Integer
  sentence              Number of sentences in post                                                           Integer
  question sentence     Number of questions in post (sentences ending in `?')                                 Integer
  exclaim sentence      Number of exclamations in post (sentences ending in `!')                              Integer
  period sentence       Number of sentences ending in a period                                                Integer
  other sentence        Number of sentences not falling into the above three categories                       Integer

                                            Table 1: The ILIAD feature set


  Feature name          Description                                                                           Type

  onThreadTopic         Post's relevance to the topic of a thread                                             Float
  overlapPrevious       Post's largest overlap to a previous post                                             Float
  overlapDistance       How far away the previous overlapping post is                                         Integer
  timeliness            Ratio of time interval from previous post to average inter-post interval in thread    Float
  lengthiness           Ratio of length of post to average length of post in thread                           Float
  formatEmoticons       How often emoticons are used with respect to number of sentences                      Float
  formatCapitals        How often capitals are used with respect to number of sentences                       Float
  weblinks              How often weblinks are used with respect to number of sentences                       Float

                                            Table 2: The WANAS feature set


communities, where it is used to give incremental mod-         ture sets. The first, which is henceforth referred to as
eration powers to users (e.g. Stack Overflow ). There
                                                 6             ILIAD, is derived from [2] and is described in Table 1.
is no body of formal research associated with it, and          The second, which is henceforth referred to as WANAS,
sometimes the exact mechanism is a closely-guarded             is derived from [16], and is described in Table 2.
secret. User-level features are relevant to this because           From each of ILIAD and WANAS we derive a user-
they can be used to more fully describe a user, which          level feature set by finding the mean of each feature
in turn can be used to compute a karma score that takes        value over all of the user's posts. These feature sets
into account more aspects of the user's participation.         are referred to as ILIADAGG and WANASAGG, respec-
                                                               tively.
3.4    Automatic Grading

Lui et al. [12] use a text classification approach to per-     4.2    Network-Based
form content analysis. This task involves automatically
                                                               Fortuna et al. [5] present a method of describing forum
grading participation by students in an online learning
                                                               data using Social Network Analysis. The network is a
community. They make use of a fairly simplistic model
                                                               graph representation of relationships within the forum,
of the content.   It may be possible to improve their
                                                               reminiscent of algorithms such as PageRank [4]. In the
approach by extracting more detailed structural infor-
                                                               case of PageRank, each node represents a webpage and
mation from participants' contributions.
                                                               each edge represents a hyperlink. In [5], the authors
                                                               define 3 author networks, where each node represents
4    User-Level Features                                       an author, and 2 thread networks, in which each node
                                                               represents a thread.    The meaning of an edge varies
In Section 2, we outlined existing methods for extract-
                                                               for each network, and each edge may be directed or
ing features to describe posts and threads. In this sec-
                                                               undirected according to the network.
tion, we present methods for extracting features for de-
scribing users.                                                    The authors then use each of these networks to ex-
                                                               tract features on a per-post basis. We briefly summarize
4.1    Aggregate                                               the method here; more detail is provided in [5].
                                                                   For Author Networks, each post is assigned a fea-
The first type of user-level feature we consider are fea-
                                                               ture vector v of length N, where N is the total number
tures derived from aggregation over features describ-
                                                               of nodes, or equivalently, the total number of authors in
ing individual posts. We implement two post-level fea-
                                                               the network. v has at least one feature set to 1, which
                                                               corresponds to the author of the post. Authors directly
   6http://www.stackoverflow.com




                                                            100

connected to the post author in the network receive a        COMMONAUTHORS Thread Network
feature value of 1, and authors that are second-level
                                                             COMMONAUTHORS           is implemented      as  described
neighbours of the post author are set to a feature value
                                                             in [5]. In this network, nodes are threads, and each
of 0.5. All other values in v are set to 0. Since each
                                                             undirected edge indicates that two threads have at
post has a unique author, this network can be used to
                                                             least m authors in common. We followed the original
describe authors without modification.                       research in setting m = 3.
    For Thread Networks, the method for computing a
feature vector is similar to that for Author Networks.
                                                             5    Evaluation
The key difference is that in this instance, the vector v
is of length T , where T is the total number of threads      We evaluate the effectiveness of the features described
in the forum. Therefore, each value vT in the vector         in Section 4 by utilizing them for a classification task.
describes a relationship to a particular thread. In [5],     In this paper, we focus exclusively on a post-level clas-
the authors are interested in the relationship between       sification task, which allows us to assess the usefulness
posts, so they assign to each post the feature vector of     of user-level features in describing post-level data.
the thread it belongs to. However, in our case we do not
                                                             5.1     Dataset
wish to describe a post directly; instead, we are inter-
ested in describing the author. To do this, we consider      The data set we are using is based on that from Weimer
every thread that the author has posted in. For each         and Gurevych [17]. The data consists of 16562 posts
of these threads, we set the feature corresponding to the    across 2956 different threads.     Separately, there are
thread to 1. We then set all the immediate neighbours of     4508 annotations spanning 4291 distinct posts, rating
the threads to 1 as well, and the second-level neighbours    the quality of the post.   Each annotation consists of
thereafter to 0.5.                                           an ordinal rating from 1 to 5 stars, with more stars
    In our work, we consider two Author Networks and         indicating better quality.   We filtered the annotated
one Thread Network:                                          posts by removing all posts with an empty body. We
                                                             also removed all posts that had an average rating of
POSTAFTER (Author Network)                                   exactly 3.0.    This eliminated posts that were rated
                                                             3 once, as well as posts that received contradictory
POSTAFTER is modeled on the reply-to network de-
                                                             ratings, such as a post rated 1 by one user and 5 by
scribed in [5]. Our data does not contain exact informa-
                                                             another, leaving 4094 rated posts. We divided posts
tion about the reply structure in a thread, so we approx-
                                                             into two groups, corresponding to posts with an average
imate this information by the temporal relationship be-
                                                             rating > 3.0, which we consider GOOD, and posts with
tween posts. Effectively, we have made the assumption
that within a thread, each post replies to the post im-      an average rating  3.0, which we consider BAD. In
                                                             the 4094 rated posts, there were 2060 GOOD posts and
mediately preceding it in terms of the time-of-posting.
                                                             2034 BAD posts. Our approach to filtering the data is
We expect that this will generally be the case, but in the
                                                             generally consistent with that in [17]. Differences in
context of the original work by [1] on partitioning users
                                                             our use of the dataset are discussed in Section 6.
by opinion, it is possible that, given three posts A, B
and C, B and C both reply in objection to A, therefore       5.2     Methodology
defining a different network from ours. Nonetheless,
our results will show that our approximation is admis-       For each post, we extracted the feature sets described

sible in that it can be used to augment a BOW feature set    in Section 4, as summarized in Table 3. For user-level

to exceed a benchmark result; we will present evidence       feature sets, we use the features corresponding to the

of this in Section 5.3.                                      post's author to describe the post. We evaluate various

    POSTAFTER is parameterized with two values: dist         combinations of these feature sets by carrying out 10-

and count. Being an Author Network, the nodes repre-         fold cross-validation [10], as follows:
sent authors. Two authors A1 and A2 have a directed            1. Divide the data randomly into 10 partitions
edge from A1 to A2 if and only if A1 submits a post
to a thread that is within dist posts of a post in the         2. For each partition, train a classifier on the other 9
same thread by A2 on at least count occasions. For                partitions
our experiments, we used dist = 1 and count = 3.
                                                               3. Use the trained classifier to predict the categories
                                                                  of the instances in the selected partition
THREADPARTICIPATION Author Network
                                                               4. Pool together the predictions from the 10 iterations
THREADPARTICIPATION is implemented as described
                                                                  and evaluate
in [5]. In this network, nodes are again authors, and
each undirected edge indicates that two authors have         The partitioning is performed once and re-used for each
posted in the same thread on at least k occasions. In the    pairing of learner and feature set. We repeat this proce-
original work, the authors set k = 5, but in out case,       dure using a number of different learners. The learners
we use k = 2 as the network is too sparse for higher         used, along with their parameter settings, are as fol-
settings of k.                                               lows:



                                                          101

     Label                         Type                                          Learner    Accuracy

     BOW                           Post                                          SVM           0.780
     ILIAD                         Post                                          SkewAM        0.812
     WANAS                         Post                                          Maxent        0.820
     ILIADAGG                      User                                          ZeroR         0.489
     WANASAGG                      User
     POSTAFTER                     Author Network            Table 4: Accuracy for each learner when utilizing only
     THREADPARTICIPATION           Author Network            the BOW feature set
     COMMONAUTHORS                 Thread Network
                                                                 Feature Set                     Acc          p
       Table 3: Feature sets used in classification
                                                                 BOW                            0.780        --
SVM: Support vector machines [8] as implemented in               ILIAD                          0.723    2.1×10-   6

     bsvm [6], using the package default values which            WANAS                          0.751    7.3×10-   4

     correspond to an RBF kernel.                                ILIADAGG                       0.831    2.4×10-   6

                                                                                                0.829              4
SkewAM: Nearest-prototype skew divergence, as imple-             WANASAGG                                2.7×10-
                                                                 POSTAFTER                      0.636             13
     mented in hydrat [13]. This is a Rocchio-style                                                     5.1×10-
                                                                 THREADPARTICIPATION            0.670             10
     approach [7], where a centroid is computed for                                                     1.1×10-
                                                                 COMMONAUTHORS                  0.671             11
     each class by finding the arithmetic mean of all the                                               4.2×10-

     instances of the class. Classification is then car-
                                                             Table 5: Accuracy for each feature set over SVM (results
     ried out by assigning the class of the single nearest
                                                             higher than the baseline are highlighted in bold; p is the
     neighbour. The distance metric we use is skew di-
                                                             probability that the result differs from the benchmark
     vergence [11], with a mixing parameter  = 0.99.
                                                             only due to chance)
Maxent: Maximum          entropy    modeling     [3]   as
                                                             majority-class result is less than 0.5 because the
     implemented in the Maximum Entropy Toolkit
                                                             majority class varies across partitions. In 8 of the 10
     [19]. We use L-BFGS for parameter estimation
                                                             partitions, it was the overall majority class (GOOD),
     [14], with 10 iterations of the training algorithm.
                                                             whereas in 2 of the 10 partitions, it was the majority
    For each cross-validated result, we report the overall   class in the training data but overall minority class
classification accuracy (Acc), which is the proportion       (BAD).
of correct predictions made by the classifier; a larger          We establish benchmark results for this task using
number is, naturally, better. When comparing a result        only the BOW feature set. The overall accuracy for
to a benchmark value, we also provide the p-value for        each learner is summarized in Table 4. Immediately,
a two-tailed paired t-test. We can conduct a paired t-       it is apparent that the benchmark result is significantly
test because for each result, the partitions used have       better than the baseline. The best result over only the
been kept constant and thus the performance over them        BOW feature set is attained by Maxent, with an accu-
is directly comparable. The null hypothesis is always        racy of 0.820.
that the difference in the mean accuracy over all 10             Next, we consider each learner over each individual
partitions is identical for both results being compared.     feature set.    For Maxent and SkewAM, this always
Therefore, a low p-value indicates that it is highly im-     leads to results that are below the BOW benchmark.
probable that the two combinations of feature sets being     For SVM, however, the aggregate features ILIADAGG
considered have led to the same results. To facilitate       and WANASAGG do better than the BOW benchmark,
discussion of statistical significance, we will consider     attaining an accuracy of 0.831 and 0.829, respectively.
a p-value < 0.05 to be statistically significant. This       These   are   different   from    the  BOW    result   with
corresponds to the 5% significance level that is com-        p = 2.4×10- and p = 2.7×10- respectively. Both
                                                                            6                       4

monly reported. In tables, p-values that are statistically   results are statistically significant. We report results for
significant at the 5% significance level are shown in        each feature set in Table 5.
bold.                                                            We then investigate the use of the various feature
    Our experiments were performed using hydrat              sets to augment BOW, as presented in Table 6. A fairly
[13],   an   open-source    framework    for  comparing      consistent picture emerges from this: the ILIAD and
classification systems.     hydrat provides facilities       ILIADAGG feature sets cause performance to drop
for managing and combining feature sets, setting up          when combined with the BOW feature set, whereas
cross-validation tasks and automatically computing           all other feature sets cause performance to rise with
corresponding results.                                       respect to BOW.
                                                                 We also experimented with feature ablation, by
5.3    Results
                                                             examining the result of removing one feature set at a
The baseline for this task is a majority-class (ZeroR)       time from the full set of features. The results for this are
result of 0.489.   Although this is a binary task, the       reported in Table 7. Surprisingly, removing a particular



                                                          102

  Learner          Feature Sets Present       Acc    p        Learner  Feature Set                         Acc      p

           BOW                               0.780  --                 ALL                                0.775    --
           BOW      ILIAD                    0.746 0.001               -BOW                               0.796   0.005
           BOW      WANAS                    0.790 0.202               -ILIAD                             0.775   1.000
           BOW      ILIADAGG                 0.768 0.136               -WANAS                             0.775   0.949
   SVM
           BOW      WANASAGG                 0.797 0.041       SVM     -ILIADAGG                          0.770   0.508
           BOW      POSTAFTER                0.780 0.978               -WANASAGG                          0.776   0.897
           BOW      THREADPARTICIPATION      0.790 0.243               -POSTAFTER                         0.775   0.949
           BOW      COMMONAUTHORS            0.786 0.492               -THREADPARTICIPATION               0.778   0.731
           BOW                               0.812  --                 -COMMONAUTHORS                     0.777   0.834
           BOW      ILIAD                    0.799 0.041               ALL                                0.776    --
           BOW      WANAS                    0.827 0.005               -BOW                               0.689   0.000
           BOW      ILIADAGG                 0.805 0.236               -ILIAD                             0.778   0.750
  SkewAM
           BOW      WANASAGG                 0.830 0.001               -WANAS                             0.769   0.248
           BOW      POSTAFTER                0.825 0.019      SkewAM   -ILIADAGG                          0.788   0.099
           BOW      THREADPARTICIPATION      0.827 0.008               -WANASAGG                          0.764   0.024
           BOW      COMMONAUTHORS            0.829 0.005               -POSTAFTER                         0.785   0.140
           BOW                               0.820  --                 -THREADPARTICIPATION               0.811   0.000
           BOW      ILIAD                    0.624 0.000               -COMMONAUTHORS                     0.812   0.000
           BOW      WANAS                    0.843 0.025               ALL                                0.741    --
           BOW      ILIADAGG                 0.564 0.000               -BOW                               0.687   0.003
  Maxent
           BOW      WANASAGG                 0.849 0.002               -ILIAD                             0.648   0.000
           BOW      POSTAFTER                0.834 0.127               -WANAS                             0.730   0.503
           BOW      THREADPARTICIPATION      0.836 0.088      Maxent   -ILIADAGG                          0.697   0.082
           BOW      COMMONAUTHORS            0.840 0.043               -WANASAGG                          0.714   0.129
                                                                       -POSTAFTER                         0.741   0.975
Table 6: Accuracy for each learner when combining                      -THREADPARTICIPATION               0.737   0.768
                                                                                                          0.738   0.825
each feature set with BOW (results better than the BOW                 -COMMONAUTHORS

benchmark for each learner are highlighted in bold; p is
                                                            Table 7: Accuracy for feature ablation over the full
the probability that a result differs from the benchmark
                                                            feature set for each learner (results better than the BOW
only due to chance, and p-values significant at the 5%
                                                            benchmark for each learner are highlighted in bold; p is
level are highlighted in bold)
                                                            the probability that a result differs from the benchmark
feature set can result in a statistically significant       only due to chance, and p-values significant at the 5%
performance increase for both SVM and SkewAM. For           level are highlighted in bold)
SVM, the feature set in question is BOW, whereas
                                                            original authors made use of 3418 posts, whereas we
for SkewAM, removing THREADPARTICIPATION or
                                                            use 4094 posts. The bulk of the difference is due to
COMMONAUTHORS leads to a statistically significant
                                                            the original authors eliminating posts which they deter-
increase in results. Maxent is the only learner where
                                                            mined to be non-English. We did not do this because
there is no significant increase resulting from removing
                                                            some of our methods do not make use of any language-
a single feature set.
                                                            specific information, so we were still able utilize the
    Finally, we proceed to test other combinations of
                                                            non-English data. According to [17], there are 668 non-
feature sets. We exhaustively tested all possible com-
                                                            English posts.
binations of two and three feature sets, as well as all
                                                               The remaining difference results from the original
feature sets, all feature sets minus one, and all feature
                                                            authors opting to eliminate any posts with `contradic-
sets minus ILIAD and ILIADAGG.            The best com-
                                                            tory ratings', in that the post received ratings both > 3
bination that we found was using BOW, WANAS and
                                                            and  3, whereas we only eliminated posts where the
COMMONAUTHORS, with Maxent as the learner. This
                                                            average rating was = 3.0. In practice, the difference is
produced an accuracy of 0.854. However, the top 10
                                                            negligible as it only accounts for 8 out of 4291 posts.
combinations of features and learners all produced very
                                                               The original authors report a maximum accuracy of
similar results, so we cannot conclude that this is the
                                                            0.775 over their ALL task. Their values are not directly
undisputed best combination overall. We also found
                                                            comparable to ours because the two tasks are not iden-
that the best combination of feature sets for SVM was
                                                            tical, as we have described above. However, they are
different from that for Maxent, but was still extremely
                                                            very similar, so our best accuracy of 0.854 suggests that
close to the best result. The top 10 combinations that
                                                            our technique would yield an improvement if applied
we found over the classifiers considered are reported in
                                                            directly to the original task.
Table 8.
                                                               We found that, even in isolation, user-level features
                                                            can outperform a benchmark based on the conventional
6   Discussion
                                                            IR bag-of-words approach, to a high level of statisti-
As noted in Section 5.1, our dataset is based on data       cal significance. This is important because it justifies
originally used in [17]. Our task is most similar to the    the use of user-level features for post-level classifica-
ALL task of [17], in that we do not divide the data on      tion tasks. Furthermore, we showed that most of the
the basis of the Nabble sub-forum it originates from.       user level feature sets can be added to the basic bag-
We have also filtered the data slightly differently. The    of-words model to improve its performance, and that



                                                         103

                                                     Feature Sets
  Learner                                                                                                      Acc    p
           BOW   ILIAD   WANAS   ILIADAGG    WANASAGG   POSTAFTER      THREADPARTICIPATION  COMMONAUTHORS

  Maxent                                                                                                      0.854 0.002
  Maxent                                                                                                      0.850 0.002
  Maxent                                                                                                      0.850 0.002
  Maxent                                                                                                      0.849 0.002
  SVM                                                                                                         0.848 0.006
  SVM                                                                                                         0.847 0.004
  Maxent                                                                                                      0.847 0.006
  Maxent                                                                                                      0.845 0.012
  Maxent                                                                                                      0.843 0.025
  Maxent                                                                                                      0.842 0.027


Table 8: Top 10 Results over different combinations of learner and feature sets (p is the probability that the result
differs from the Maxent BOW benchmark only due to chance; p-values significant at the 5% level are highlighted
in bold)


this behaviour is consistent across a range of different        7    Further Work
learners.
                                                                Previous studies have either used only a single classifi-
    Table 8 suggests that the ILIAD feature set is gener-       cation method [5, 16, 17, 18], or have not found signif-
ally ineffective, which may explain the poor results re-        icant differences between the relative performance of
ported in [2]. However, the SVM learner is able to make         learners with respect to a given feature set [2]. How-
effective use of the user-level aggregates of ILIAD,            ever, we have seen that over the data being examined in
ILIADAGG, whereas the Maxent learner is not. This               this paper, learners respond better to particular feature
is reflected in both the single-featureset experiment re-       sets. We intend to investigate this further by applying
ported in Table 5, as well as the overall results in Ta-        the technique to a wider variety of tasks over a greater
ble 8. The reason for this is not immediately obvious,          number of datasets.
and further investigation may yield insight into how to             We have also adopted a relatively simplistic
reconcile the two.                                              approach to aggregating post-level features at a user
                                                                level, simply computing the arithmetic mean of the
    Another obvious difference between the results
                                                                feature values. Further work would involve taking more
from the Maxent and SVM learners is that Maxent
                                                                information into account, for example the variance and
performs best in the presence of the BOW features,
                                                                skew in each post-level feature when examined at a
whereas     SVM performs     better  without   the  BOW
                                                                user-aggregate level. Another dimension to be taken
features.    This trend is clearly visible in Table 7,
                                                                into account is that a user's knowledge and attitude
where for SVM, removing the BOW features leads to a
                                                                evolve over time, so we may need to introduce some
statistically significant increase in the results, whereas
                                                                kind of temporal weighting to the post-level features
for Maxent and SkewAM, it causes a significant drop.
                                                                we aggregate to produce the user-level profile.
This trend is also visible in Table 8, where we see
                                                                    Finally, it is also important to note that the gold-
that the top results for Maxent include BOW, whereas
                                                                standard labels are provided by anonymous internet
the top results for SVM exclude it. Again the reason
                                                                users, and that each post often has only a single
for this is not immediately clear.      What is clear is
                                                                annotation. It is therefore difficult to establish exactly
that each learner is effective over different sets of
                                                                how well the annotation reflects the opinion of the
features, so there may be scope for further work in
                                                                entire community with respect to the post annotated.
terms of applying meta-classification techniques such
                                                                Further work in this respect would involve establishing
as stacking in order to further improve results.
                                                                datasets where there are a number of annotations for
                                                                each post, so as to be able to judge inter-annotator
    It is important to consider the implications of using
                                                                agreement and have a feeling for the upper bound in
user-level features for performing a classification task
                                                                terms of possible classifier performance on the task.
over the `quality' of a post. The fact that user-level
features in isolation can perform better than the baseline
is a strong case for the argument that users are con-           8    Conclusion
sistently good or consistently bad, indicating that the
quality of a user's previous posts is a good predictor for      In this paper, we have shown that user-level features can
the quality of future posts. However, we also expect that       improve performance over classification tasks involving
the quality of an individual post can vary; it therefore        posts.  We started by defining threaded discourse as
makes sense that the best results we have obtained use          an umbrella term for online discussions, and deriving
a mixture of features, some reflecting purely the con-          several sets of features for describing users based on
tent of the post, and some reflecting the overall posting       techniques for describing other aspects of the threaded
trends of the user.                                             discourse.



                                                          104

    We evaluated our features over a dataset that                [9] Jihie Kim, Grace Chern, Donghui Feng, Erin Shaw and
has been used in previous research, defining a task                  Eduard Hovy. Mining and assessing discussions on the
similar to that previously investigated. We established              web through speech act analysis.      In Proceedings of

a majority-class baseline for the task, as well as a                 the ISWC'06 Workshop on Web Content Mining with
                                                                     Human Language Technologies, Athens, USA, 2006.
benchmark result based on a conventional bag-of-
words model for each post. We investigated our feature          [10] Ron Kohavi.      A study of cross-validation and boot-
sets in isolation, as well as their interactions, across             strap for accuracy estimation and model selection. In

three different off-the-shelf learners.     We found that            Proceedings of the 14th International Joint Conference
                                                                     on Artificial Intelligence (IJCAI-95), pages 1137­1145,
in general, user-level features performed significantly
                                                                     Montr´eal, Canada, 1995.
better than simple BOW features on the given task,
and that different learners seemed to prefer a different        [11] Lillian Lee. Measures of distributional similarity. In

combination of feature sets.                                         Proceedings of the 37th Annual Meeting of the As-
                                                                     sociation for Computational Linguistics, pages 25­32,
    We succeeded in our primary goal of showing that
                                                                     College Park, USA, 1999.
user-level features are effective in classifying posts ac-
cording to quality, and we expect that the use of these         [12] Andrew Kwok-Fai Lui, Siu Cheung Li and Sheung On

features will generalize well to tasks over other aspects            Choy. An evaluation of automatic text categorization
                                                                     in online discussion analysis.    In Proceedings of the
of threaded discourse, for example in profiling users or
                                                                     Seventh IEEE International Conference on Advanced
in classifying threads.
                                                                     Learning Technologies (ICALT 2007), pages 205­209,
                                                                     Niigata, Japan, 2007.
References
                                                                [13] Marco Lui and Timothy Baldwin.          hydrat.   http:
                                                                     //hydrat.googlecode.com, 2009.            Retrieved on
 [1] Rakesh Agrawal, Sridhar Rajagopalan, Ramakrishnan
                                                                     15/09/2009.
     Srikant and Yirong Xu.       Mining newsgroups using
     networks arising from social behavior. In Proceedings      [14] Robert Malouf. A comparison of algorithms for max-
     of the Twelfth International World Wide Web Conference          imum entropy parameter estimation.       In Proceedings
     (WWW'03), pages 529­535, Budapest, Hungary, 2003.               of the 6th Conference on Natural Language Learning
                                                                     (CoNLL-2002), pages 49­55, Taipei, Taiwan, 2002.
 [2] Timothy Baldwin, David Martinez and Richard Baron
     Penman. Automatic thread classification for linux user     [15] Christopher D. Manning, Prabhakar Raghavan and Hin-
     forum information access. In Proceedings of the Twelfth         rich Sch¨utze.   Introduction to Information Retrieval.
     Australasian Document Computing Symposium (ADCS                 Cambridge University Press, Cambridge, UK, 2008.
     2007), pages 72­9, Melbourne, Australia, 2007.             [16] Nayer Wanas, Motaz El-Saban, Heba Ashour and
 [3] Adam L. Berger, Stephen A. Della Pietra and Vincent             Waleed Ammar. Automatic scoring of online discussion
     J. Della Pietra. A maximum entropy approach to natural          posts.  In Proceedings of the 2nd ACM Workshop on
     language processing. Computational Linguistics, Vol-            Information Credibility on the web (WICOW '08), Napa
     ume 22, Number 1, pages 39­71, 1996.                            Valley, USA, 2008.

 [4] Sergei Brin and Larry Page. The anatomy of a large-        [17] Markus Weimer and Iryna Gurevych.         Predicting the

     scale hypertextual web search engine. Computer Net-             perceived quality of web forum posts. In Proceedings

     works and ISDN Systems, Volume 30, Number 1-7,                  of the 2007 Conference on Recent Advances in Natural

     pages 107­117, 1998.                                            Language Processing (RANLP-07), Borovets, Bulgaria,
                                                                     2007.
 [5] Blaz Fortuna, Eduarda Mendes Rodrigues and Natasa
                                                                [18] Markus Weimer, Iryna Gurevych and Max M¨uhlh¨auser.
     Milic-Frayling. Improving the classification of news-
                                                                     Automatically assessing the post quality in online dis-
     group messages through social network analysis.       In
                                                                     cussions on software. In Proceedings of the 45th Annual
     Proceedings of the Sixteenth ACM Conference on Infor-
                                                                     Meeting of the ACL: Interactive Poster and Demonstra-
     mation and Knowledge Management (CIKM '07), pages
                                                                     tion Sessions, pages 125­128, Prague, Czech Republic,
     877­880, Lisboa, Portugal, 2007.
                                                                     2007.
 [6] Chih-Wei Hsu and Chih-Jen Lin. BSVM-2.06. http:
                                                                [19] Le Zhang.        Maximum entropy toolkit.         http:
     //www.csie.ntu.edu.tw/cjlin/bsvm/, 2006. Re-
                                                                     //homepages.inf.ed.ac.uk/lzhang10/maxent_
     trieved on 15/09/2009.
                                                                     toolkit.html, 2004. Retrieved on 15/09/2009.
 [7] David Hull.     Improving text retrieval for the routing
     problem using latent semantic indexing. In Proceedings
     of the 17th Annual International ACM SIGIR Con-
     ference on Research and Development in Information
     Retrieval (SIGIR '94), pages 282­291, Dublin, Ireland,
     1994.

 [8] Thorsten Joachims.     Text categorization with support
     vector machines: learning with many relevant features.
     In Proceedings of the 10th European Conference on Ma-
     chine Learning, pages 137­142, Chemnitz, Germany,
     1998.



                                                             105

   Investigating the use of Association Rules in Improving Recommender
                                                      Systems

                         Gavin Shaw                                                  Yue Xu

          School of Information Technology                        School of Information Technology
        Queensland University of Technology                     Queensland University of Technology
                 Brisbane QLD Australia                                  Brisbane QLD Australia
              g4.shaw@student.qut.edu.au                                    yue.xu@qut.edu.au

                                                    Shlomo Geva

                                      School of Information Technology
                                   Queensland University of Technology
                                            Brisbane QLD Australia
                                                s.geva@qut.edu.au


Abstract     Recommender systems are widely used on-              Our proposal here focuses on one aspect of the cold-
line to help users find other products, items etc that they   start problem; short user profiles. When a user (can
may be interested in based on what is known about that        be a new user) has very few ratings in their profile,
user in their profile.                                        recommender systems may fail to provide recommen-
   Often however user profiles may be short on infor-         dations that interest the user. Both collaborative based
mation and thus when there is not sufficient knowledge        [5] and content based [2] can suffer, due to collaborative
on a user it is difficult for a recommender system to         systems failing to find similar users and content systems
make quality recommendations. This problem is often           having problems due to lack of being able to obtain the
referred to as the cold-start problem.                        content interests of the user.
   Here we investigate whether association rules can              We propose expanding a user profile (eg. so it con-
be used as a source of information to expand a user           tains more ratings) through the use of association rules
profile and thus avoid this problem, leading to improved      derived from the dataset. By doing so we expand pro-
recommendations to users. Our pilot study shows that          files based on patterns and associations of items, topics,
indeed it is possible to use association rules to improve     categories etc (which should give relevant consequents)
the performance of a recommender system. This we              and thus give more information to a recommender sys-
believe can lead to further work in utilising appropriate     tem. This would reduce the effect of the cold-start prob-
association rules to lessen the impact of the cold-start      lem and result in better quality recommendations earlier
problem.                                                      on.

Keywords      Information Retrieval, Personalised Doc-
uments, Recommender Systems, Association Rules.               2    Related Work

                                                              Much work has been done in the area of recommender
1    Introduction                                             systems. Work focusing on solving the cold-start prob-
                                                              lem includes collaborative & content hybrids [1] [5],
Recommender systems are designed to understand a
                                                              ontology based systems [3] and taxonomy driven rec-
users interests, learn from them and recommend items
                                                              ommender systems [6] [7]. However, all of these pro-
(whether they be products, books, movies etc) that
                                                              posals have drawbacks. The hybrid systems can lack
will be of interest to the user. This requires them to
                                                              novelty, resulting in recommendations that are exces-
personalise their recommendations. Recommendation
                                                              sively content centric [7]. The onotolgy based system
systems usually work most effectively when user
                                                              requires a well defined ontology to be created, some-
profiles are extensive and/or the applicable dataset has
                                                              thing that can be difficult and would limit the system to
a high information density. When the dataset is sparse
                                                              what is defined/covered within the ontology. Taxonomy
or user profiles are short, then recommender systems
                                                              based systems work better, but still have low perfor-
struggle to provide quality recommendations. This is
                                                              mance. Also the HTR system proposed in [6] performs
often known as the cold-start problem.
                                                              only marginally better than the TPR system proposed
Proceedings of the 14th Australasian Document Comput-         in [7], although it is more time efficient. The taxonomy
ing Symposium, Sydney, Australia, 4 December 2009.            based approach in [7] does have the advantage of being
Copyright for this article remains with the authors.          able to be applied to many domains.




                                                           106

    Work has also focused on the other cold-start prob-       u is a 'transaction' and all the topics t in the taxonomy
lem, when a new item is introduced and recommenda-            make up the datasets attributes. Then we populate the
tions are required, but no one has yet rated that item [5].   dataset using the set of users U, the set of rated items
For this cold-start problem collaborative systems can         R and the set of taxonomic descriptors D. This is done
not help, but content based systems can [5]. We focus         by determining the items i rated by the user u and their
on the cold-start problem of new users, rather than new       positions within the taxonomy. Each item will corre-
items.                                                        spond to one or more paths through the taxonomy from
                                                              the root to a leaf. That is, the item may have one or
3    Proposed Approach - Using Association                    more descriptors. For a user ux and a  R(Ux), using
     Rules to Expand User Profiles                            the descriptors {d1(a), d2(a), ...}, we place a positive
                                                              value '1' in the user's transaction at each topic involved
Here we outline our proposed approach and investiga-
                                                              in these descriptors. All other topics in the transaction
tion into solving the cold-start problem in recommender
                                                              will be marked with a negative value '0'. From this we
systems.
                                                              can construct a transactional dataset that shows users'
3.1     Background                                            interests in topics, not items.
                                                                  Second, we then mine the transactional dataset for
In its simplest form, a recommender system takes what
                                                              frequent patterns and derive association rules from
it knows about a particular user (perhaps a profile) and
                                                              these patterns. The frequent patterns and rules will not
attempts to predict what that given user would be inter-
                                                              come from just one taxonomy level, but rather mutiple
ested in and recommend those items to the user. Infor-
                                                              levels and will also include cross-level patterns and
mation can take the form of what a user has rated, what
                                                              rules. This will give us association rules between topics
other users with similar tastes have rated, the content
                                                              that interest users.  These rules allow us to discover
of the rating (if any) and the content of the item(s). In
                                                              topics that frequently appear together as part of a user's
the case of TPR [7] it uses a given user's rating history
                                                              interest. This rule set will then be used to expand user
of items and the user's implicit taxonomic preferences
                                                              profiles to solve the cold-start problem.
to determine new items that will interest the user. Both
                                                                  Next, we create the user profiles that will be needed
a user's history and implicit taxonomic preferences can
                                                              by the recommender system. For our investigation we
be easily obtained from the dataset without any extra
                                                              use the TPR system first proposed in [7]. In order to
effort on the users behalf.
                                                              achieve this we will use the set of users U, the set of
    The drawback to this is when a user has a small
                                                              rated items R, the taxonomy T and the set of descriptors
number of ratings (what we refer to as a 'short profile')     D to create a set of user profiles P = {p1, p2, ..., pn}.
it becomes difficult to effectively determine the users       Here for each user u  U we determine the leaf topics
implicit taxonomic preferences. Thus recommendation           that correspond to each item i  R(ux) through the
quality suffers.
                                                              use of the descriptor(s) d(i). These leaf topics are then
3.2     Expanding User Profiles                               added to the profile p(ux) so that p(ux) contains a list
                                                              of leaf topics for which user ux has rated at least one
sually at the heart of every recommender system are the       item i in each leaf topic t. The set of user profiles P is
user profiles. It is from this that recommenders base         known as the user taxonomy profiles and is used by the
their work. In order to improve the quality of recom-         TPR approach to perform recommendations. This set
mendations being made to users with short profiles, we        of profiles P will serve as our baseline.
propose to use association rules to expand the number             Finally, we expand the user profiles. For this we
of entries in their profiles.                                 take the set of user profile P and the association rule set
    We have a set of users U = {u1, u2, ..., un} and a        we derived in the second step. For each user profile
set of items I = {i1, i2, ..., im}. Every user u  U           p(u ) we extract all of the topics t within and gen-
has a set of rated items R(u)  I whereby if an item               x
                                                              erate a list of all the combinations possible from the
i is in the set R then user u has rated it. We also have      group of topics. Each combination represents a pos-
a taxonomy T containing topics (or categories) t in a         sible antecedent of an association rule. We take each
multi-level structure, where each topic has one parent        combination and search the set of association rules for
or supertopic, but may have many children or subtopics.       any rules that have that exact set of topics as its an-
Thus the taxonomy can be visualised as a tree. Each           tecedent.   If such a rule exists we can then take its
path from the root to a leaf is called a descriptor which     consequent and the topics within and add them to the
is an ordered list consisting of the topics on the path.      profile p(ux). Thus this generates a set of expanded
For any item, it may have more than one descriptor.           user profiles which we show in our experiments have
Let D = {d1, d2, ..., do} be the set of all descriptors,      the potential to improve recommendations over profiles
for an item i  I, its descriptors can be represented as       that have not been expanded.
{d1(i),d2(i),...} which is a subset of D. All of this in-
formation can be used to expand existing short profiles.
    Firstly, using the set of users U and the taxonomy
T we can build a transactional dataset where each user



                                                           107

3.3    Imposing Restrictions on User Profile                 4.2    Dataset
       Expansion
                                                             For this investigation we use the BookCrossing
We have outlined our proposal for using association          dataset  (obtained    from   http://www.informatik.uni-
rules to expand user profiles in order to improve rec-       freiburg.de/ cziegler/BX/) which contains users, books
ommender system quality. However, it is possible that        and the ratings given to those books by the user. The
we may want to place limitations on the expansion of         taxonomy tree and descriptors are orginally sourced
user profiles.                                               from Amazon.com and are exactly the same as those
                                                             used in [6]. From this we build a transactional dataset
  1. Restrict the expansion to short profiles. The idea      that contains 92,005 users (transactions) and 12,147
     behind this proposal is to expand users who have        topics from the taxonomy.     The dataset is populated
     very few ratings and thus suffer from the cold-start    using the descriptors that belong to 270,868 unique
     problem. Users with many ratings do not have this       books.     This dataset is then mined to derive the
     problem. Thus a restriction should be imposed on        association rules from it.
     how many topics can be in the user profile p before         From the BookCrossing dataset we also build the
     there is too many to warrent expansion. This limit      base set of user profiles P. This set of profiles contains
     would be dependent on several factors.                  85,415 distinct users with a total of 10,662 leaf topics
                                                             contained in the taxonomy. As already mentioned the
  2. Restrict the number of rules used when expanding
                                                             ratings for each user are divided into a training set and
     a user profile. It is entirely possible that when de-
                                                             a test set. The set of user profiles P is based on the
     riving the association rules from the transactional
                                                             training set. The average number of leaf topics in a
     dataset that a large list may be generated. It is
                                                             user profile is 27.08 and the highest number of leaf
     also possible that from this, when expanding a user
                                                             topics in a given user profile is 3,173 leaf topics. This
     profile that a large number of rules and their con-
                                                             set of user profiles will serve as the baseline in our
     sequents will be considered for inclusion in the
                                                             experiments and is also the set that will be expanded
     expanded user profile. This may lead to poorer
                                                             using the derived association rules.
     performance as many more topics are added and
     more items from a wider selection become recom-         4.3    Experiment Results
     mended. Therefore it may be benefical to limit the
     number of association rules that are used in expan-     To validate our proposal we conducted a series of ex-

     sion to those that have the highest support, con-       periments to see whether using association rules to ex-

     fidence or other appropriate interestingness mea-       pand user profiles improves recommendation quality.

     sure.                                                   From the transactional dataset we set the minimum con-
                                                             fidence threshold to 50% and are able to derive 37,827
                                                             association rules using the MinMax rule mining algo-
4   Experiments and Evaluation
                                                             rithm [4]. We then go through the user profiles in the
Here we outline the pilot experiment we undertook to         training set and for any profile p  P (train) that has
study the value of our proposal to use association rules     5 or less topics listed we attempt to expand using the
in expanding user profiles to improve recommendation         association rules.   This yields a total of 15,912 user
quality.                                                     profiles which we consider to be short profiles. After
                                                             attempting profile expansion we then make up to 10
4.1    Evaluation Metrics
                                                             recommendations for these 15,912 users and measure
In order to evaluate the performance of the baseline set     the overall performance of the recommender system.
of profiles and the expanded set of profiles we follow       We compare our proposed approach (involving the ex-
the same approach detailed in [6]. The past ratings of       panded profiles) against the baseline of the same 15,912
each user u  U is divided into a training component          user profiles with no expansion. All experiments use
and a test component. For the experiments, the recom-        the TPR recommender system first presented in [7].
mender system will recommend a list of n items for user          As shown in Table 1 the baseline set of user pro-
u based on the training set. The recommendation list         files (which there is no profile expansion) scores only
 i
will be evaluated against the test set. For our experi-      0.00619, 0.0571 and 0.0112 for precision, recall and
ments we use exactly the same training and test sets as      F1-measure respectively. When using expanded pro-
used in [6].                                                 files we manage to achieve up to 0.00815, 0.0754 and
   In our work we use precision, recall and F1-              0.0147 for precision, recall and F1-measure. This is an
measure to determine the overall performance of the          improvement of approximately 31.5% over the base-
recommender system.         This allows us to compare        line.  This level of improvement was achieved when
the standard approach against our proposal of using          we used the top 5 rules (ranked by their confidence
association rules for user profile expansion.                score) to expand user profiles. Table 1 also shows that
                                                             the performance of our proposed approach improves
                                                             as more rules are used in expanding a user's profile.
                                                             However, the improvement is between using the top 2



                                                          108

                         Table 1: Experimental results for TPR using the short user profiles.
                   Approach            Precision       %       Recall         %       F1-Measure          %
             Baseline (No Rules)        0.00619               0.0571                     0.0112
            Expanded (1 Top Rule)       0.00649      4.77%    0.0595       4.28%         0.0117         4.72%
           Expanded (2 Top Rules)       0.00714     15.21%    0.0655      14.66%         0.0128        15.16%
           Expanded (3 Top Rules)       0.00732     18.15%    0.0672      17.77%         0.0132        18.12%
           Expanded (4 Top Rules)       0.00792     27.79%    0.0729      27.75%         0.0143        27.79%
           Expanded (5 Top Rules)       0.00815     31.54%    0.0749      31.22%         0.0147        31.51%


or top 3 rules is small. A similar situation also occurs     the best for expanding the user's profile. Also more in-
between the top 4 and top 5 rules.                           vestigation into how many of the top rules to use needs
    This experiment shows that the overall performance       to be undertaken. This would help determine if it is
of the recommender system can be improved through            possible to use too many rules during profile expansion
the use of our proposed approach, with a 30+% im-            such that recommender performance is degraded. Fi-
provement being achieved, which we believe supports          nally, with the issue of redundant rules, this application
our proposal.   The efficiency of the recommender is         could be used to help confirm that rules removed as
not negatively impacted, as while our expanded pro-          redundant do not cause information loss. This could
files take longer to make recommendations for, the time      be done by comparing the performance of a rule set
taken is inline with that needed to process a profile with   containing redundant rules against one that does not.
a similar number of topics without profile expansion.        Acknowledgements          Computational      resources   and
    We also conducted a second smaller experiment.           services used in this work were provided by the HPC
We took the 15,912 user profiles that had been deemed        and Research Support Unit, Queensland University of
to be short and while attempting to expand them, we          Technology, Brisbane, Australia.
determined which profiles were actually expanded.
From the 15,912 short profiles we were able to expand        References
11,273 profiles. We then evaluated the recommender
                                                             [1] R. Burke. Hybrid Recommender Systems: Survey and
system on just these profiles. As Table 2 shows the
                                                                 Experiments. User Modelling and User-Adapted Inter-
use of the top 5 association rules to expand these user          action, Volume 12, pages 331­370, 2002.
profiles resulted in an improvement in the performance
                                                             [2] P. Melville, R. J. Mooney and R. Nagarajan. Content-
of the recommender system of 39.7% over the                      Boosted Collaborative Filtering for Improved Recom-
baseline. This experiment shows that for users whose             mendations.     In 18th National Conference on Artifi-
profile can be expanded, noticable improvements in               cial Intelligence (AAAI'02), pages 187­192, Edmonton,
recommendation performance are achievable. Thus it               Canada, July 2002.
appears that association rules can make a difference in      [3] S. E. Middleton, H. Alani, N. R. Shadbolt and D. C. De
recommender system performance.                                  Roure.    Exploiting Synergy Between Ontologies and
                                                                 Recommender Systems. In The Semantic Web Workshop,
Table 2: Experimental results for TPR using only the             World Wide Web Conference (WWW'02), pages 41­50,

short user profiles that were sucessfully expanded.              Hawaii, USA, May 2002.

           Approach            F1-Measure        %           [4] N. Pasquier, R. Taouil, Y. Bastide and G. Stumme. Gener-

     Baseline (No Rules)          0.0113                         ating a Condensed Representation for Association Rules.
                                                                 Journal of Intelligent Information Systems, Volume 24,
   Expanded (5 Top Rules)         0.0158       39.74%
                                                                 pages 29­60, 2005.

                                                             [5] A. I. Schein, A. Popescul, L. H. Ungar and M. Pennock.
5    Conclusion and Future Work                                  Methods and Metrics for Cold-Start Recommendations.
                                                                 In 25th Annual International ACM SIGIR Conference
In this paper we proposed the idea of using association          on Research and Development in Information Retrieval
rules to expand user profiles in order to improve recom-         (SIGIR'02), pages 253­260, Tampere, Finland, August
mendations. We outline an approach whereby the rules             2002.
can be discovered and used, increasing the number of         [6] L.-T. Weng, Y. Xu, Y. Li and R. Nayak. Exploiting Item
topics in a user profile that only has a few existing rat-       Taxonomy for Solving Cold-start Problem in Recommen-
ings. Our experiment shows that the proposed approach            dation Making.     In IEEE International Conference on
can improve the performance of a recommender system              Tools with Artificial Intelligence (ICTAI'08), pages 113­
under the cold-start problem. This approach allows a             120, Dayton, Ohio, USA, November 2008.

user profile to obtain more topic information without        [7] C.-N. Ziegler,    G. Lausen and L. Schmidt-Thieme.
extra input from a user and allows a new user to get             Taxonomy-driven Computation of Product Recommen-

better recommendations faster.                                   dations.    In International Conference on Information
                                                                 and Knowledge Management (CIKM'04), pages 406­
    Further work includes discovering if there is a better
                                                                 415, Washington D.C., USA, November 2004.
measure to rank the rules so that the rules selected are



                                                          109

The Methodology of Manual Assessment in the Evaluation
                                               of Link Discovery
    Wei Che (Darren) Huang                          Andrew Trotman                          Shlomo Geva

  Faculty of Science and Technology            Department of Computer Science      Faculty of Science and Technology
  Queensland University of Technology               University of Otago            Queensland University of Technology
          Brisbane, Australia                      Dunedin, New Zealand                   Brisbane, Australia
   w2.huang@student.qut.edu.au                     andrew@cs.otago.ac.nz                  s.geva@qut.edu.au


Abstract - The link graph extracted from the Wiki-                  In a growing collection, such as the Wikipedia,
pedia has often been used as the ground truth for               the maintenance of the link graph can quickly be-
measuring the performance of automated link dis-                come more time-consuming and complicated than
covery systems. Extensive manual assessments expe-              adding content. Newly created documents should be
riments at INEX 2008 recently showed that this is               linked to from text anchors in existing pages. Links
unsound and that manual assessment is essential.                to deleted documents must be erased. There is also
This paper describes the methodology for link dis-              general maintenance of the link graph for documents
covery evaluation which was developed for use in the            that change or are extended.
INEX 2009 Link-the-Wiki track. In this approach
                                                                    Several [2, 3, 4, 5] automated link discovery algo-
both manual and automatic assessment sets are gen-
                                                                rithms have been proposed as methods to alleviate
erated and runs are evaluated using both. The ap-
                                                                the link maintenance problem. The INEX Link-the-
proach offers a more reliable evaluation of link dis-
                                                                Wiki Track [6] takes the traditional link discovery
covery methods than just automatic assessment. A
                                                                problem a step further with focused link discovery.
new evaluation measure for focused link discovery is
                                                                The aim is to identify text anchors in a source docu-
also introduced.
                                                                ment and a best entry point (BEP) in a target docu-
Keywords                                                        ment. In HTML a BEP is a named anchor and an
                                                                anchor-to-BEP link is specified using #name on the
Wikipedia, Link Quality, Manual Assessment, Eval-
                                                                end of the target document's URL.
uation.
                                                                    Focused systems are potentially more useful to
                                                                the user because of the reduced need to navigate (es-
1. Introduction
                                                                pecially in a long document or on a mobile device).
The Wikipedia free encyclopedia is the most popular             Anchor-to-BEP link discovery is also a harder (and
collaborative information repository on the web. It             more interesting) problem than anchor to document
continues to enjoy increasing popularity amongst                discovery because of the focused relationship be-
web users as well as amongst a diverse set of know-             tween the anchor context and the target document
ledge content editors [1]. Wikipedia documents are              BEP context. The current method of link discovery is
densely linked in the traditional way, from text anc-           based on the page name matching or similarity. A
hors in one document to a target document. Although             broad range of technologies, e.g. natural language
external links to other web pages outside the Wiki-             processing, data mining, machine learning, informa-
pedia also exist, the link structure within the Wikipe-         tion retrieval, information extraction and link discov-
dia is quite different from that of the Web. The use of         ery, are encouraged to integrate to resolve the issue
hyperlinks on the Web tends to vary, ranging from               of linking anchor to best entry points.
elaboration to referential to navigational. Text anc-
                                                                    After two years of INEX experiments it appeared
hors do not necessarily denote the concept of the
                                                                as though the problem of the file-to-file link discov-
target, and even if they do they often take the user to
                                                                ery was solved. Two fundamentally different ap-
a different but related web site.
                                                                proaches (anchor link analysis [3] and page name
   The Wikipedia link structure is typically built by           analysis [2]) could identify high quality links when
matching text anchors to semantically related entries.          evaluated against links already in the Wikipedia.
Most links within the Wikipedia have a strong se-               Near perfect precision scores at high recall levels
mantic relationship between the anchor context and              were seen.
the target context. The purpose of a Wikipedia link is
                                                                    However, after extensive manual assessment of
almost invariably to provide more detailed informa-
                                                                INEX runs it became clear that the use of the existing
tion about something. The majority of the links are
                                                                link graph lead to biased and optimistic evaluation
conceptual rather than navigational.
                                                                [7]. It appears as though the near perfect scores are
                         th                                     achieved because a substantial proportion of the links
Proceedings of the 14       Australian Document Computing
                                                                in the Wikipedia are in fact generated automatically
Symposium, Sydney, Australia, 4 December 2009.
Copyright for this article remains with the authors             using similar methods to those used by the link dis-




                                                             110

covery systems being evaluated. Manual assessment                                            Presented in Figure 1 is the relationship between
appears to be essential for robust evaluation of link                                    document size and the number of outgoing links.
quality.                                                                                 There are no very short documents with a high link
                                                                                         count and there are very few long documents with
                    There are other reasons for manually assessing
                                                                                         few links. Most documents link to between 10 and
link discovery systems at INEX:
                                                                                         100 different pages within the collection.
1. There appear to be many links in the Wikipedia
                                                                                             There are 24,168 homonym disambiguation pag-
                     that are not useful. Some links are inserted au-
                                                                                         es. These pages are not suitable for link discovery
                     tomatically and may not be considered relevant
                                                                                         experiments as they are (essentially) content-free.
                     by users of the Wikipedia (for instance, links to
                     year documents).
                                                                                         3. Related Work
2. The Wikipedia is largely linked from an anchor
                     to a whole document; best entry points are rarely                   The 2008 INEX Wikipedia collection [8] was con-
                     seen. It is, therefore, not possible to use the ex-                 verted from Wiki-markup into XML for XML-IR
                     isting Wikipedia link graph to evaluate anchor to                   experiments. The 2009 INEX collection was also
                     BEP link discovery systems.                                         converted into XML but for use in a broader set of
                                                                                         experiments. One point of difference between the
3. In the Wikipedia it is quite reasonable to expect
                                                                                         two collections is the semantic annotations present in
                     some anchors to target multiple destinations.
                                                                                         the 2009 collection (see Schenkel et al. [9]).
                     There could, for example, be a variety of themat-
                     ic links, multilingual links, or links which extend                     The Wikipedia has already been used as an IR
                     the anchor's context with varying degrees of                        corpus in several evaluation initiatives. Since INEX
                     complexity (simple vs. full Wikipedia). The ex-                     2006 it has been used for the evaluation of ad hoc
                     isting link graph does not support the evaluation                   XML retrieval and for XML Document Mining. At
                     of systems which support multiple links per anc-                    CLEF 2006, it was used for question answering [10].
                     hor discovery.
                                                                                         A link suggestion tool, Can We Link It, was devel-
                    The need for a robust and standardized manual                        oped by Jenkins [11]. It extracts a number of anchors
assessment and evaluation methodology is the moti-                                       which have not been discovered in an article and that
vation for this paper. We hope that this methodology                                     might be linked to other Wikipedia documents. Us-
will be adopted for link discovery experiments                                           ing this tool the user can add new anchors and cor-
beyond INEX 2009.                                                                        responding links back from a Wikipedia article. Mi-
                                                                                         halcea & Csomai present the Wikify [4] system. It
                                                                                         integrates automatic keyword extraction and word
2. Wikipedia
                                                                                         sense disambiguation to identify the important con-
There are more than 200 different language versions                                      cepts in a document and links these to corresponding
of the Wikipedia (September 2009). They are freely                                       documents in the Wikipedia.
available as a database and are particularly well
                                                                                             Link discovery systems are typically evaluated
suited to IR experiments.
                                                                                         against the Wikipedia itself. Pages are selected as IR
                    Between 2006 and 2008 INEX used a dump of                            topics, the algorithms are run over the topics, and the
the English Wikipedia consisting of 659,388 docu-                                        result compared to the links that are already in the
ments. For 2009 INEX has used a fresh dump con-                                          document. Mihalcea & Csomai [4] used the Turing
sisting of 2,666,192 documents. The documents were                                       test to further validate their results. Milne & Witten
converted from the original Wiki-markup to XML.                                          [5] used the Mechanical Turk to solicit links for the
                                                                                         AQUAINT collection. INEX considers link discov-
                                                                                         ery to be a recommender task and so the results list is
                                            337661                        0-1024

                    350000                                                1024-2048      ranked; set based evaluation is inappropriate.
                    300000                                                2048-5120
                                     205184
                                                                                             Two evaluation frameworks, DIRECT [12] at
                                                                          5120-10240
   st                250000
                                                  170797
     n                                                                    10240-51200
      e                                                                                  CLEF and EPAN [13] at NTCIR, provide a GUI and
                     200000
       m                        98395
                                                                          > 51200
        cu           150000
          o                                                                              modules for evaluation. INEX assesses all topics, and
           d
            f                                           56203
                     100000
             o                                                                           also uses a GUI evaluation tool for ad hoc retrieval.
                                                                        > 51200
              er                                                       10240-51200
                                                             3972
                b     50000
                                                                     5120-10240
                 m                                                  2048-5120
                          0
                  Nu                                               1024-2048
                                                                                         4. Experimental Methodology
                                                                 0-1024


                                                                  Pure text in           A subset of the Wikipedia collection is chosen as a
                                                                     bytes
                             Number of links
                                                                                         topic set. All anchor links to and from the topics,
                                                                                         from and to the collection, are removed (orphaning
  Figure 1: Relationship between document length and
                                                                                         the documents). Specifically, a random set of (6600
number of link in the INEX 2009 Wikipedia collection.
                                                                                         in 2008, 5000 in 2009) documents was chosen as the
                                                                                         topics for file-to-file linking; track participants no-




                                                                                      111

minated topics (50 in 2008, 33 in 2009) for anchor-        5.2 Assessment
to-BEP linking. The goal is to identify both outgoing
                                                           Built on experience using the INEX ad hoc assess-
and incoming links from and to those topics.
                                                           ment tool, a GUI-based relevance assessment tool
    INEX offers two linking tasks: file-to-file and        (i.e. GPXrai) was custom designed and built for the
anchor-to-BEP. The former is a low-cost entry-level        manual assessment of link discovery pools (see Fig-
task for new participants (and as a sanity check for       ure 2). The interface is comprised of a split screen.
the latter task). The task is to identify up to 250 doc-
uments that the topic should link to; no anchor or             The topic pane is located on the left hand side.

BEP need be identified.                                    The right hand pane is used to show the target docu-
                                                           ment. Two distinct assessment modes are provided,
    In the anchor-to-BEP task the system can identify      one for outgoing links, the other for incoming links.
up to 50 outgoing anchor texts per topic. For each
anchor at most 5 target document/BEP pairs are al-
lowed. For incoming link discovery, a set of at most
250 anchors (in the collection) targeting BEPs in the
                                                                   Status of anchor is pre-
topic are to be identified. Both incoming and out-                 sented by different color.

going links are from anchor to BEP.
                                                                         Ongoing

    A text anchor is identified by its position (offset                  Fully non-relevant

and length) within the document. A BEP is identified                     Completed

by its position. Positions are specified as character                    Current Selected

offsets (excluding markup) from the document start.
                                                                                                BEP Icon with relevant BG
                                                                                                color (Green)
    Participants were invited to submit runs. In total
30 runs were submitted in 2008. It was prior to the
2009 submission deadline at the time of writing.

                                                            Figure 2: INEX 2009 Link-the-Wiki Assessment Tool
5. Manual Assessment
                                                               Outgoing links are initially assessed. The topic
                                                           document is displayed with highlighted pool anchors.
5.1 Methodology
                                                           In the first instance the assessor goes through all the
In 2008 two sets of assessments were generated, one        anchors and rejects (a mouse right-click) those which
from the Wikipedia and the other from the runs.            are obviously irrelevant. The pool can contain many
                                                           such anchors, for instance year or other coincidental
    The Wikipedia ground-truth assessment set con-
                                                           links. Each link for each remaining anchor is then
sisted of just those links already in the Wikipedia. It
                                                           displayed, in turn, with the right pane showing the
is an automatically generated set of links from anc-
                                                           target document. The assessor then either rejects (a
hors to documents.
                                                           mouse right-click), or accepts the target as relevant
    Submission runs were pooled. The pooling               (by double-clicking to indicate the BEP, then mouse
process combines overlapping anchor texts to form a        left-click).
pool-anchor which is presented to the assessor. A
                                                               Incoming links are assessed in a similar manner,
pool-anchor might contain a number of anchors as
                                                           but the locations of the anchor and BEP are swapped.
well as a set of target BEP links. All the links already
                                                           Now the anchors are from other documents and the
in the Wikipedia topic were added to the pool. The
                                                           BEPs are inside the topic document. The assessor is
pool was then manually assessed.
                                                           required to accept or reject each prospective link.
    For the purpose of evaluation it is assumed that
                                                           In INEX 2008 the pools contained between 405 and
all non-assessed links are non-relevant. However, as
                                                           1722 links. Assessment logs suggest that between 4
the pool was exhaustively assessed, there is a reusa-
                                                           and 6 hours were required to assess a topic. On aver-
bility issue and does not affect submitted runs. We
                                                           age, only 7.4% of a pool was judged relevant.
note that the same convention is used in other forums
and tracks (such as TREC).
                                                           6. Evaluation
    A validation tool was provided and distributed to
assist developers of focused link discovery systems.       A portable (Java) evaluation tool, LtwEval, was de-
                                                           veloped for evaluation purposes. It is GUI based and
It allowed participants to view their submissions in
                                                           provides numerous evaluation metrics including:
an interface similar to that used by the assessors. The
tool helped participants debug their submissions (the      precision, recall, MAP, and precision@R. Different

calculation of BEP can be non-trivial), as well as         runs can be evaluated and compared to each other.

perform sanity checks on their algorithms.                 Precision/recall graphs can be generated for sets of
                                                           runs (see Figure 3). Anchor-to-BEP runs can be eva-




                                                        112

luated as either file-to-file or anchor-to-BEP. The           ad hoc track the BEP is considered to be subjective.
tool was distributed to participants.                         If the search engine can put the relevant passage on
                                                              the user's screen then it is considered a "hit". The
                                                              contribution of the links' BEP is a function of dis-
                                                              tance from the assessor's BEP:

                                                                             - 0.9 × (
,)
                                                               () =                               0  (
,)  
                                                                                 0.1              (
,) > 

                                                                  Where (
,) is the distance between submis-
                                                              sion BEP and result BEP in character. Therefore, the
                                                              score of () varies between 0.1 (i.e. d is greater
                                                              than n) and 1 (i.e. the submission and result BEPs are
                                                              exactly matched). The score of 0.1 is reserved for the
                                                              right target document with an indicated BEP not in
                                                              range of n. n typically is set up as 1000 (characters).
                                                              The score of a result in the results is then:
 Figure 3: INEX 2008 Link-the-Wiki Evaluation Tool
                                                                                                           
    The "best" metric to use for focused link discov-            = (()) ×                () × ()!
                                                                                           

ery evaluation is not obvious. As with all metrics, it                                              
is important to first define the use-case of the appli-
cation. The assumption at INEX is that link-
                                                                  Where m is the number of returned links for the
discovery is a recommendation tasks. The system
                                                              anchor and m is the number of relevant links for the
produces a ranked list of anchors and for each a set                        i
                                                              anchor in the assessments. As the result list is re-
of recommended target/BEP pairs. The user navi-
                                                              stricted to 5 targets per anchor m is capped at 5 for
gates a limited number of anchors and selects only a                                              i
                                                              evaluation. A perfect run can thus score a MAP of 1.
few to embed in the new document.

    A link discovery system might identify a very
                                                              7. Conclusion and Outlook
large number of possible links. The Wikipedia has a
page for each letter in the Latin alphabet and so each        Although it has appeared as though link discovery is
letter of each word might be linked. It also contains         a solved problem, manual assessment of participants
potentially overlapping links, for example there is a         runs at INEX 2008 showed that, in fact, it is not. The
page for world, a page for war, and a page for world          INEX result raises new questions about methodolo-
war. The user is expecting the system to identify             gies for link discovery evaluation, and in particular
relevant anchors and links, and to place these at the         focused link discovery systems.
top of the results list. The list should also be com-
                                                                  In this contribution we propose and describe a
prehensive because it is not clear that the document
                                                              new comprehensive methodology. This methodology
author can know a priori which links will be relevant
                                                              is based on manual assessment of link relevance. A
to a reader of the document. That is, link discovery
                                                              new metric is proposed to measure the performance
is a recall oriented task
                                                              of a run. Our methodology is being used for the IN-
    The Mean Average Precision based metrics are              EX 2009 Link-the-Wiki track.
very good at taking rank into account and are recall
                                                                  Our further work will focus on evaluation quality
oriented. They are also very well understood. A good
                                                              and on the efficiency of the manual assessment. This
metric for link discovery should, consequently, be
                                                              will be done using assessor surveys and interviews.
based on MAP. The difficulty is computing the re-
levance of a single result in the results list.                   We remain fascinated by the appalling perfor-
                                                              mance of the Wikipedia itself when evaluated against
    For the anchor The Theory of Relativity, an equal-
                                                              the manual assessments. It is our expectation that,
ly good anchor might be Relativity. For evaluation
                                                              once the methodology is stable, link discovery sys-
purposes it is assumed that if the target is relevant
                                                              tems will outperform human created hypertext links.
and the anchor overlaps a relevant anchor then the
anchor is relevant; f       (i) = 1. This is subtly differ-
                      anchor
ent from the world war problem above, different in            References
so far as the target must also be relevant. Of course,
                                                              [1] Alexa, The Web Information Company http://www.
this definition of relevant anchor aids in reusability.
                                                                   alexa.com/topsites.
                                                              [2] Geva, S., GPX: Ad-Hoc Queries and Automated Link
    The assessor might have assessed any number of
                                                                   Discovery in the Wikipedia, INEX 2007, pp. 404-416.
documents as relevant to the given anchor. If the
                                                              [3] Jenkinson, D., K.-C. Leung, A. Trotman, Wikisearch-
target of the anchor is in the list of relevant document
                                                                   ing and Wikilinking, INEX 2008, pp. 374-388.
then it is considered relevant; f (i) = 1. In the INEX
                                  doc




                                                           113

[4] Mihalcea, R., A. Csomai, Wikify!: linking documents     [10] WiQA: Question answering using Wikipedia (2006)
    to encyclopedic knowledge, CIKM 2007, pp. 233-242.           http://ilps.science.uva.nl/WiQA/index. html
[5] Milne, D., I.H. Witten, Learning to link with wikipe-   [11] Jenkins, N., Can We Link It, http://en.wikipedia.
    dia, CIKM 2008, pp. 509-518.                                 org/wiki/User:Nickj/Can_We_ Link_It.
[6] INEX     (2009)   http://www.inex.otago.ac.nz/tracks/   [12] Mitanura, T., Nyberg, E. Shima, H., Kato, T., Mori,
    wiki-link/wiki-link.asp.                                     T., Lin, C.Y., Song, R., Lin, C. J., Sakai, T., Ji, D.,
[7] W.C. Huang, Trotman, A., Geva, S (2009), The Im-             Kando, N., Overview of the NTCIR-7 ACLIA Taska:
    portance of Manual Assessment in Link Discovery,             Advanced      Cross-Language    Information    Access
    SIGIR 2009, pp. 698-699.                                     NTCIR-7, pp. 11-25.
[8] Denoyer, L., Gallinari, P. (2006) The Wikipedia XML     [13] Di Nunzio, G. M. and Ferro, N., DIRECT: A System
    Corpus, ACM SIGIR Forum, 40(1):64-69.                        for Evaluating Information Access Components of
[9] Schenkel, R., Suchanek, F. M., Kasneci, G. (2007)            Digital Libraries, ECDL 2005, pp. 483-484.
    YAWN: A Semantically Annotated Wikipedia XML
    Corpus, BTW 2007, pp. 277-291.




                                                         114

Web Indexing on a Diet: Template Removal with the Sandwich Algorithm

                              Tom Rowlands, Paul Thomas, and Stephen Wan

                                               CSIRO ICT Centre
               tom.rowlands@csiro.au, paul.thomas@csiro.au, stephen.wan@csiro.au


Abstract     Web pages contain both unique text, which       ing the document object model of the HTML docu-
we should include in indexes, and template text such as      ment. Tree comparison methods have been used to ex-
navigation strips and copyright notices which we may         amine similarities in HTML tag elements [8]. Simi-
want to discard. While algorithms exist for removing         larly, Wang et al. [9] look for tables specifying layout.
template text, most rely on first completing a crawl and     Visual blocks have been segmented using classification
then parsing each page. We present a cheap and effi-         approaches [7].
cient algorithm which does not parse HTML and which              In contrast to examining document structure, other
requires only a single pass of the document. We have         approaches simply examine page text and are thus
used two web corpora to investigate the performance of       cheaper to run.     Word-level features such as term
a retrieval system using our algorithm and have found        frequency and word position statistics have been
similar eectiveness with an index 9-54% smaller. Fur-        exploited to induce templates [2]. A similar approach
ther experiments using a marked-up corpus have shown         using  text   fragment   frequencies   is  explored   by
97% of desired lines are returned.                           Gibson et al. [3]. Our work is similarly non-structural
                                                             but does not require any statistical modelling.
Keywords      Web documents, information retrieval


1    Introduction                                            3    The sandwich algorithm

                                                             We investigate template detection and removal from
Retrieving information from within a web document is
                                                             the viewpoint of improving the efficiency of a web
made more difficult by the presence of template text.
                                                             search engine. As such, we start with the constraint
Such templates include, for example, the header and
                                                             that the solution must be able to operate as documents
footer information that sandwiches the real content of
                                                             are crawled.
the document. These are typically inserted automat-
                                                                 The algorithm is derived from the intuition that,
ically by HTML authoring tools and scripts that dy-
                                                             given the prevalence of HTML authoring tools and
namically generate HTML pages, in order to provide
                                                             website content management systems, documents in
a website with a consistent look-and-feel. Ideally, an
                                                             the same directory will likely share the same template.
information retrieval system would be able to discard
                                                             The template lines are detected by comparing the
such template material when it doesn't contribute to the
                                                             target file--line by line--with a sibling document
topic of a page.
                                                             in the directory, referred to here as a peer.       The
   In this paper, we treat template detection and re-
                                                             longest common subsequence (LCS) of lines is a
moval as a longest common subsequence (LCS) prob-
                                                             non-contiguous set of lines in common to a document
lem, giving an efficient solution. Our experiments with
                                                             and its peer. Our approach assumes all such lines are
the WT10g corpus and an enterprise data set demon-
                                                             from a template and can be discarded. The remaining
strate gains in efficiency with low complexity.
                                                             lines are considered indexable material and kept.     If
                                                             there are no other pages in the directory, and therefore
2    Related work
                                                             no candidate peers, no template removal is attempted.
Related work has been characterised as using either a            Our approach is global but reduces to a single pass.
local or a global approach [3]. A local approach exam-       That is, identification of the template is performed
ines a page in isolation to find the template material. In   per document, and template material is removed at
contrast, a global approach determines shared templates      the same time.     As a result, this approach can be
by examining two or more documents from a collection.        implemented in a crawler before material is stored. If
   Most approaches handle templates with a two-pass          the crawl is breadth-first, in most cases an appropriate
algorithm: the first pass identifies the template and the    peer will simply be the last page crawled.
second extracts it. Approaches to identifying the tem-           Different algorithms produce the LCS in O(n )     2

plate have included structural comparisons, often us-        to O(nlogn) time [5, 6], where n is the number
                                                             of lines in each document.       No HTML parsing is
Proceedings of the 14th Australasian Document Comput-
                                                             required;  the algorithm is entirely independent of
ing Symposium, Sydney, Australia, 4 December 2009.
Copyright for this article remains with the authors.




                                                          115

the markup language.         1     The algorithm can remove                                                   As-is          LCS removed
template text from "split" content, where template
                                                                          WT10g                              10.7 GB             9.0 (-17%)
material is injected in between portions of useful text.
                                                                                                         1.4×10 postings
                                                                                                                9          1.2×10  9  (-9%)
Implementation is straightforward and simpler than
                                                                            media                            12.3 GB             4.0 (-67%)
competing approaches, which makes template removal
                                                                                                         1.1×10 postings
                                                                                                                9          0.5×10 (-54%)
                                                                                                                                   9
an option where engineering resources are limited.

                                                                       Table 1: Corpus and index sizes for two corpora, before
4    Experiments
                                                                       and after processing.
Our early experiments consider two measures. First,
we examine the effectiveness and efficiency of a re-
                                                                                       1.0                           As-is
trieval system which employs the sandwich algorithm.                                                              

                                                                                                                     Templates removed
Second, a corpus with templates explicitly marked al-
                                                                                          0.8
lows us to investigate our algorithm's accuracy. (These                                                    


only provide a quick check of the algorithm's perfor-
                                                                                             0.6           

mance; in this first work we have not conducted an
                                                                                    AP
in-depth comparison with competing, more complex,                                               0.4         


techniques.)                                                                                                 


                                                                                                             
    To investigate the performance of a retrieval system                                           0.2
which incorporates the sandwich algorithm, we used                                                           


PADRE [4]--which implements a variant of BM25--                                                       0.0                            


                                                                                                         0    10   20     30    40
and two corpora.          The WT10g corpus, used by the
                                                                                                                     Query
TREC web track [1], includes about 1.7 million web
pages from a variety of hosts. Peers were found for
                                                                       Figure 1: AP scores for queries 451­500, processed
92% of pages. We used three sets of associated queries
                                                                       with and without templates in the index.
("topics").      Topics 451­500 (from TREC 2000) and
501­550 (from TREC 2001) are reverse engineered
from search engine query logs. Topics EP1­145, also                    media corpus represent a substantial savings. The fig-

from TREC 2001, concentrate on finding home pages.                     ures for WT10g are smaller than the 40­50% suggested

Since by removing navigation blocks we will remove a                   by Gibson et al., but the WT10g crawl is older than the

number of links to each site's entry page, performance                 one used there and the use of templates has been grow-

on this latter set of queries seems likely to degrade.                 ing since [3]. By insisting on exact string matches we

    The second corpus is in the media domain, and was                  are also conservative in identifying possible templates.

collected from a large, national media organisation's                      Although a substantial fraction of the index has been

website.     It comprises about 760,000 documents for                  removed, retrieval performance is unaffected. Figure 1

which peers were found for 98%. 88 queries were used                   illustrates the AP scores for each of topics 451­500: on

from a sample of the organisation's query log, with                    most queries there is no discernable change and overall

judgements by an author who was familiar with the                      there is no significant difference (Wilcoxon p > 0.99).

organisation.       A subset of this corpus has templates              Topics 501­550 and EP1­145, and the media set, are

explicitly marked.                                                     similar (p > 0.2, p > 0.5, and p > 0.4 respectively).

    In these experiments, which used a pre-existing                        A further question is:                     how accurate are we in
crawl, a page's peer was based on its name n:                     it   removing templates?                        We compared our output, line

was that page in the same directory whose name was                     by line, with a subset of the media corpus explicitly
closest to n. "Closest" was defined with respect to edit               marked by the organisation. Blank lines and content-

distance.                                                              less HTML (e.g.                        a sole <p> on a line) were not
    The first question we ask is: how much more effi-                  considered in the comparison. The precision and recall
cient can an index be if templates are removed? To our                 of lines classified as non-template material (and hence

knowledge, template removal approaches have not been                   kept) is 57% and 97% respectively, with an F1 score

examined by this measure. Table 1 summarises the size                  of 0.59. The algorithm is correctly keeping the great

of each corpus with and without processing; and the                    majority of interesting text, although our conservative

number of postings in an index of each.                                approach means we are also keeping a portion of

    Since a lot of templates are formatting or script-                 templates.

ing instructions, which will not be indexed anyway, the
savings in postings are less than the savings in corpus                5    Conclusions and Future Work
size--however even the smallest saving, 9% of postings
                                                                       Templates represent a substantial, though generally
for WT10g, seems worthwhile, and the figures for the
                                                                       uninformative, portion of text on the web. Removing
   1If the input is known to be, e.g., HTML or SGML then a tokeniser   templates leads to a reduction in index size, without
could be run first and the LCS computed over streams of tokens. We     a drop in query performance.                            Line-based LCS
have not yet pursued this idea.




                                                                    116

comparison provides a cheap method for template
detection and removal, allowing for easy integration
within a web crawler.       In future work, we intend to
use the sandwich algorithm with question answering
systems and automatic text summarisers, both of
which can benefit greatly with the accurate removal of
irrelevant template material.


Acknowledgements

We thank the anonymous reviewers for their feedback
and for their useful ideas.


References

[1] Peter Bailey, Nick Craswell and David Hawking. Engi-
    neering a multi-purpose test collection for web retrieval
    experiments.    Info Proc & Management, Volume 39,
    Number 6, pages 853­871, November 2003.

[2] Laing Chen, Shaozhi Ye and Xing Li. Template detection
    for large scale search engines. In Proc. ACM Symposium
    on Applied Computing, pages 1094­1098, 2006.

[3] David Gibson, Kunal Punera and Andrew Tomkins.
    The volume and evolution of web page templates.       In
    Proc. WWW, pages 830­839, 2005.

[4] David Hawking, Peter Bailey and Nick Craswell. Effi-
    cient and flexible search using text and metadata. Tech-
    nical Report 2000/83, CSIRO Mathematical and Infor-
    mation Sciences, 2000. http://es.csiro.au/pubs/
    hawking_tr00b.pdf.

[5] Daniel S. Hirschberg. Algorithms for the longest com-
    mon subsequence problem. J. ACM, Volume 24, Num-
    ber 4, pages 664­675, 1977.

[6] James W. Hunt and Thomas G. Szymanski.            A fast
    algorithm for computing longest common subsequences.
    Comm. ACM, Volume 20, Number 5, pages 350­353,
    1977.

[7] Ruihua Song, Haifeng Liu, Ji-Rong Wen and Wei-Ying
    Ma. Learning block importance models for web pages.
    In Proc. WWW, pages 203­211, 2004.

[8] Karane Vieira, Altigran S da Silva, Nick Pinto, Edleno S
    de Moura, Jo~ao M B Cavalcanti and Juliana Freire. A fast
    and robust method for web page template detection and
    removal. In Proc. CIKM, pages 258­267, 2006.

[9] Yu Wang, Bingxing Fang, Xueqi Cheng, Li Guo and
    Hongbo Xu. Incremental web page template detection.
    In Proc. WWW, pages 1247­1248, 2008.




                                                             117

             Analyzing Web Multimedia Query Reformulation Behavior


      Liang-Chun Jack Tseng                  Dian Tjondronegoro                         Amanda Spink

          Faculty of Science and                Faculty of Science and                 Faculty of Science and
               Technology                             Technology                            Technology
        Queensland University of              Queensland University of               Queensland University of
               Technology                             Technology                            Technology
      Brisbane, QLD 4001, Australia        Brisbane, QLD 4001, Australia           Brisbane, QLD 4001, Australia

        ntjack.au@hotmail.com                   dian@qut.edu.au                       ah.spink@qut.edu.au



Abstract       Current multimedia Web search engines       query than video and audio searches [18]. Therefore, it
still use keywords as the primary means to search.         is important to investigate user's multimedia query
Due to the richness in multimedia contents, general        formulation behavior in order to better understand the
users constantly experience some difficulties in           characteristics and obstacles in different types of
formulating textual queries that are representative        multimedia searches.
enough     for  their  needs.   As   a   result,  query
                                                               Existing studies have attempted to understand
reformulation becomes part of an inevitable process
                                                           user's information searching behavior and the search
in most multimedia searches. Previous Web query
                                                           trends from Web log analysis [7, 15, 19]. These
formulation     studies   did   not   investigate    the
                                                           studies have shown that users submit relatively short
modification sequences and thus can only report
                                                           search queries, typically around three terms per query
limited findings on the reformulation behavior. In this
                                                           [15]. Most users do not review many results, typically
study, we propose an automatic approach to examine
                                                           only the first result page [9, 11]. Such little contextual
multimedia query reformulation using large-scale
                                                           information and brief interaction between the user and
transaction logs. The key findings show that search
                                                           the search engine limited the understanding of user's
term replacement is the most dominant type of
                                                           searching behavior, especially when the analysis is
modifications in visual searches but less important in
                                                           based on individual transaction records [13, 16]. Thus,
audio searches. Image search users prefer the
                                                           it is necessary to investigate multiple queries in order
specified search strategy more than video and audio
                                                           to provide more contextual information for Web log
users. There is also a clear tendency to replace terms
                                                           analysis.
with synonyms or associated terms in visual queries.
The analysis of the search strategies in different types       The current study aims to discover multimedia
of multimedia searching provides some insights into        query reformulation behavior and search strategies by
user's searching behavior, which can contribute to the     applying novel log analysis procedures. To our
design of future query formulation assistance for          knowledge, this is the first study to automatically
keyword-based Web multimedia retrieval systems.            analyze     contextual    information     beyond      two
                                                           consecutive transaction logs. This approach also
Keywords       Web log analysis, multimedia search,
                                                           allows    us    to   compare     the   search     strategy
query reformulation, search strategy
                                                           characteristics among different types of multimedia
                                                           searches and provide insights for future system
1. Introduction                                            development.

The prevalence of multimedia information on the Web
has changed user's information need from textual to        2. Related studies
multi-modal (i.e. audio, image, and video) searching.
Multimedia search is more complex compared to
                                                           2.1 Limitations of current Web log
general Web searches as evidenced by the longer
                                                           analysis
session times and query lengths [11, 18]. Web
multimedia search users also perform many query            Web logs can be effectively used to understand
modifications, and have more difficulties in finding       general users' online searching behavior on a large
the appropriate terms to represent their needs. In         scale [6, 8, 9, 11, 15] and are generally more objective
addition, image search has the longest session length      and non-intrusive than other data collection methods
(i.e. more queries per session) [19] and more terms per    [6]. Such unique characteristics make Web log data
                                                           representative of user's unaltered behavior and thus
Proceedings of the 14th Australasian Document              regarded as the most convenient way to study real
Computing                                                  users [6]. However, the findings from individual
Symposium, Sydney, Australia, 4 December 2009.             transaction log are usually limited to the descriptive
Copyright for this article remains with the authors.       data without explanatory information about user's




                                                        118

searching behavior [16]. Recent studies have begun          Narayan [10] investigated query reformulation
extracting contextual information from consecutive          behavior among large-scale Web log data. Despite the
query modifications [5, 13, 14]. However, most              large proportion of formulating new search queries,
studies are limited in the amount of queries that can be    query    reformulation    (which    is   equivalent  to
investigated because of the need for manual reviewing       substitution in [4]) accounted for more than 15% of all
processes [12, 16, 20]. Other studies using large-scale     eight types of modifications, with specialization (i.e.
data only examine searching behavior based on two           addition) occurring more than twice of generalization
consecutive query modifications. Thus, the full             (i.e. deletion). They also concluded that major search
potential of contextual Web log analysis has yet to be      content transitions were between Web and image
discovered.                                                 collections.

    The analyses of contextual search information               Currently, only limited query modification studies
mainly focus on identifying new search sessions based       have investigated more than two consecutive queries
on query modifications between consecutive queries          to infer user's search strategies [16] or tactics [2].
[5, 10, 13]. He, Goker, and Harper [5] compared the         Rieh and Xie [16] manually investigated 313 sessions
effectiveness of using the time interval between two        of five modifications or more to classify the overall
clicks and query modification patterns in detecting         query reformulation approach into one of the eight
new search sessions. While the combination of these         distinct strategies, including: generalized, specified,
two methods produced the best results, query                dynamic, parallel, block-building, multi-tasking,
modification patterns accounted for the majority of the     recurrent, and format (details in Section 4.6).
improvements. Ozmutlu and Cavdur [13] applied this          Although they did not report the frequency for each
method to Excite search engine logs. Their findings         strategy, they concluded that the first four (i.e.
supported the usefulness of query modification              generalized, specified, dynamic, and parallel) are the
patterns. Query modification pattern and time interval      most popular strategies. A similar categorization of
also have a significant effect on judging topic shifts      search strategies can also be found in [2].
[14]. In the comparison with several Support Vector
Machine methods, the use of query modification
                                                            3. Research questions
pattern achieved at least 95% precision and recall in
topic continuation, and 35% or more in topic shift          Our focus is user's Web multimedia searching
cases, far better than its SVM counterparts. Similarly,     behavior which can be revealed by consecutive query
Lau and Horvitz [12] used query modification pattern        modifications. We investigate the entire session of
and time intervals between two consecutive queries to       user's query modifications to infer the searching
successfully predict user's upcoming search behavior        behavior. The three main questions that we attempt to
based on a Bayesian probability model. Query                answer are:
modification has also been used for studying the
                                                            1. What are the frequent modifications in Web
uptake and effectiveness of terminology feedback
                                                            multimedia queries and do they differ among the
provided by retrieval systems [1].
                                                            multimedia searches?

                                                            2. What can the sequence of query modifications tell
2.2 Web query reformulation behavior
                                                            us about user's query reformulation behavior?
and search strategies
                                                            3. What search strategies can be inferred from query
The term "modification" and "reformulation" have
                                                            modification sequences? How can they contribute to
been used interchangeably in many Web log analysis
                                                            the improvement of keyword-based Web multimedia
studies without explicit clarification of the differences
                                                            retrieval systems?
[10, 16]. In this study, we use "reformulation" to refer
to user's overall behavior of formulating different
versions of related queries in a session, whereas           4. Methodology
"modification" represents each change to the query.
Thus query modifications can be classified into certain     4.1 Dogpile log aggregation and query
patterns and the overall query reformulation behavior
                                                            modification records
implies user's search strategies.
                                                            Dogpile is one of the leading online meta-search
    Bruza and Dennis [4] investigated user's query
                                                            engines, which incorporates the indices of top search
reformulation behavior by manually classifying more
                                                            results from Google, Yahoo!, MSN Live, and
than one thousand queries into one of the eleven types
                                                            Ask.com. For this study, a total of 1,228,310 records
of query modifications. With the exception of the
                                                                                 th
                                                            taken on May 15 , 2006 have been used in our
repeating queries, term substitution was found to be
                                                            analysis. The original Dogpile transaction log contains
the most dominant type of query modifications,
                                                            five fields that we use for our analysis:
followed by term addition and deletion. A similar
finding was also reported from the study on a meta-         IP: the IP address of the computer submitting the
search engine Dogpile.com [10]. Jansen, Spink, and          query.




                                                         119

Cookie: the unique identifier which Dogpile system                       Deletion modification (D): current query omits some
sends to a particular computer with a pre-defined valid                  terms from the previous query
period.
                                                                         Replacement modification (R): deletion and addition
Time: the time of the day when user submits the                          of terms happen simultaneously to form the current
query.                                                                   query

Query: the original search text submitted to the                         Thus an initial query represents a new search topic
system.                                                                  since no search terms are carried over from previous
                                                                         query. Some studies also classify replacement
Vertical: the search type option which user selected
                                                                         modification as "reformulation" [5, 13, 14]. However,
on Dogpile's search page. In this study, we separate
                                                                         as users can freely reformulate the query by changing
the logs with "images", "video", and "audio" option
                                                                         the order of search terms without affecting the search
selected and replicate the analyses for comparison.
                                                                         results, we use the term "replacement" to clearly
                                                                         indicate    such    modification.   Details    of   the
4.2 Browsing record aggregation                                          classification algorithm can be found in [5]. We built a
                                                                         program to automatically classify queries by their
The Dogpile transaction logs are sorted based on
                                                                         modification patterns and aggregate consecutive
different user identification (i.e. a unique combination
                                                                         browsing records in Table 2.
of Internet Protocol (IP) and cookie) in a
chronological order. Consecutive transaction logs with
identical queries represent browsing records and are                     4.4 Session aggregation
aggregated with the foremost record. If the last record
                                                                         A search session is a series of related queries
has the same time stamp as the first record in current
                                                                         submitted by same user. In addition to being defined
browsing aggregation, the duration will be logged as
                                                                         by a unique combination of IP and cookies, a query
zero length (e.g. the last aggregation in Table 1 and
                                                                         with no terms in common with its preceding query is
Table 2). Table 1 and 2 illustrate the original log and
                                                                         regarded as the beginning of a new session, thus
aggregated record respectively.
                                                                         classified as the "initial query". By calculating the
     IP               Cookie         Time       Query         Vertical   number of sessions with same IP and cookie
                                                                         combination, we are able to identify the average
 64.105.73.70  2187RDPA47YLJOB   6:05:33 PM pod of dolphins   Images
                                                                         search topics submitted by a user.
 64.105.73.70  2187RDPA47YLJOB   6:06:03 PM group of dolphins Images

 64.105.73.70  2187RDPA47YLJOB   6:06:18 PM group of dolphins Images
                                                                         4.5 Modification sequence
 64.105.73.70  2187RDPA47YLJOB   6:06:18 PM group of dolphins Images


 64.105.73.70  2187RDPA47YLJOB   6:08:56 PM     dolphins      Images     Once the query modification records have been
 64.105.73.70  2187RDPA47YLJOB   6:08:56 PM     dolphins      Images
                                                                         generated, consecutive modifications within each
 64.105.73.70  2187RDPA47YLJOB   6:08:56 PM     dolphins      Images
                                                                         session can be classified into several predetermined
 Table 1. Original Dogpile search logs with browsing
                                                                         modification sequences. We used our program to
                             records
                                                                         identify the occurrence of thirty-six types of
 Session                                   Modification
            Current query   Modified terms                 Duration      modification sequences, incorporating two or three
  No.                                        pattern
                                                                         predetermined modifications. Sessions with less than
    1       pod of dolphins                      I          0:00:30
                                                                         two modifications are discarded as they provide little
    1      group of dolphins  pod,group         R           0:02:53
                                                                         information about user's behavior. The two­
              bottlenose        group
    1          dolphins      of,bottlenose      R           0:00:00      modification-sequences comprise one initial query (I),
  Table 2. Query modification table with aggregated                      followed by two query modifications which can be
                      modification records                               either of the replacement, addition, or deletion
                                                                         modification. Thus, nine patterns (3*3) can be
                                                                         formulated for the two-modification sequences.
4.3 Modification pattern classification
                                                                         Similarly,     twenty-seven    patterns    of    three-
Each aggregated transaction record is classified into a                  modification-sequences (3*3*3) can be formulated.
query modification pattern based on the content of the                   The main purpose of this analysis is to discover the
current query and the previous query. We use four                        frequent patterns of modification sequences that users
modification patterns for our classification. The                        follow,    thus   revealing   user's   preference   for
definitions for each modification pattern are:                           consecutive query modifications and providing in-
                                                                         depth information for search strategy analysis.
Initial query (I): current query has no terms in
common with the previous query
                                                                         4.6 Search strategies based on
Addition modification (A): current query contains all
                                                                         modification sequence analysis
search terms from the previous query, as well as some
new terms                                                                When typical modification sequences emerge from
                                                                         our analysis, we calculate the changes in the number




                                                                      120

of query terms within each sequence. Such changes        multimedia search whereas video is the least popular.
can determine if users adopt some particular search      As Table 4 shows, initial queries are the majority of
strategies. We construct our strategy classification     query modification across all multimedia searches.
based on the higher level categorization in [16]. The    Replacement modification is more than twice of the
list of modification strategies used in this study, as   addition modification in visual searches (i.e. image
well as the detail descriptions of our analysis          and video searches) but much less in audio searches.
assumptions are as follows:                              Deletion is the least type of modification in all
                                                         searches.
Generalized reformulation
                                                             Comparing         the    distribution    of   the    four
A user may begin with several search terms and
                                                         modifications in multimedia searches, audio search
subsequently drop some of the terms to include more
                                                         users are more likely to formulate new search topics as
results. This generalized reformulation is often
                                                         they have larger proportion of initial queries and more
manifested by consecutive term deletion changes [16].
                                                         topics per user than image and video searches. The
It can also be characterized by replacing the query
                                                         number of topics submitted by both image and audio
with fewer terms. Modification sequences in which
                                                         users varies a lot (SD=21.60 and 22.39 respectively)
subsequent queries always have fewer or equal terms
                                                         while video users shows a much uniformity pattern
to the precedent queries belongs to this category.
                                                         (SD=8.33). For the number of modifications, image
Specified reformulation                                  and video users have the same amount of
                                                         modifications (1.71 modifications on average) while
When a user persistently specifies a query by adding
                                                         audio users show slightly fewer modifications per
more terms or changing to more specific phrases, we
                                                         session (1.63 on average). Overall, image and video
classify this approach as specified reformulation. In
                                                         search users are very similar in terms of query
our analysis, modification sequences in which a
                                                         modifications.
subsequent query always has more or equal terms to
its preceding query belongs this category.                            Image    %    Video   %    Audio   %      Total
                                                          Log records 597,760 48.7 231,941 18.9 398,609 32.5  1,228,310
Dynamic reformulation                                     Sessions    183,825 52.9  52,405 15.1 110,945 32.0    347,175
                                                          Users        60,701 52.1  21,677 18.6  34,088 29.3    116,466
When a user inconsistently switches between
                                                          Table 3. Statistics of image, video, and audio search
generalized     and   specified    reformulation,  we
                                                                             logs in Dogpile dataset
characterize such approach as dynamic reformulation.
Such modification pattern manifests the unplanned                       Image     %      Video   %      Audio     %
nature of user's search process. Users who adopt this    Initial       183,825 58.6      52,405  58.5   110,945   61.2
search     strategy   generally    have     the   most   Replacement 82,292 26.2         22,225  24.8    33,645   18.6

unconsolidated search problems, and require more         Addition        30,716   9.8     8,817   9.8    22,553   12.4
                                                         Deletion        16,757   5.3     6,124   6.8    14,176    7.8
interaction with the retrieval system. Modification
                                                         Total         313,590 100.0     89,571 100.0   181,319 100.0
sequences in which subsequent queries can have either
                                                         Topics per user
fewer or more terms than precedent queries exhibit
                                                         Average            3.03            2.42            3.25
dynamic search strategy.
                                                         SD               21.60             8.33          22.39

Constant reformulation                                   Modifications per session
                                                         Average            1.71            1.71            1.63
Constant search occurs when a user modifies terms of
                                                         SD                 1.68            1.68            1.42
the same concept level which shares some common
characteristics, for example when substituting with          Table 4. Statistics of query modification records

related objects (e.g. from PC to Mac) or synonyms.
This strategy is characterized by having a constant      5.2 Modification sequence
number of query terms across the entire modification
sequence, regardless of the existence of replacement     Two-modification-sequence analysis

modifications. The same query specificity suggests a     The frequencies of each modification sequence pattern

one-to-one relationship between the original and new     (in percentages) are presented in Table 5 and 6. Table

terms. We used the term "constant reformulation" to      5 signifies the popularity of replacement modification

reflect this unique characteristic.                      in all types of multimedia searches, as evidenced by
                                                         the dominance of I-R-R and I-A-R sequences. On the
                                                         contrary, the unlikelihood of consecutive deletion
5. Results
                                                         modification is manifested by the low occurrence of I-
                                                         D-D sequences (i.e. less or equal to 1% in all
5.1 Query modification                                   multimedia searches).

From Table 3, image searches are the dominant type           Figure 1 shows that both image and video searches
of multimedia search in our dataset with more than       have prominently more I-R-R sequences than audio
50% of sessions and users attributed to image            searches. The audio searches have much more I-A-D
searches. Audio is the second popular type of            sequences than the other two types of searches, thus




                                                      121

making it more evenly distributed in the top three             multimedia searches and only one pattern contains the
modification sequence patterns. All multimedia                 deletion modification. When we further differentiate I-
searches show a similar distribution beyond the top            R-R sequence into I-R-R-R, I-R-R-A, and I-R-R-D
three patterns.                                                sequences, the prevalence of replacement over
                                                               addition and addition over deletion continued (I-R-R-
                 Image         Video              Audio        D not shown in Table 5). Hence, user's preference for

      I-R-R      41.2%         39.2%              26.8%        replacing terms and the unlikelihood of deletion
                                                               modification in the early stage of query modification
      I-A-R      24.9%         22.2%              23.6%
                                                               can be confirmed.
      I-A-D      10.9%         13.4%              21.9%

                                                                               Image          Video             Audio
      I-R-A      5.6%           5.3%               6.2%

                                                                                35.3%         32.4%             21.6%
      I-R-D      5.4%           6.8%               6.9%            I-R-R-R

                                                                                20.1%         17.3%             16.7%
      I-D-A      5.1%           6.2%               7.1%            I-A-R-R

      I-A-A      3.9%           3.3%               4.3%           I-A-D-A       5.3%           6.3%             11.1%

      I-D-A      2.3%           2.6%               2.1%            I-R-A-R      4.5%           4.1%              3.6%

      I-D-D      0.6%           1.0%               1.0%            I-R-R-A      3.9%           3.5%              2.6%
     Table 5. Comparison of the frequencies in two-
                                                                    Total       69.1%         63.7%             55.6%
             modification-sequence patterns
                                                               Table 6. Comparison of the top 5 frequencies in three-
    45%
                                                                           modification-sequence patterns
    40%

                                     Image Video Audio
                                                                 40%
    35%


    30%                                                          35%

                                                                                                    Image Video Audio
    25%
                                                                 30%

    20%
                                                                 25%

    15%

                                                                 20%
    10%

                                                                 15%
     5%


                                                                 10%
     0%

        IRR  IAR  IAD  IRA  IRD  IDA   IAA      IDA    IDD
                                                                  5%

  Figure 1. The distribution of the two-modification-
                                                                  0%
  sequence patterns among Image, Video, and Audio                      IRRR      IARR     IADA       IRAR         IRRA

                        searches
                                                                     Figure 2. The distribution of the top 5 three-
Three-modification-sequence analysis                           modification-sequence patterns among Image, Video,
     The dominance of replacement and addition                                   and Audio searches
modification continued in the analysis of three­
modification-sequences. As shown in the high
                                                               5.3 Search strategies based on
frequencies of both I-R-R-R and I-A-R-R sequences in
                                                               modification sequence
Table 6, about 50% of all three modification
sequences in image and video searches are associated           We investigated search strategies based on the
with     replacement   and    addition     modifications.      consecutive replacement sequences (i.e. the I-R-R and
Similarly to the distribution in two-modification-             I-R-R-R sequences) because of their prominence in the
sequence analysis, both image and video searches               modification sequence analysis. As Table 7 shows,
have much       higher proportion of consecutive               about 40% of all I-R-R sequences exhibit a dynamic
replacement modifications (i.e. I-R-R-R sequences)             search strategy. From Table 8, the proportion of
than audio searches. The top three modification                dynamic search increases to more than 50% in I-R-R-
sequence patterns distribute more evenly in audio              R sequences. While this large proportion of dynamic
searches with a slightly more I-A-D-A sequences than           search can be anticipated, constant search which
the other two types of searches. For modification              accounts for nearly one-third of all consecutive
sequence patterns beyond the top five, all multimedia          replacement sequences is more revealing. Because the
searches demonstrate similar distribution, thus provide        query length is held at constant within each session in
little information for characterizing different types of       constant searches, it appears to be a one-to-one
multimedia searches. Figure 2 shows that I-R-R-R               relationship between the replaced term pairs. A
sequences are more prominent in both image and                 reasonable explanation is the interchange of synonyms
video searches as the distribution decreased more in           or associated terms of the same construct (e.g. "PC" to
the top three modification sequence patterns than              "Mac", "UK" to "USA", or "girls" to "boys"). Both
audio searches. The top five patterns account for over         Table 7 and 8 show more specified searches than
half of the three-modification­sequence in all                 generalized searches, but the difference is only




                                                            122

noticeable in image searches. This indicates that                       45%

image users are more prone to adopt specified strategy                  40%
                                                                                                               Image Video  Audio
(i.e. gradually adding more search terms as the                         35%

searching progresses) than other types of multimedia                    30%

users. In other words, image users progressively                        25%

consolidate or learn more information about their                       20%

problems through the interaction with the Web search                    15%

engine. The percentage for the search strategy analysis
                                                                        10%

from I-R-R and I-R-R-R sequences are presented in
                                                                         5%

Table 7 and Table 8 respectively.
                                                                         0%

                                                                              Dynamic     Constant    Specified      Generalized

                           Image            Video          Audio
                                                                       Figure 3. Comparison of the search strategies from
     Dynamic               40.1%            40.6%          42.7%
                                                                                  I-R-R modification sequences
     Constant              34.8%            34.3%          28.5%
                                                                       60%

     Specified             15.2%            12.8%          15.2%
                                                                       50%
                                                                                                              Image  Video   Audio
    Generalized             9.9%            12.3%          13.5%
                                                                       40%
   Table 7. The percentage of each search strategy
                                                                       30%
             from I-R-R modification sequences
                                                                       20%


                                                                       10%

                           Image            Video          Audio
                                                                        0%

     Dynamic               52.9%            52.9%          58.3%              Dynamic     Constant    Specified        Generalized


     Constant              27.2%            25.7%          22.1%
                                                                       Figure 4. Comparison of the search strategies from
     Specified             11.9%            11.3%          10.1%                 I-R-R-R modification sequences

   Generalized              8.0%            10.1%           9.4%

 Table 8. The percentage of each search strategy from                6. Discussion and future work
               I-R-R-R modification sequences                        The statistics of query modification revealed that all
    As shown in Figure 3 and 4, both strategy analyses               multimedia search users shift their search topics more
from I-R-R and I-R-R-R sequences suggested the                       than refining their queries. Such phenomenon is most
highest constant search strategy in image searches.                  evident in audio searches as initial queries are more
Thus image searches require most synonym or related                  than triple of the replacement queries. The
term replacement modification than other types of                    replacement queries are more than twice the addition
multimedia searches, and such characteristic should                  queries in both image and video searches, whereas
benefit image searches more from term suggestion                     audio searches have notably more addition queries.
functionalities when refining the search queries. While              Deletion queries are the least type of modifications in
video searches have slightly less proportions of                     all multimedia searches, especially in image searches.
constant searches than in image searches, they shared                Overall, when users do modify their queries, they tend
very similar distribution across the four types of                   to replace their search terms rather than adding or
search strategies. On the other hand, audio search                   removing them. Such modification tendency is more
users are more prone to adopt a dynamic search                       prominent for visual searches (i.e. image and video
strategy.                                                            searches). Although the number of topics searched by
                                                                     one user varies a lot, users searched around two to
    In order to verify the replaced terms in constant
                                                                1    three different topics on average. In terms of in-
search sequences, we implemented a Brill tagger [3]
                                                                     session modification analysis, the majority of users
to identify the part-of-speech of the replaced term
                                                                     only perform little modifications to their queries and
pairs (i.e. the terms from the original query paired
                                                                     visual search users modify their queries slightly more
with the terms from the replacement query). Among
                                                                     than audio users.
the randomly selected 1465 constant I-R-R-R
modification sequences, a total of 3003 replaced term                    The analysis of modification sequence pattern
pairs have been successfully tagged using the Brill                  suggests the tendency to replace and add search terms
tagger. More than 70% of these term pairs (2125 in                   when modifying visual queries. The distribution of
total) have same part-of-speech, reassuring our                      modification sequences shows a tendency toward the
explanation of interchanging between synonyms or                     consecutive replacement modification sequences (i.e.
associated terms of same construct in these constant                 the I-R-R and I-R-R-R sequences) in visual searches.
search sequences.                                                    This tendency also distinguishes visual searches from
                                                                     audio search and suggests the need for interchanging
                                                                     related search terms. In other words, visual search
1
 Details on the tagger implementation can be found in [17].




                                                                  123

users are more willing to interact with the system than   The     prevalence    of    consecutive      replacement
audio search users.                                       modifications implies the need for an effective
                                                          relevance feedback mechanism that would help users
    In terms of search strategies, the changes in
                                                          refine the importance of their query terms, perhaps
number of terms within I-R-R and I-R-R-R sequences
                                                          with advanced search term suggestions based on the
reveal that about 40%-50% of users engage in
                                                          replaced terms (e.g. automatically displays synonyms
dynamic searches. This typically reflects the
                                                          or associated terms when user deletes a term). In terms
unplanned nature of Web multimedia searching, which
                                                          of search strategies, the current study confirms the
manifests the need for initiating several guessing runs
                                                          preference for the specified approach among image
to consolidate user's problem, or to find the
                                                          searchers. An interactive retrieval system that can
appropriate search terms. Nevertheless, about one
                                                          gradually obtain more information about user's image
third of users adopt constant search strategy in which
                                                          problem would be helpful in guiding the user to
they replace search terms with an equal number of
                                                          explore the entire collection, and hence improve the
terms, suggesting the high likelihood of interchanging
                                                          query reformulation effectiveness.
with synonyms or related terms. This constant search
strategy also differentiates visual searches from audio
searches. While visual searches always have higher        7. Limitations
proportion of constant searches, image users adopt
                                                          Due to the aim of using automatic approach to
most constant search strategy among all multimedia
                                                          discover user's query modification behavior, this
searches. Hence it can be assumed that image users
                                                          study only perform the part-of-speech analysis of
should benefit most from knowledge or ontology
                                                          replaced terms in constant search sequences. This
based query expansion or term suggestion assistance.
                                                          limits our understanding of the types of terms being
The reason of less constant search strategy in audio
                                                          modified during the reformulation process. However,
searches may be that audio searchers tend to use the
                                                          user's overall search strategy can still be inferred from
song title or singer's names in their queries [19],
                                                          our analysis. Although we have successfully
resulting the replacement of these proper nouns other
                                                          discovered some unique characteristics among
than interchanging similar terms in visual searches.
                                                          different types of multimedia searches, these findings
    When the change of terms shows a unidirectional       are yet to be compared with general Web searches to
pattern, all multimedia searchers are more prone to       address the differences in terms of query modification
adopt the specified approach. This finding is             behavior and search strategies.
consistent with prior study's conclusion on user's
primary concern of retrieval precisions [9]. In
                                                          8. Conclusion
particular, image search users show a stronger
preference for adopting this approach than other types    The current study investigated users' multimedia
of multimedia users. Typical scenario would be that       searching behavior based on their query modification
image search users need to see widely before they         methods. Our analysis showed that around 60% of
know exactly what they are searching for or how their     query modifications are to formulate new search
target images should look like. This characteristic       topics. Image and audio users searched more topics on
implies the importance of a browsing tool that helps      average than video users. Our approach to analyze
users compare different results and thus consolidate      Web multimedia query modifications went beyond
their problems quicker. A hierarchy arrangement of        two consecutive queries. The analysis of session
the results or term suggestions should also be useful.    modifications revealed that visual search users (i.e.
                                                          both image and video users) modify their queries
    Compared with general Web search studies, the
                                                          slightly more than audio users. Visual search users
current study findings are consistent with Jansen and
                                                          also tend to replace search terms with other related
Spink's [8] conclusion on the complexity of user's
                                                          terms rather than merely narrowing or broadening
Web search behavior as one-query session increased
                                                          their searches. Generally speaking, visual searches
over the years and users modify their queries less and
                                                          showed similar modification patterns with much more
less. This is to say that general Web users share the
                                                          consecutive replacement modifications than in audio
same characteristics with our user pool. Hence the
                                                          searches. In terms of search strategies, the relatively
effectiveness of the interaction between the user and
                                                          high proportion of constant search strategy in visual
the system is substantial to the improvement of query
                                                          searches indicates the importance of term suggestion
modification process. Future work should include a
                                                          assistance that helps user find the synonyms or related
user study to understand the reasons behind each
                                                          terms more easily. Our search strategy analysis also
modification, as well as the corresponding search
                                                          showed the tendency of adopting a specified approach
strategies. A semantic level analysis of replaced terms
                                                          in image searches, which suggests a need for query
would also help discover the aspects of multimedia
                                                          formulation assistance to help users gradually specify
content that users modify most, such as the visual
                                                          of their problems.
descriptors or the semantic meanings of the retrieved
objects.                                                      We present an automatic analysis procedure in this
                                                          study, thus maximizing the ability to apply the same




                                                       124

analysis to different data sets, as well as allowing             Information Technology (ITNG'07), pages 439-
comparisons with general Web user's searching                    444, 2007.
behavior. By adopting the analysis procedure, it is
possible to extract more information about user's           [11] B. J. Jansen, A. Spink, and J. Pedersen. An
query modification behavior, especially the search               analysis of multimedia searching on AltaVista.
strategies based on the statistical evidence. Future             In 5th ACM SIGMM International Workshop on
multimedia retrieval systems can utilize these different         Multimedia Information Retrieval, pages 186-
search characteristics to improve query formulation              192, 2003.
process and search efficiency.
                                                            [12] T. Lau and E. Horvitz. Patterns of Search:
                                                                 Analyzing and Modeling Web Query Refinement.
9. References
                                                                 In Proceedings      of  the  7th   international
[1] P. Anick. Using terminological feedback for web              conference on user modeling, pages 119-128,
    search    refinement:   a  log-based    study.    In         1999.
    Proceedings of the 26th annual international ACM
    SIGIR conference on Research and development in         [13] H. C. Ozmutlu and F. Cavdur. Application of
    information retrieval, pages 88-95, 2003.                    automatic topic identification on Excite Web
                                                                 search engine data logs. Information Processing
[2] M. J. Bates. Information search tactics. Journal of          & Management, Volume 41, pages 1243-1262,
    the American Society for Information Science,                2005.
    Volume 30, pages 205-214, 1979.
                                                            [14] S. Ozmutlu. Automatic new topic identification
[3] E. Brill. A simple rule-based part of speech tagger.         using multiple linear regression. Information
    In Proceedings of the workshop on Speech and                 Processing & Management, Volume 42, pages
    Natural Language, pages 112-116, 1992.                       934-950, 2006.

[4] P. D. Bruza and S. Dennis. Query Reformulation          [15] S. Ozmutlu, A. Spink, and H. C. Ozmutlu.
    on the Internet: Empirical Data and the                      Multimedia web searching trends: 1997-2001.
    Hyperindex Search Engine. In Proceedings of the              Information Processing & Management, Volume
    RIAO 97 Conference, pages 488-499, 1997.                     39, pages 611-621, 2003.

[5] D. He, A. Goker, and D. J. Harper. Combining            [16] S. Y. Rieh and H. Xie. Analysis of multiple
    evidence for automatic Web session identification.           query reformulations on the web: The interactive
    Information Processing & Management, Volume                  information    retrieval  context.  Information
    38, pages 727-742, 2002.                                     Processing & Management, Volume 42, pages
                                                                 751-768, 2006.
[6] B. J. Jansen. Search log analysis: What it is, what's
    been done, how to do it. Library & Information          [17] T. Simpson and T. Dao. WordNet-based
    Science Research, Volume 28, pages 407-432,                  semantic similarity measurement. The Code
    2006.                                                        Project.com, Oct. 1, 2005. [Online]. Available:
                                                                 http://www.codeproject.com/KB/string/semantic
[7] B. J. Jansen, A. Goodrum, and A. Spink. Searching            similaritywordnet.aspx. [Accessed: Jun. 10,
    for multimedia: analysis of audio, video and image           2009].
    Web queries. World Wide Web, Volume 3, pages
    249-254, 2000.                                          [18] A. Spink and B. J. Jansen. Searching multimedia
                                                                 federated content web collections. Online
[8] B. J. Jansen and A. Spink. An analysis of Web                Information Review, Volume 30, pages 485-495,
    searching by European AlltheWeb. com users.                  2006.
    Information Processing and Management, Volume
    41, pages 361-381, 2005.                                [19] D. Tjondronegoro, A. Spink, and B. J. Jansen. A
                                                                 study and comparison of multimedia Web
[9] B. J. Jansen and A. Spink. How are we searching              searching: 1997-2006. Journal of the American
    the World Wide Web? A comparison of nine                     Society for Information Science and Technology,
    search engine transaction logs. Information                  Volume 60, pages 1756-1768, 2009.
    Processing & Management, Volume 42, pages
    248-263, 2006.                                          [20] M. Zhang, B. J. Jansen, and A. Spink.
                                                                 Information Searching Tactics of Web Searchers.
[10] B. J. Jansen, A. Spink, and B. Narayan. Query               In 69th Annual Meeting of the American Society
     Modifications Patterns During Web Searching.                for Information Science and Technology, Austin,
     In    Fourth    International     Conference    on          USA, 2006.




                                                         125

        Term Clustering based on Lengths and Co-occurrences of Terms

                                   Michiko Yasukawa and Hidetoshi Yokoo
                                      Department of Computer Science
                                                Gunma University
                             1-5-1 Tenjin-cho, Kiryu, Gunma 376-8515, Japan
                                      {michi, yokoo}@cs.gunma-u.ac.jp


Abstract    Document clustering is useful for address-
ing vague queries and managing large volumes of docu-
ments. However, conventional algorithms for document
clustering do not consider the lengths of terms in the
cluster labels. Some cluster labels have considerably
different lengths. Cluster labels with different lengths
                                                            Figure 1: Comparison between clustering methods
result in wasted space on the screen. To counter this
problem, we have developed a new method for term
clustering. Our method considers both lengths and co-
occurrences of terms while clustering them. Therefore,
our method can achieve an efficient document search
even with limited area on the screen.

Keywords      Information Retrieval, Web Documents            Figure 2: Comparison between occupied areas

                                                          and Schvaneveldt[5], pairs of associated terms such
1    Introduction
                                                          as (BREAD-BUTTER) and (NURSE-DOCTOR) are
A single-term query is usually ambiguous, and it          more promptly recognized by users than pairs of
results in a large number of documents. Search result     unassociated terms such as (BREAD-DOCTOR) and
clustering is very effective in managing such a large     (NURSE-BUTTER). Hence, the proposed clusters are
number of searched documents[1][2][3].          We have   considered to be effective in the selection of preferable
developed a model for classifying a set of searched       terms on the display screen by users.
documents into clusters of related terms[4].         The      Further, users do not have to input each letter in
developed system was found to be useful for PC users      the terms when using related-terms clusters. Users may
but not for the users of mobile terminals.        This is simply select preferable terms on the screen. Moreover,
because the number of terms in each cluster label         users have an option of selecting a number on the screen
varies.  Further, the number of letters in each term      for accessibility; for example, they can push button "2"
varies. For example, the number of letters in cafe is     to indicate a set of terms "restaurant, sushi, tempura"
less than half the number of letters in restaurant. The   at once. As shown in Figure 2, clustering (b) can be
situation worsens when we use a proportional font to      more informative than clustering (a) because the results
represent the cluster labels.   In a proportional font,   of the former occupy a larger area on the screen.
the space required to represent the letter "w" is larger
than that required for "i," thereby resulting in wasted   2    Proposed Method
space on the screen (Figure 1 (a)). In order to make
optimal use of the limited space on mobile terminals,     Our proposed method is described as follows. We as-

we propose a new clustering method. Our proposed          sume that mobile users will enter a short query (typi-

method generates a set of related-term clusters that fit  cally just one term such as a location name) and will

in a rectangular region (Figure 1 (b)). The related-term  seek suggested terms in response to the query; The sys-

clusters are based on the co-occurrence of related terms  tem should present a well-organized menu of various

and are supposed to be intuitively better understood by   suggestions in response to the query, and the user will

users than randomized related terms. This is because      then select one of the suggestions in the menu as an ex-

co-occurrent terms in documents are supposed to be        panded requirement. After this, the system will present

terms associated with each other. According to Meyer      a number of web pages related to that expanded re-
                                                          quirement. The proposed method is explained in the
Proceedings of the 14th Australasian Document Comput-     following paragraphs.
ing Symposium, Sydney, Australia, 4 December 2009.            In our proposed clustering method, we first generate
Copyright for this article remains with the authors.      a set L(Q) of terms related to the short primary query




                                                         126

Q and determine the relationship between the elements       (Step 6) Determine the total length of each cluster to
in L(Q). Specifically, we denote the i-th term selected          decide whether to select or reject the cluster. If
from L(Q) as ti(Q). For example, for Q = "Shinjuku,"             the total length is adequate, select the cluster for a
t (Q) = "restaurant" may be a related term.
 i                                                  Next,        cluster label. If the total length is less than min,
we define a query consisting of Q and ti(Q) as qi =              reject the cluster. If the total length is greater than
Q, ti(Q). From the web pages that are searched by                
                                                                   max   , select terms from the cluster as many as pos-
q , we extract the adjacent terms of ti(Q).
 i                                               We call         sible until the total length is in the range between
these terms association terms of ti(Q). Let Ai(Q) be             
                                                                   min   and max.
the list of association terms of ti(Q). Note that Ai(Q)
may include another related term tj(Q). This is because     (Step 7) Remove the terms used for the cluster labels
the term tj(Q) = "sushi" may be adjacent to ti(Q) =              from the list L(Q). If L(Q) is empty or if no more
"restaurant" in the web pages of Q = "Shinjuku." In              cluster labels are generated, write out the cluster
                                                                 labels, and end the algorithm. Otherwise, return to
order to determine the relationship between the terms
t (Q) and tj(Q) with respect to the primary query Q,             Step 3 and continue.
 i
we define their co-occurrence score, scoreij, by
                                                            3   Implementation
    scoreij = (andij/orij)  (1 + log(andij)),         (1)
                                                            In order to measure the actual length of a term on the

where andij denotes the number of lists of association      screen, we use Graphviz1 and IPA font2.             With this
terms that include both ti(Q) and tj(Q) and orij            software and font, we can generate the text image of the

denotes the number of lists of association terms            term. Then, we measure the lengths of terms by using
that include either ti(Q) or tj(Q).     The equation is     the generated images.           In order to calculate scoreij,
defined empirically on the basis of our exploratory         we used a tool called GETA3 for large-scale text re-

experiments.     We have observed that in order to         trieval. We used Search API of Yahoo!JAPAN4 to col-

consider the co-occurrences of terms, the equation         lect search results of (1) related terms; (2) URLs, titles,

should amplify andij; however, the amplification must      and summaries; and (3) web pages. An actual appli-

not be excessive.                                          cation of the proposed method in a mobile web search

   In the algorithm, we set the minimum and maximum        system has been demonstrated in [6].

acceptable lengths per line of the display screen to min
and max, respectively.

 Algorithm--Rectangular Clustering

(Step 1) Read a list L(Q) of terms related to every
     query Q. Determine the length of each term in the
     list L(Q). Here, the length is the actual length of
     the term on the screen.

(Step 2) For every pair ti(Q) and tj(Q) of terms in
     L(Q), calculate scoreij using equation (1).

(Step 3) For every term ti(Q) in L(Q), select the two
     highest co-occurrence terms tk1(Q) and tk2(Q).
     Then, merge the selected terms to generate a prim-               Figure 3: Length of terms on the screen
     itive cluster ci = ti(Q), tk1(Q), tk2(Q). Note
     that terms may overlap in the primitive clusters.
                                                            4   Experiment
     Before proceeding to Step 4, calculate the score of
     c as the sum of scoreik1 and scoreik2 .                We compare the proposed algorithm with two other
      i
                                                            clustering      algorithms--        complete-link   clustering
(Step 4) Remove overlapping terms from clusters. If         (CLINK) and single-link clustering (SLINK). These
     there are overlapping terms among multiple clus-       algorithms are widely used conventional algorithms
     ters, retain only those terms that are in the cluster  and have been described in detail in [7].          While our
     with the highest co-occurrence score. Eliminate        algorithm considers both lengths and co-occurrences
     all terms that are repeated in other clusters.         of terms, these conventional algorithms consider only
                                                            co-occurrences of terms.
(Step 5) Determine the total length of each cluster to
     alter the cluster.  If the total length of a cluster
     is less than min, merge the cluster with another
                                                               1http://www.graphviz.org/
     cluster.  If two clusters ci and cj had common            2http://ossipedia.ipa.go.jp/ipafont/
     terms when they were primitive clusters, they can         3http://geta.ex.nii.ac.jp/e/
     be merged.                                                4http://developer.yahoo.co.jp/




                                                          127

                             Figure 4: Accessible web pages for different terms on screen


    The names of major places in Tokyo were used as        5     Conclusion
queries in the experiment. For each query, 100 related
                                                           We have proposed a new clustering method that en-
terms and 10,000 web pages were obtained. Term clus-
                                                           ables efficient term clustering in a mobile web search.
ters that fit in a rectangular region of 160 × 160 pixels
                                                           In the proposed method, a set of primitive clusters are
were generated using the 16-pixel proportional font. In
                                                           generated on the basis of the co-occurrences of terms.
the experiment, the parameters of CLINK and SLINK
                                                           Then, the clusters are altered on the basis of the co-
were adjusted to generate as many clusters as possible
                                                           occurrences and lengths of terms. Finally, the clusters
with each cluster having two or more terms.
                                                           are evaluated and adjusted on the basis of the lengths of
                                                           terms. Term clusters obtained by the proposed method
4.1    Area Occupied on Screen
                                                           effectively use a small rectangular region on the screen.
One of the key features of the proposed method is that     Hence, the clusters are informative and can aid mobile
it takes into consideration the term lengths, thereby op-  users to search documents efficiently. In the future, we
timizing the use of screen space. We investigated the      intend to apply the proposed method to various infor-
total length s of the clusters for each query and then     mation retrieval systems.
calculated the ratio of the total length s of the clusters
to the total length r of the lines in the rectangular re-  References
gion. In Figure 3, we can observe that the term clusters
                                                           [1] O. Zamir and O. Etzioni. Web document clustering: A
generated by using the proposed algorithm occupy a
                                                                feasibility demonstration. In SIGIR, pages 46­54, 1998.
larger area on the screen as compared to SLINK and
                                                           [2] D. Beeferman and A. L. Berger. Agglomerative cluster-
CLINK. Hence, the proposed algorithm is considered
                                                                ing of a search engine query log. In KDD, pages 407­416,
to provide more information than others.
                                                                2000.

4.2    Efficiency of Web Search                            [3] S. Osinski. Improving quality of search results clustering
                                                                with approximate matrix factorisations. In ECIR, pages
Another key feature of the proposed method is its high          167­178, 2006.
search efficiency.    In Figure 4, "AND" indicates the     [4] M. Yasukawa and H. Yokoo. Related terms clustering for
condition that the web pages include two or more terms          enhancing the comprehensibility of web search results. In
in the clusters, e.g., ((restaurant AND sushi) or (sushi        DEXA, pages 359­368, 2007.
AND tempura) or (tempura AND restaurant)). Further,        [5] D. E. Meyer and R. W. Schvaneveldt.        Facilitation in
"OR" indicates the condition that the web pages include         recognizing pairs of words: Evidence of a dependence
one or more terms in the clusters, e.g., (restaurant OR         between retrieval operations.   Journal of Experimental
sushi OR tempura). The proposed algorithm enables               Psychology, 90:227­234, 1971.
users to obtain desired pages more efficiently than con-   [6] M. Y. Yasukawa and H. Yokoo. Clustering search results
ventional algorithms.                                           for mobile terminals. In SIGIR, pages 880­880, 2008.
                                                           [7] C. D. Manning and H. Schutze. Foundations of Statistical
                                                                Natural Language Processing. The MIT PRESS, 1999.




                                                          128

 WriteProc: A Framework for Exploring Collaborative Writing Processes

                                    Vilaythong Southavilay, Kalina Yacef

                    School of Information Technologies, The University of Sydney
                                              NSW 2006, Australia

                              vstoto@it.usyd.edu.au, kalina@it.usyd.edu.au

                                                 Rafael A. Calvo

           School of Electrical and Information Engineering, The University of Sydney
                                              NSW 2006, Australia

                                              rafa@ee.usyd.edu.au


Abstract Collaboration and particularly collabora-         becomes a more social process in which students share
tive writing is an increasingly essential skill needed in  their works with each other". They also noted that when
the workplace and education. Until recently most of        using computers, students prefer to make revisions
the focus of research has been the final product of the    while producing, rather than after producing, text.
writing, rather than the process itself. In this paper,    Between initial and final drafts, students also tend to
we propose an innovative framework for investigating       make more revisions when they write with computers.
collaborative writing processes. The WriteProc frame-      In most cases, students also tend to produce longer
work utilizes both process and text mining tools to ana-   passages when writing with computers. In addition,
lyze the process that groups (or individual) writers fol-  review feedback, especially peer review, has been
low, and how the process correlates to the quality and     recognized as one effective way to learn writing [3, 4].
semantic features of the final product. Furthermore,       When students write with computers, they engage
WriteProc is integrated with existing web 2.0 writing      in the revising of their work throughout the writing
tools, providing full support for writing, reviewing and   process, more frequently share and receive feedback
collaboration. We describe the architecture that inte-     from their peers, and benefit from teacher input earlier
grates tools for analyzing the process and semantics of    in the writing process. Although these studies show
the writing. We also provide a case study on data col-     that computer-supported writing including automatic
lected from a group of undergraduate students writing      feedback tools efficiently assists students in writing
collaboratively an essay, with peer reviewing and use      and reviewing, understanding the writing process is
of an automatic feedback tool.                             crucial for developing support technologies for CW.
                                                              Over the past two decades, there has been abun-
Keywords Document workflows, web documents,
                                                           dant text-mining research for improving the support of
process mining
                                                           quality writing. But work such as automatic scoring
                                                           of essays [11], visualization [9], and document cluster-
1 Introduction
                                                           ing [1] focus on the final product, not on the writing
Computer-Supported Collaborative Work (CSCW),              process itself. Our vision is to investigatehow ideas and
particularly Collaborative Writing (CW), has received      concepts are developed during the process of writing
attention since computers have been used for word          could be used to improve not only the quality of the
processing. Due to the availability of the Internet,       documents but more importantly the writing skills of
people increasingly write collaboratively by sharing       those involved.
their documents in a number of ways.             Writing      Improving the process of writing requires
individually and collaboratively are considered            understanding how certain sequence patterns (i.e.
essential skills in most industries, academia, and         the steps a group of writers follow) lead to quality
government. This has led to increased research on how      outcomes. We see the sequence pattern as comprised
to support the production of better documents.             both of time events (as used in other process mining
    In Education, computer-supported writing has been      research) and of the semantics of the changes made
studied for decades. Goldberg et al. [6] collected a       during that step.
decade of empirical data and in a meta-study found            We combine here two techniques: process mining,
"that when students write on computers, writing            which focuses on extracting process-related knowledge
                                                           from event logs recorded by an informationsystem, and
Proceedings of the 14th Australasian Document Comput-      semantic analysis, which focuses on extracting knowl-
ing Symposium, Sydney, Australia, 4 December 2009.         edge about what the student wrote (or edited). The
Copyright for this article remains with the authors.




                                                        129

field of process mining covers many areas, like perfor-    use the reviewing tool and how review feedback affects
mance characteristics (e.g. throughput times), process     changes in reviewed documents.
discovery (discovery of the control flow), process con-        Process and semantic analysis tools are used in the
formance (checking if the event log conform specifica-     framework. Based on both the information (such as
tion), and social networks (e.g. cooperation) [2]. Par-    timestamp and writers' identification) of all revisions
ticularly, process mining analysis is necessary to under-  and event logs of reviewing activities, a process mining
stand group awareness, and writers' participation and      tool is used to discover sequence patterns of writing
coordination. Text mining combines indexing, cluster-      activities. The process analysis provides a way to ex-
ing, latent semantic analysis and other techniques stud-   tract knowledge about writers' interaction and cooper-
ied by the document computing community.                   ation. The analysis can identify interactions' patterns
    In this paper, a conceptual framework and tools for    that lead to a positive outcome and indicate patterns
supporting collaborative writing (CW) are introduced.      that may lead to problems. In addition, a text mining
Our framework is based on a taxonomy of collaborative      technique is performed to analyze text-based changes
writing proposed by Lowry et al. [8] and defines writing   of all revisions of documents. The text-based analyses
activities, strategies, work modes and roles involved in   can provide semantic meaning of changes in order to
CW. With this taxonomy, the framework incorporates         gain insight into how writers develop idea and concept
process mining and text mining technologies in order       during writing process.
to gain insight of collaborative writing process.
                                                           2.2 Implementation
    The remainder of the paper is organized as follows.
In Section 2, WriteProc, a framework for supporting
CW and the analysis of its process and semantics is
presented. A case study of process mining for a re-
viewing tool, Glosser is then presented in Section 3.
Finally, Section 4 provides discussion of our case study
and future work planned in this area.


2 WriteProc

Let us describe WriteProc, a framework for analyzing
individual and collaborative writing process. It consists
of three tools: writing, reviewing and analysis tools.
The analysis tool utilizes both process and text mining
techniques.
                                                           Figure 1: WriteProc: A framework supporting collabo-
    Our aim of developing WriteProc is to assist in-
                                                           rative writing.
dividual or groups of writers during the writing pro-
cess. Particularly, WriteProc can advise writers with          Based on the overall concept described above, the
reviewing feedback and visualization of the analyses of    framework utilizes process and text mining technolo-
writing activities and text changes during the process of  gies. It employs open-source utilities of those tech-
writing.                                                   niques to conduct analysis of writers' interaction and
                                                           text in order to assist writers in identifyingand realizing
2.1 Overall conceptual description
                                                           their writing process in collaborative manner. Figure 1

The framework integrates a front-end writing tool          shows the framework for supporting collaborative writ-

which not only supports collaborative writing              ing (CW).

activities, but also stores all revisions of documents
                                                           2.2.1 Writing environment: Google Docs
created, shared and edited by groups of writers. Each
revision of particular documents must contain all          In order to use a process and semantic analysis tool
needed information such as edited text, timestamp of       in real scenarios, the tool must be closely integrated
committing change, and identification of the writer.       to the writing environment. Tools such as Microsoft
In order to perform analysis of writing process for        Word or OpenOffice do not keep traces of the writing
particular documents, all revisions of the documents       process. Web 2.0 tools such as Google Docs (and the
are retrieved and traced.                                  incipient Microsoft Word Live) allow users to write on
    A reviewing tool is also embedded in the frame-        a web application (or offline and then synchronizing).
work. It assists writers in revising their own pieces      The service provider keeps the different versions of the
of writing and reviewing others works. After receiv-       document. Therefore, we selected Google Docs in our
ing feedback generated automatically by the reviewing      implementation of WriteProc.
tool, writers can edit and change their documents' con-        In WriteProc, Google Docs (GD) is used as a front-
tent accordingly. The tool keeps records of writers'       end writing tool of the CW. It is a web-basedutility with
reviewing activities in event logs. The event logs of the  most needed functionalities for word processing and it
tool are then extracted to gain an insight on how writers  allows users to share their documents with other team



                                                        130

members and to write synchronously. Users can access           Tool              Description
GD through their web browsers from anywhere and at             Home Tool         showing basic statistics such as
anytime they want. Each user needs a Gmail account to          (HOT)             numbers of words and revisions.
access the tool that they can obtain from Google free of       Topic Tool        checking if content provides evi-
charge.                                                        (TOT)             dence to support its topic senten-
    At the center of the framework is Google Docu-                               ces.
ment Lists Data API (GDAPI) used to integrate GD to            Flow Tool         reviewing coherence and checking
our CW system as shown in Figure 1 The API allows              (FLT)             how paragraphs and sentences fol-
WriteProc to retrieve and track all versions of docu-                            low from previous ones.
ments created, shared and edited among groups mem-             Keyword           showing semantic flow.
bers. In GD, each document created is uniquely as-             Tool - HTML
signed a document identification number. The GD also           (KTH)
keeps track of all version numbers of each document            Keyword           depicting the visualization of se-
by incrementing its version numbers each time the doc-         Tool - Graph      mantic flow.
ument is edited. Every time a writer makes changes             (KTG)
and edits a particular document, the identification of the     Group Tool        showing participation of authors
writer, the edited content of the document, timestamp          (GRT)             for different versions.
of committing changes and the version number of the
edited document can be retrieved and stored at the cen-                 Table 1: Reviewing tools of Glosser
tral relational database of CW system by using the API.
                                                             2.2.3 Process and text mining tools
This information extraction is executed seamlessly of-
fline and users as writers are not aware of it and are able  The interesting components of WriteProc are the pro-
to perform their writing tasks seamlessly. The API also      cess and text mining tools. The event log of Glosser
provides us the ability to build an interface to create and  is stored at the central relational database. The event
share documents in CW system. This can be very help-         log is used as a source to a process mining tool in or-
ful for instructors or supervisors to create and assign      der to gain an insight on writing activities and writ-
documents to groups of writers and reviewers without         ers' interaction. The process mining tool utilized in the
accessing GD. An appointed owner of a document can           WriteProc is ProM [15]. In the next section, ProM
edit it, where as an assigned 'viewer' can only review       will be used to demonstrate a process mining technique
it.                                                          for our case study. In addition, an independent mea-
                                                             sure is developed to analyze the changes in each ver-
2.2.2 Reviewing tool: Glosser                                sion of the documents in order to understand the na-
                                                             ture of changes and the level of these changes. The
Glosser is a web-based application providing support
                                                             analysis uses a text mining technique to find semantics
for writing in English [16]. It was designed and im-
                                                             changes among all versions of documents. This tech-
plemented to support a review feedback model. Figure
                                                             nique uses information from all the versions of docu-
2 shows such a model. Glosser assists users to revise
                                                             ments performed by groups of writers. The text infor-
their own document and review other documents. It
                                                             mation of each version stored in the central database
has the analysis and revision tracking system used for
                                                             is indexed using Lucene [7] so that text produced by a
reviewing. Writers can use Glosser in order to gain
                                                             group of writers can be systematically searched, sorted,
insight into their essays' structure and coherence. To re-
                                                             filtered, and highlighted. After indexing all versions of
view particular documents, the system consists of sev-
                                                             documents, the system then analyzes the relationship
eral functionalities, as shown in Table 1:
                                                             between them and their terms using Text Mining Li-
                                                             brary (TML) in order to produce a set of concepts and
                                                             nature of text changes in all versions of the documents.


                                                             3 Case study

                                                             As a way of evaluating the architecture and implemen-

         Figure 2: Automated writing feedback.               tation of WriteProc and illustrating how it can be used,
                                                             we discuss a case study where the tool is used to study
    In the case study described in Section 3, students
                                                             writing processes in a software engineering unit con-
used a reviewing tool, Glosser [16]. A document cre-
                                                             ducted during the first semester of 2009 at the Uni-
ated and shared among a group of writers can be re-
                                                             versity of Sydney. It is important to note that Human
viewed in Glosser, which also accesses each revision
                                                             Research Ethics Clearance has been completely granted
using the Google Document Lists Data API. Users can
                                                             fromthe universityfor this study. All students involving
access Google Docs from Glosser or vice versa.
                                                             in the study signed an informed consent.
                                                                 There were 58 students in the course, which
                                                             was E-business Analysis and Design.           They were



                                                          131

organized in groups of two and asked to write Project
Specification Documents (PSD) for their proposed
e-business projects. Each group had to submit one
PSD of between 1,500 and 2,000 words (equivalent
to 4-5 pages). Students were required to write their
PSD on Google Docs and share the documents with
the course instructor. They were asked to submit their
PSD using Glosser, a reviewing tool mentioned in
Section 2.2.2. The submitted PSD was reviewed by
other two students who were members of different
groups. Students had one week to review each others'
documents and submit their feedback. After getting
feedback on their documents from their peers, students
could revise and improve their writing if necessary
before submitting the final version one week later.
The submission of the final version of PSD also used
Glosser. The total event log file of the system consisted
of usage data of Google Docs and Glosser for three
weeks. In addition to this log file, the marks of the
final submissions of the PSD together with a very good
understanding of the quality of each group through
the semester was used to correlate behaviour patterns
to quality outcomes. In particular, to be able to give
insight into how students used the reviewing tool for
revising their own documents and reviewing others and
to give recommendation to improve the system, we
performed a process diagnostics method to give a broad
overview of students' interaction and collaboration.

3.1 Log Preparation

Unlike data preprocessing for workflow mining [5], our     Figure 3: Comparing number of events of 29 docu-
approach used a data preprocessing method for behav-       ments ranked by their final marks.
ior pattern mining [12]. This method was used with a
                                                           documents, representedby the length of the bar (DocXX
process mining tool like ProM [15]. Glosser's event log
                                                           denotes the document of Group XX). The documents
was a typical Web server log which was a text file. The
                                                           are ranked based on their final mark ranging from 4/10
first step of data preprocessing was to filter and clean
                                                           to 10/10. For example, Doc07, Doc08, Doc09, Doc10,
up the data. The next step of data preprocessing was to
                                                           Doc14, Doc17, Doc21, Doc27 and Doc29 all obtained
define process instances (cases). Our approached used
                                                           the highest mark, i.e. 10/10, while Doc11 obtained
a document as a notion of process instance. We utilized
                                                           the lowest mark of 4/10. On average there are 161
the concept of perspective,proposedby Song et al. [13]
                                                           events per document. The maximum number of events
to partition event sequences. Our perspective of the
                                                           is 369, with Doc12. Doc10 has the smallest number of
event data log was based on documents. Particularly,
                                                           45 events associated with it.
we wanted to find out how users interact and coordinate
                                                              Based on the number of events presented in
for writing and reviewing documents. The final step
                                                           Figure 3, we could not distinguish the better from the
in data preparation was to transform the log file to a
                                                           weaker groups. Although Group 12 has the maximum
standard format for process mining. Process mining
                                                           number of interaction events, it was ranked in the
tools such as ProM use MXML (as in Mining XML)
                                                           6th place. In contrast, the document of Group 10
files as sources [15]. The transformed MXML file was
                                                           with the least number of interactions was given the
then used as a source for a process mining tool like
                                                           highest mark. In addition, simple statistics drawn from
ProM.
                                                           the figure could not clearly provide understanding of
                                                           students' interaction. Therefore, further analysis was
3.2 Log inspection
                                                           made in order to distinguish group performance and
After preprocessing, the resulting event log consisted     cooperation. We will describe it next.
of 29 documents with a total of 4,677 events. Each
process case represented one document. There were          3.3 Historicalsnapshot ofreviewing activ-
8 different types of events (Section 3.4 described the            ities
process model and event types). The bar chart of Fig-      The Dotted Chart Analysis utility of ProM [10] was
ure 3 shows the number of events for each of the 29        used to analyze students' reviewing activities. The dot-



                                                        132

Figure 4: Dotted chart of 29 reviewed documents ordered by their first events' timestamps (from ProM tool [10]).
Grey denoted events generated by by authors; white by reviewers, black by reviewers' group member (indicated
by ovals) and brown by others (indicated by rectangles).


ted chart is similar to a Gantt chart [14], showing the     before submitting for peer review. Nevertheless, these
spread of events over time by plotting a dot for each       seven documents received high marks in the final as-
event in the log. Figure 4 illustrates the output of        sessment.
the dotted chart analysis of students' interaction for re-     In addition, we observed that all activities of peer
viewing their PSD documents. All instances (one per         review happened in the second week before the sub-
document) are sorted by start time (the first event ever    mission of feedback. Most of the reviewing activities
happening for a particular document during the three-       were performed by the assigned reviewers as indicated
week usage of the system). As shown in the figure,          by white dotted events. This met the intention of the
there are three important dates due to the three compul-    course of using Glosser for peer review. There are two
sory submissions: PSD for peer review on 27th March         interesting types of events in Figure 4. Firstly, 4 doc-
2009; feedback of peer review on 3rd April 2009, and        uments have events originated by students who were
final PSD on 10th April 2009. In the figure, points         not the authors nor the assigned reviewers, as can be
represent events occurring at certain time. For particu-    seen by black dots of documents of groups: 9 (received
lar documents, different color denotes events generated     a mark of 10/10); 26 (9/10); and 1, 3 (8/10). Those
by different roles of users: grey events generated by       events suggest that students either assisted their team
authors, white events by reviewers assigned for peer re-    members to review their assigned documents or per-
view, black events (circled in the figure) by team mem-     formed the peer review task together with their group
bers of assigned reviewers, and brown events (shown         members sitting side-by-side using only one account.
in rectangles) by non-author users who were neither         We discussed this matter with the course instructor who
assigned reviewers nor assigned reviewers' team mem-        was also aware of this problem and will try to find a
bers.                                                       solution to prevent this problem happening in the next
    We can clearly see from the figure that 22 docu-        semester. Secondly, there are a small number of events
ments have been revised using Glosser in the first week     where students reviewed others' documentswhich were
before the submission for peer review. Obviously, those     not assigned to them nor to their team members for peer
documents were only used in the system by the authors       review, as indicated by brown dotted events for docu-
as indicated by grey events. There were 7 documents         ments of groups: 3, 13, 22, and 26. These documents
starting in the second week. They belonged to groups:       received good marks ranging from 8/10 to 9/10. We
7, 9, 10 (received the same marks of 10/10); 15, 19, 25     believe this happened when students shared their own
(9/10); and 3 (8/10). This means that these documents       PSD to their friends to assist them using Glosser. We
have never been revised by their authors using Glosser




                                                         133

will perform further investigation to prevent this case        We were naturally interested in finding out more
from happening next year.                                  about individual group activity and the path each group
    In addition, from Figure 4 we can notice that eight    was following in this process. ProM provides a Per-
different documents were not revised by their authors      formance Sequence Analysis plug-in to find the most
using Glosser before the final submission. These docu-     frequent paths in the event log [2]. Figure 6 illus-
ments were 8, 9 (10/10); 15, 16, 19, 25 (9/10); 3 (8/10);  trates the interaction for documents of two groupsin the
and 5 (5/10). Except document of group 5, all doc-         course, with Group 1 (received a mark of 8/10) at the
uments received the top three highest marks. In fact,      top and Group29 (10/10)at the bottom. All eight events
three of them (9, 15 and 25) have never been revised by    represented on horizontal axis are according to events
their authors using Glosser at all. This implies that the  discovered by the process model mentioned above. We
better groups used feedback received from peer review      examined sequence patterns for all documents of 29
and the instructor as main source for revising their PSD.  groups. We discovered that only one document (doc10)
They did not spent muchtime using Glosser for revising     was not used with all reviewing tools. In fact, the au-
their own documents. It is also interesting to note that   thors and reviewers of the document only utilized the
reviewing activities did not evenly spread out for the     HOT tool.
three-week period of running the system. In fact, the
system has only been used extensively for peer review
in the second week as we can see in the figure. There
were not many interactions in the third week. However,
a number of activities happened a few days before the
final submission.
    To sum up, the dotted chart tool in ProM allows
us to analyze reviewing activities in order to seek in-
formation on how each of 29 documents was reviewed
by groups of students with different roles. We further
investigatedpatterns of students' interaction for review-
ing those documents, as described in the next subsec-
tion.                                                      Figure 6: Sequence analysis of documents of Group 1
                                                           (above) and Group 29.
3.4 Process discovery and sequence
                                                               We have also used the same plug-in to extract all
       analysis
                                                           reviewing interactions for each document. This further
From the event log of our case study data, we extracted    investigation gives an insight on how different users
the process model shown in Figure 5, which represents      reviewed documents using Glosser. For instance, Fig-
the process common to all the groups. Groups began         ure 7 depicts the users' interactions of two documents
with events of opening a particular document (ROD).        (doc01 on the right and doc29 on the left). Each col-
Then, the reviewing tool was requested (TOR). After        umn represents a user, where G29-1 is user 1 of Group
that, different reviewing activities were performed and    29 and so on. We analyzed all documents and found
the resulting feedbacks were displayed. The process re-    that more than half of them were revised by only one
iterated until users logged off or closed their browsers.  author using Glosser. In other words, although students
As discussed in Section 2.2.2, the reviewing activities    worked in groups, only one member actually performed
involve these tools: HOT, FLT, KTG, GRT, TOT, and          the reviewing task using the system.
KTH.




                                                           Figure 7: Users' interaction of Group 1 (right) and
                                                           Group29.

                                                               In this section, we illustrated the potentialof process
                                                           mining techniques in understanding how writers react

Figure 5: Process model of usage of the reviewing tool,    to peer review feedback and to the use of an automatic
Glosser.                                                   feedback tool like Glosser. In the log preparation, our



                                                        134

notion of process instance is based on documents be-         actually outlining the documents (instead of drafting),
cause we would like to analyze user interactions on a        it will provide information about their writing activities
same document. Dotted chart and sequence analyses            as feedback to the group. In this case, the writers
were used to gain insights on how students reviewed          can either adjust and modify their writing process
their documents.                                             specification, or investigate and change their written
                                                             content according to the feedback given by the system.

4 Discussion and conclusion                                      In conclusion, we contribute here the description of
                                                             WriteProc a framework that combines process and text
The work described here is a work in progress. While
                                                             mining techniques. The architecture of the system is
the case study presented here illustrates how our frame-
                                                             described together with its integration to Google Docs
work, WriteProc can be used, the data we used did not
                                                             as an environment for users to do the actual writing,
allow us to discover sequence patterns correlated to bet-
                                                             and to the Google API that allows the tool to collect the
ter outcomes. Although a pattern of users' interaction
                                                             revision information. A case study with a real teaching
can be extracted for a particular group, there are differ-
                                                             scenario is described and used to show how the tool can
ent patterns for different groups. Indeed, we could not
                                                             be used to analyze the process component of a collabo-
draw a significant pattern among groups in order to dis-
                                                             rative writing task.
tinguish the better from the weaker groups. However,
                                                             Acknowledgements This project has been funded
this gave us direction for the next step of our work.
                                                             by Australian Research Council DP0986873.               The
One way to improve our understanding of what writ-
                                                             authors would like to thank Jorge Villalon and Stephen
ing processes lead to better outcomes so software tools
                                                             O'Rourke for their help with Glosser and TML.
can be used to provide advice during the writing pro-
cess, is to use text mining techniques. For collaborative
writing, we would like to have insights on how each          References
version of documents changes in order to understand
                                                              [1] N. O. Andrews and E. A. Fox. Recent Developments
the writing process of each document. Although we                 in Document Clustering. Technical Report TR-07-35,
are able to track all versions of documents that were             Computer Science, Virginia Tech, 2007.
reviewed in the system, this tracking analysis does not
                                                              [2] M. Bozkaya, J. Gabriel and J. M. van der Werf. Process
yet give us meaningful insights about the purpose of the
                                                                  diagnostics: A mothod based on process mining. In
text changes between each version. One possibility to             International Conference on Information, Process, and
systematically capture and interpret writing activities in        Knowledge Management, February 2009.
collaborative writing is to understand changes in text
                                                              [3] P. A. Carlson and F. C. Berry.        Using computer-
written in each version of the document. Currently,
                                                                  mediated peer review in an engineering design course.
we are working on extracting changes in concepts and              IEEE Transactions on Professional Communication,
ideas during the writing of documents. The text mining            Volume 51, Number 3, pages 264­279, Sept. 2008.
algorithms use vector representations of the documents
                                                              [4] K. Cho and C.D. Schunn.         Scaffolded writing and
accounting for the temporal nature of the data and the            rewriting in the discipline: A web-based reciprocal peer
character of writing interaction. The result of the text          review system. Computers & Education, Volume 48,
mining tool will be analyzed and combined with the                Number 3, pages 409­426, 2007.
outcome of process mining (like the one described in
                                                              [5] C. A. Ellis, K. Kim and A. J. Rembert. Workflow
the current case study).                                          mining: Definition, techniques, and future directions.
    Based on the process mining tool illustrated                  Workflow Handbook, pages 213­226, 2006.
in the case study and text mining techniques as
                                                              [6] A. Goldberg, M. Russell and A. Cook.         The effect
described above, we are developing WriteProc to
                                                                  of computers on student writing: A meta-analysis of
provide visualization depicting users' interaction and            studies from 1992 to 2002. Journal of Technology,
collaboration in order to support writing activities. For         Learning, and Assessment, Volume 2, 2003.
example, a user interface can be built to assist a group
                                                              [7] E. Hatcher and O. Gospodnetic.       Lucene in Action.
of writers in identifying a plan for their writing process.       Manning Publications Co., 2004.
This plan is created at the beginning of writing process
                                                              [8] P. B. Lowry, A. Curtis and M. R. Lowry. Building a
representing a master plan of all writing activities and
                                                                  taxonomy and nomenclature of collaborative writing to
tasks. At particular point in time, writers can specify
                                                                  improve interdisciplinary research and practice. Journal
which stage they are on their writing process. During a           of Business Communication, Volume 41, pages 66­99,
time of writing, the system monitors if current writing           2003.
activities are according to the writer's specification.
                                                              [9] S. O'Rourke and R. A. Calvo. Semantic visualisations
For instance, a leader of a group of writers assigns              for academic writing support. In Vania Dimitrova, Ri-
all writing tasks to his or her members. The group                ichiro Mizoguchi, Benedict du Boulay and Art Graesser
leader specifies that the group is currently drafting its         (editors), 14th Conference on Artificial Intelligence in
documents. WriteProc will track the group's writing               Education, pages 173­180. IOS Press, July 2009.
activities and perform semantic analysis of the written      [10] ProM.     Version 5.2.    http://prom.win.tue.nl/
texts. If it finds out, for example, that the members are         tools/prom/, 2009.



                                                          135

[11] M.D. Shermis and J. Burstein. Automated essay scor-
    ing: A cross-disciplinary perspective, Volume 16. MIT
    Press, 2003.

[12] J. Song, T. Luo and S. Chen. Behavior pattern mining:
    Apply process mining technology to common event
    logs of information systems. In IEEE International
    Conference on Networking, Sensing and Control, Sanya,
    April 2008.

[13] J. Song, T. Luo, S. Chen and Feng Gao. The data
    preprocessing of behavior pattern discovering in col-
    laboration environment. In IEEE/WIC/ACM Interna-
    tional Conferrence on Web Intellengence and Intenllent
    Agent Technology, pages 521­525, Silicon Valley, USA,
    November 2007.

[14] M. Song and W. M. P. van der Aalst. Supporting process
    mining by showing events at a glance. In 7th Annual
    Workshop on Information Technologies and Systems,
    pages 139­145, 2007.

[15] B. F. van Dongen, H.M.W. Verbeek A. K. A. de
    Medeiros, A. J. M. M. Weijsters and W. M. P. van der
    Aalst. The ProM framework: A new era in process min-
    ing tool support. Lecture Notes in Computer Science:
    Application and Theory of Petri Nets, pages 444­454,
    2005.

[16] J. Villalon, P. Kearney, R.A. Calvo and P. Reimann.
    Glosser: Enhanced feedback for student writing tasks.
    In The 8th IEEE International Conference on Advanced
    Learning Technologies, Santander, Spain, July 2008.




                                                           136

                 An Analysis of Lyrics Questions on ahoo! Answers:
                    Implications for Lyric / Music Retrieval Systems

                                Sally Jo Cunningham, Simon Laing

                                   Computer Science Department
                                          University of Waikato
                                    Hamilton 3240 New Zealand

                                {sallyjo, simonl} @cs.waikato.ac.nz

Abstract This paper analyzes 237 questions posted           analyze a set of lyrics related questions posted on
to Yahoo! Answers, a popular community-driven               Yahoo! Answers, an open Web-based question and
question and answer service. The questions are all          answer forum. Once this understanding emerges of
natural language and are self-categorized by their          what lyrics seeking behavior `in the wild' (that is,
poster as being related to music lyrics, and as such        outside the constraints of a retrieval system, and as
they provide a rich context for understanding lyrics-       expressed in natural language) then we can identify
related information behavior outside the constraints        remaining problems in supporting lyrics retrieval.
imposed by specific lyrics retrieval systems. We
categorize the details provided in the queries by the
                                                            2 Previous work
types of music information need and the types of
music details provided, and consider the implications           At present music retrieval research is only lightly
of these findings for the design of music/lyric             informed by an understanding of user needs. For a
systems and for music retrieval research.                   variety of reasons--including intellectual property
                                                            law, limited access to a significant and standard
Keywords User studies, multimedia document                  music testbed, and lack of access to usage records
retrieval, music digital libraries                          for emerging commercial music systems--it has
                                                            been difficult for researchers in music retrieval to
                                                            develop or exploit data concerning the music
1 Introduction
                                                            information behavior of target users. This situation is
Creating a useful and usable music retrieval system         particularly problematic in that the common
is a notoriously difficult task. A music document           assumptions of `typical' music behavior made by
may consist of a symbolic representation of a work          retrieval researchers and music system developers
(eg, a score or MIDI encoding), an audio file (eg,          have been found to differ markedly from actual
MP3), an image (eg, a CD cover), textual metadata           music behavior in the real world [4].
(a work's title, artist, composer, etc.), lyrics, a video       Query log analysis of music related interactions
of a performance--or a combination of any or all of         on Web search engines (eg, [12]) yield extremely
the above [4]. Significant problems have yet to be          coarse-grained information on music behavior;
resolved with document / query representation               sessions are generally short, queries are generally
schemes, retrieval algorithms, and interface support        brief, and the log provides no insight into the
in this challenging research area.                          searchers' motivations, intended use of retrieved
    This paper focuses on identifying problems in           music documents, or satisfaction with the search
developing systems for supporting lyrics-based              results. Few usage studies exist of music digital
information needs. At first glance it would appear          libraries or specific music collections (eg, [5], [8]).
that creating a lyrics-based music digital library          These types of investigations are necessarily limited
would be one of the more straightforward                    to providing insights into the usability of features
development efforts in music retrieval, given that          implemented in the system studied; log data cannot
text-based retrieval is a better understood endeavor        suggest additional functionality or document types
than image, video, and audio retrieval. This paper is       appropriate for the users. For both search engines
a preliminary investigation into whether or not             and digital libraries, the user's information need is
existing music retrieval research can address (or is        obscured by the requirement of complying with the
addressing) support for lyrics retrieval systems.           query formats of a specific system.
    Our approach is based on developing an                      What is required, then, is a source of authentic
understanding of what people want to find, and how          music information behavior and needs. Earlier
they describe what they want, when they are trying          examinations of music behavior are based on
to satisfy a lyrics information need. To that end, we       information requests harvested from music-related




                                                         137

newsgroups [3], question-answer services [7], and         language music­related questions (eg, [1], [3], [7]).
archives of mailing lists [2]. These resources are        These categories were regarded as tentative and were
seeing use to the extent of providing immense             revised based on examination of the Yahoo!
quantities of raw data on a scale similar to web logs;    Answers Lyrics queries. An iterative coding process
however, manual analysis methods limit in practice        was employed, continuing until the two researchers
the size of a harvested dataset to at most a few          agreed on both the coding categories and the codes
hundred requests. This type of investigation              assigned to each question.
complements log analysis with a finer-grained
understanding of music behavior.
                                                          4 Characterizing the desired outcome
     Most technical music retrieval research focuses
on integrating lyrics with audio:        for example,     At this point, we examine the types of music
aligning lyrics to audio signals (eg, [9]); or using      information that the posters have specified that they
lyrics as a basis for thematic or genre clustering and    would like to receive as a response to their
classification of related audio files (eg, [10]). Lyric   question--that is, the types of music document or
retrieval has proved to be a special case of text         details that they are seeking (Table 1).
retrieval, inspiring additional research into problems
such as identifying and matching multiple (non-            Category              No. of queries % (of 237)
identical) lyrics for a single song [6] and supporting
                                                           Lyrics                           51        21.6%
search    over lyrics     that are syllabicated     as
performance instructions [13].                             Metadata                         95        40.3%

                                                           Identification                   36        15.3%
3 Data gathering and analysis
                                                           Copy                              6         2.5%
Yahoo! Answers is an internet based reference site
                                                           Example of type                  16         6.8%
that allows users to both submit and answer
questions. Unlike some earlier `ask an expert              Explanation                      16         6.8%
systems' (eg, Google Answers), there is no charge to
                                                           Feedback                         18         7.6%
post a question and no financial reward to answer
questions. Instead, the system is driven by a `points'     Creative Practice                 7         3.0%
and `levels' arrangement that rewards posters of
                                                           Other                             7         3.0%
correct answers with status within the Yahoo!
Answers community.
                                                               Table 1. Desired responses to questions
    When posting a question to Yahoo! Answers, the
user is required to specify one or more categories for    ·   Lyrics: requests for the complete lyrics to a
it. We focus in this paper exclusively on
                                                              song, or for specific lines (sometimes in a
Entertainment & Music > Music > Lyrics posts.
                                                              specific performance of a song)
Yahoo! Answers sees heavy use; as of September            ·   Metadata: requests for the title of a song and/or
2009, the Lyrics subcategory alone contained over
                                                              its artist / composer (`who it's by').
226,000 questions that had been `resolved' (that is,      ·   Identification: questions asking some variation
had received at least one acceptable response).
                                                              on `what is this song?' without further
    We harvested 250 questions posed on a single
                                                              specification of the desired result.
day in September 2009, from the newly posted              ·   Copy: requests to obtain a copy of an audio or
(`open') section of the Lyrics category. Twelve were
                                                              video version of a song (by downloading or
discarded as duplicates and one discarded as off
                                                              streaming).
topic, leaving 237 questions for analysis. The            ·   Example of type: requests for a song that fits into
average question length was approximately 58
                                                              a specified category or genre (eg, a `love song').
words; the longest question contained 291 words (a        ·   Explanation: requests for `the meaning' of a
request for an explanation of a song's meaning,
                                                              song and/or portions of the lyrics
including the full lyrics), and the shortest a mere 7     ·   Feedback: the question solicits feedback on
(`What are some of.....? your favourite lyrics?'). By
                                                              original song lyrics.
contrast, audio queries to conventional search            ·   Creative Practice: requests for technical or
engines are far more brief (eg,        [12] report an
                                                              creative process information to be used in
average of 3.1 terms in a 2006 study of the
                                                              creating new songs.
metasearch engine Dogpile).                               ·   Other: questions that fall outside the above
    Grounded theory ([11]) was used to develop
                                                              categories.
categories to elicit characterizations of the desired
outcome for the queries (Section 4) and the
                                                             A close examination of the questions and their
information features provided by the poster (Section
                                                          posted answers indicates Metadata and Identification
5). Initial categories were established by bringing
                                                          can be collapsed into a single category; the desired
together features from previous studies of natural
                                                          result in both is a single song matching the given




                                                       138

criteria, with title and/or artist provided as a             Similarly, requests for Feedback and critique of
response. The Copy category is obviously closely             original lyrics written by the poster and assistance in
related; a link to a song's audio will allow the poster      the Creative Practice of creating audio are well
to verify whether that song is indeed the requested          beyond the capacities of existing digital libraries.
music, and further the site hosting the audio (eg,           However, these questions highlight that a great deal
YouTube) commonly includes music metadata such               of music behavior is embedded in a social context--
as title and author. A further breakdown of the 95           we listen to music at social gatherings, talk about the
Metadata requests indicates that the title is the            latest hits in casual conversation, and play songs on
primary identifier for a song: 91 questions request a        the radio or a CD as we drive. It seems appropriate
title, 25 ask for both title and artist, and 4 request the   that a music retrieval system should support music
artist only.                                                 experts, aficionados, and keen novices in discussion
    The next largest category is that of requests for        and in community-based reference services--that the
full or partial Lyrics for a specific song--the only         vision of a music digital library could include people
surprise being that this is not the largest category,        as well documents and software.
given that the poster has explicitly tagged the
question as Lyrics focused. Most Lyrics requests
                                                             5 Characterizing               the      information
appear to assume that there is only one set of lyrics
                                                                  features provided
for a song--they ask for `the words' or `the lyrics'.
    For a minority of the Lyrics requests, the lyrics        The features or characteristics used to describe the
desired are to a specific performance or version of a        204 Lyrics, Bibliographic Details, Copy, Identify,
song and so may not necessarily be the authoritative         Explanation, and Example queries are as follows:
lyrics (eg, one question presents an audio link and
asks, `Can anyone decipher the lyrics up to the 25th            Category               No. of queries    % (of 204)
second? plz? i am a nice guy?'). There may not
                                                                Lyric fragments             113             47.9%
even exist an authoritative version for some songs, or
portions of a song: for example, a `freestyle'                  Storyline                    24             10.2%
improvisation (`the song is called "close my eyes" by
                                                                Video references             18             7.6%
Matisyahu. I can not find the lyrics for the freestyle
he does in the middles of the song'). Some queries              Metadata                     95             40.3%
explicitly request non-authoritative versions of the
                                                                Genre/Style                  42             17.8%
lyrics: for example, `what's the lyrics of the song
paradise by the kpop group "the melody"? the                    Orchestration                30             12.7%
english    translation,    hangul    and    romanization
                                                                Similarity                   11             4.7%
please!' The goal of existing work on identifying
multiple sets of lyrics for a single song [6] is to             Where heard                  50             21.2%
identify the authoritative version and eliminate
                                                                Undesired result             7              3.4%
`mistakes' in other lyrics; these queries suggest that
alternative lyrics should not necessarily be rejected,          Other                        2              0.8%
and that the identification of different versions may
                                                                Table 1. How the information needs are described
be more difficult than previously anticipated (for
example, in matching translations to the original).
                                                             ·    Lyric fragments: the remembered portions of a
    Example of type questions are not answered by a
                                                                  desired song.
specific song (a `known item search'), but instead
                                                             ·    Storyline or message: a description of `what
seek to elicit one or more songs that match a type
                                                                  happens' in a song, or a message conveyed by
description. Picking out an answer from a set of
                                                                  the song (eg, `I love her and miss her').
potential matches is problematic; the standard
                                                             ·    Video references: details about a video including
default for a music retrieval system is to present
                                                                  the desired song (most frequently a music video
textual metadata (eg, song title and artist), which is
                                                                  for the song itself), provided either as a link to a
unlikely to convey the point of similarity between a
                                                                  video file or as a text description of the action
song and the type description (eg, `A Happy
                                                                  occurring in the video.
optomistic, catchy song'). Providing appropriate
                                                             ·    Metadata: bibliographic details, further broken
support for browsing remains an open problem in
                                                                  down into Title, Artist, Collection Title, Date,
music retrieval; coming to a deeper understanding of
                                                                  Remix, and Tempo.
the song facets that are used to judge a match is
                                                             ·    Genre or style: can be a standard genre such as
required to drive interface development (eg, tempo?
                                                                  R&B, or a genre constructed by the poster (eg,
lyrics? affect?).
                                                                  `contemporary, modern').
    Explanation questions (`What Is This Song
                                                             ·    Orchestration: an indication of the instruments
About?') require a deep understanding of the
                                                                  and vocal parts in a recording.
semantics of the lyrics, and are unlikely to be
                                                             ·    Similarity: another song or an artist that is
addressable     by    automated      retrieval   systems.
                                                                  similar to the desired song(s).




                                                          139

·    Undesired result: another song, artist, or               value of a community-based answering service to
     performance that is not the desired result.              work with heavily context dependent questions.
·    Where heard: the circumstances in which the
     poster heard a song performance or broadcast.
                                                              6 Conclusions
    Finding a song based on the lyrics can be
                                                              This paper analyzes a set of Lyrics-related questions
surprisingly difficult. Frustratingly, a person may
                                                              to tease out the types of details presented to describe
remember the Storyline or gist of the song but not
                                                              the information need (Section 5) and the expected
recall any of the lyrics themselves (`the the song
                                                              responses (Section 4); the findings can inform
talks about hating someone so much they wish they
                                                              further music retrieval research and development by
would some how die'). The lyrics for a song can be
                                                              suggesting new directions in search facilities,
difficult to understand as sung, making it difficult to
                                                              browsing structures and interfaces, and document
construct a text search based on the known partial
                                                              representation.
lyrics (`I have NO CLUE what ANY of the lyrics are
except two words because I saw someone mouth
them while the song was playing behind me... All I            References
know is in the chorus it's "something something
                                                              [1] D. Bainbridge, S.J. Cunningham, and J.S. Downie. How
git'cha git'cha"'). A related difficulty is the                    people describe their music information needs: a grounded
                                                                                                               th
mondegreen--a misheard lyric that may seem                         theory analysis of music queries. In 4          International
                                                                   Conference on Music Information Retrieval (ISMIR),
plausible but is incorrect (`someone              in   the
                                                                   Baltimore, Maryland, 2003.
backmground singing fly high or sky high or
                                                              [2] Cunningham, S.J., Bainbridge, D., Falconer, A. More of an
something like that'). It can be difficult to decide
                                                                   art than a science:       playlist and mix construction. In
how to enter lyrics as search terms; should "git'cha
                                                                   International Conference on Music Information Retrieval
git'cha" be entered written? As Get Ya Get Ya? Get                 (ISMIR '06), Vancouver.
You Get You? Gitcha Gitcha? Moreover, some lyrics
                                                              [3] J.S. Downie and S.J. Cunningham. Towards a theory of
are not dictionary words (`Cannot remember any of                  music   information     retrieval queries:  system design
                                                                                       rd
the lyrics for the life of me besides the chorus lyics             implications. In 3      International Conference on Music
                                                                   Information Retrieval (ISMIR), Paris, France, 2002.
which simple go: ooo ooo OOoo ooo ooo, ooo ooo
OOoo ooo ooo (repeat)'). These problems push                  [4] J.S. Downie. Music Information Retrieval. Annual review of

conventional IR matching techniques such as latent                 information science and technology, Volume 37, pages 295-
                                                                   340, 2003.
semantic analysis to their limits and beyond.
    A word or phrase in the lyrics may be too                 [5] M. Itoh. Subject search for music: quantitative analysis of
                                                                   access point selection. In 1       st Annual    International
common to be helpful in constructing a search, but
                                                                   Symposium on Music Information Retrieval, Amherst MA,
the manner in which it is sung can be distinctive                  2000.
enough to be useful ("Free-ee-e-e-ee"). Combining
                                                              [6] P. Knees, M. Schedl, and W. Gerhard. Multiple lyrics
facilities for text and `sung' audio in a query would
                                                                   alignment: automatic retrieval of song lyrics. In 6        th

neatly solve this problem (eg, [9]).                               International Conference on Music Information Retrieval
    Posters are sometimes able to point to songs                   (ISMIR '05), pages 564-569, London, UK, 2005.

Similar to the desired result, or conversely to               [7] J.H. Lee, J.S. Downie, and S.J. Cunningham. Challenges in
indicate songs that are known to not be an answer to               cross-cultural / multilingual music information seeking. In
                                                                   6 International Conference on Music Information Retrieval
                                                                    th
the question (`its definitely not Land of 1000
                                                                   (ISMIR `05), London, UK, 2005.
dances'). Facilities for indicating closeness/distance
                                                              [8] J.R. MCPherson and D. Bainbridge. Usage of the MELDEX
of results to an exemplar would be useful for these
                                                                   digital music library. In 2 Annual International Symposium
                                                                                              nd
queries and also to represent a song's degree of
                                                                   on Music Information Retrieval, Bloomington IN, pages 19-
membership in a Genre.                                             20, 2001.
    Metadata provided is frequently tentatively
                                                              [9] M. Muller, F. Kurth, D. Damm, C. Fremery, and M.
presented as likely to contain errors (`im not sure of             Clausen. Lyrics-based audio retrieval and multimodal
the name of it i believe it's called "spirit"'; `i think it        navigation in music collections. In European Conference on

is by nirvana or rhcp or something like that')--                   Digital Libraries 2007, pages 112-123, Berlin, 2007.

understandably, since if the person had the correct           [10] R. Neumayer and A. Rauber. Integration of text and audio
                                                                                                                              th
metadata then they could answer their question                     features for genre classification in music retrieval. In 29
                                                                   European Conference on Information Retrieval, pages 724-
themselves. The challenge for a retrieval system is to
                                                                   727, Rome, Italy, 2007.
gracefully identify similar values to those suggested,
                                                              [11] A. Strauss and J. Corbin. Basics of Qualitative Research:
for query refinement or ranking of results (eg, terms
                                                                  Grounded Theory Procedures and Techniques. Sage, 1990.
related to spirit, groups whose music is similar to
                                                              [12] D. Tjondronegoro, A. Spink, and B.J. Jansen. Multimedia
that of Nirvana or the Red Hot Chili Peppers).
                                                                   web searching on a meta-search engine. In 12 Australasian
                                                                                                                 th

    Where the poster heard the song might be useful
                                                                   Document    Computing       Symposium, pages 80 ­ 83,
in answering the question (`What was the song                      Melbourne, Australia, 2007.
played at the end of GH on 9/22/09?')--or it might
                                                              [13]      B. Wingenroth, M. Patton, T. DiLauro. In ACM/IEEE
not (`What's the name of a song I heard at Red
                                                                   Joint Conference on Digital Libraries '02, pages 308-309,
Lobster?'). Again, this type of detail suggests the
                                                                   Portland, Oregon, 2002.




                                                           140

               Positive, Negative, or Mixed? Mining Blogs for Opinions

                                 Xiuzhen Zhang, Zhixin Zhou, Mingfang Wu

                                     School of CS & IT, RMIT University
                                GPO Box 2476v, Melbourne 3001, Australia
                        {xiuzhen.zhang, zhixin.zhou, mingfang.wu}@rmit.edu.au


Abstract The rich non-factual information on the             reviews from a certain domain, such as a movie
blogosphere presents interesting research questions.         review or a product review [8], while less success
In this paper, we present a study on analysis of             was seen in those studies conducted within the TREC
blog posts for their sentiment by using a generic            (Text REtrieval Conference) Blog Track on polarity
sentiment lexicon. In particular, we applied Support         search task [6]. On the other hand, existing studies
Vector Machine to classify blog posts into three             have also shown that mixed sentiment is especially
categories of opinions: positive, negative and mixed.        challenging [5] given its uncertainty in nature.
We investigated the performance difference between               The TREC Blog Track's polarity task is to identify
global topic-independent and local topic-dependent           the polarity of the opinions in the retrieved documents
opinion classification on a collection of blogs. Our         (blogs) in respond to a search topic. A problem with
experiment shows that topic-dependent classification         the evaluation of this task is that sentiment analysis is
performs significantly better than topic-independent         mingled with topic search and rank task, as a result, it is
classification, and this result indicates high interaction   hard to ascertain the effectiveness of a certain sentiment
between sentiment words and topic.                           analysis method.
                                                                 This paper presents a study on analysis of blog posts
Keywords blog, sentiment analysis, opinion classifi-
                                                             for their sentiments, or opinions. Specifically a blog
cation, opinion words, information retrieval
                                                             post is analyzed and classified into three categories:
                                                             positive sentiment, negative sentiment or mixed
1 Introduction
                                                             sentiment.    We adopt a dictionary-based approach
With the wide availability of broadband network fa-          by using a generic sentiment lexicon developed by
cilities, internet has become an indispensable channel       a linguistic study [12].      We propose to represent
for people to communicate. More and more people are          blog posts as bags of sentiment words and use the
publishing their own experience and opinions, as well        Support Vector Machine (SVM) [3] learning model
as seeking other people's opinions. With the explo-          to classify blog posts. Given that both the sentiment
sive amount of information generated daily, it is almost     lexicon and the classification model are generic, our
impossible for people to read through all the informa-       research question is: if a global classification of blog
tion even on a narrow topic. This demands for new            posts accross topic genres would achieve a similar
techniques to help track sentiment trends and search         performance as a local classification of blog posts of a
for various opinions, a task that is very different from     certain topic genre.
factual information task as in traditional information           The remainder of this paper is organised as follows.
retrieval; and sentiment analysis is a key component of      We review some related work in Section 2 and describe
such techniques.                                             the sentiment lexicon used in this study in Section 3.
    Sentiment analysis is the technology to evaluate a       We present our classification approach in Section 4, and
text and predicate the text's subjectivity (subjective ver-  experiment setup and evaluations in 5. We discuss the
sus objective) and/or sentiment (positive versus nega-       experiment result and conclude the paper in Section 6.
tive). A general approach is to find out those keywords
from the text that are of evaluative feature or sentiment    2 Related Work
orientation to represent the text, and to use a classi-
                                                             Sentiment analysis was initially applied to a corpus of
fication method to predicate the text's probability of
                                                             documents that are from the same genre, such as a cor-
belonging into pre-defined categories; the classifier is
                                                             pus of movie reviews or a corpus of product review. The
usually trained on a set of labeled texts.
                                                             task of sentiment analysis is to specify if a document
    The above approach has shown success in some
                                                             (or a review) expresses a positive or negative opinion.
earlier work where sentiment analysis was used to
                                                             Naturally most studies adopted machine learning clas-
Proceedings of the 14th Australasian Document Comput-        sification approaches [1, 8, 11]. Pang et al [8] applied
ing Symposium, Sydney, Australia, 4 December 2009.           and compared three machine learning methods, naive
Copyright for this article remains with the authors.         bayes, maximum entropy and support vector machines,




                                                          141

on a corpus of movie reviews with uniform class dis-        subjective expressions that do not have positive or nega-
tribution. Their results showed that the support vector     tive polarity. The property type indicates the expression
machine model generally performed the best.                 intensity and here it has binary values: strong or weak.
    While most sentiment analyses classify comments         As annotation was done within context of a sentence,
or documents into two categories: positive versus nega-     the grammar function of a word is also annotated, for
tive, Koppel and Schler [5] argued that there were other    example, the word admire here is a verb. Thus a word
comments that might express a mixed or neutral senti-       may occur twice or more in the list depending on which
ment. Their study showed that by incorporating neutral      grammar function a word acts in the original text for
category can lead to significant improvement in overall     annotation, for example, the word "cooperation" is an-
classification accuracy, and this is achieved by properly   notated as adjective and none. This list also includes
combining pairwise classifiers.                             words with multiple morphemes, for example, cooper-
    With so many opinionated documents available on         ate, cooperation, cooperative, and cooperatively.
the Web, people are actively seeking other people's
opinion toward a certain topic. In this case, we need       4 Opinion Classification
to do more than sentiment analysis: we need first to
                                                            This section presents our classification method.
retrieve a set of documents that are about the topic,
then judge if a document indeed contains any opinion
                                                            4.1 Support Vector Machine
at all - so called subjectivity analysis, then analyse if
a subjective opinion or sentiment is positive, negative,    Support Vector Machine (SVM) has been widely used
or mixed. Such an opinion polarity finding task was         in text categorisation, and with reported success [3]. In
introduced in TREC 2007 conference [6]. A commonly          an SVM model, objects are represented as vectors. In
adopted approach by participants is to use baseline         learning a model to classify two classes, the basic idea
search engines to search topic-relevant documents           of SVM is to find a hyperplane, represented by a vec-
first, and then use polarity-finding heuristics to re-rank  tor, that separates objects of one class from objects of
documents for polarity.       Machine learning models       other classes at a maximal margin. When using a linear
have not been widely used to improve the polarity           kernel, SVM learns a linear threshold function. With
classification accuracy.                                    polynomial and radial basis kernels, SVM can also be
                                                            used to learn polynomial and radial basis classifiers.
                                                                                  1
3 A Generic Sentiment Lexicon                                   SVM                is an implementation of the multi-
                                                                      multiclass
                                                            class SVM, and is based on Structural SVMs [10]. Un-
Identification of sentiment words is fundamental to sen-    like regular SVMs, structural SVMs can predict com-
timent analysis and classification. There are two broad     plex objects like trees, sequences, or sets. SVM
                                                                                                                struct
methods to identify sentiment words and build senti-        can be used for linear-time training of binary and multi-
ment lexicon. One method is through manual construc-        class SVMs under the linear kernel. Features extracted
tion in which annotators manually annotate a list of        jointly from inputs and outputs are used to form an
words or phrases [9] or find and annotate sentiment         optimal separation plane.
words from a given corpus [8, 12].
    Another method is to build a lexicon from a small       4.2 Opinion Word Extraction
number of seed words with pre-determined sentimental
                                                            To apply a classification model effectively, a key issue
polarity, and then populate the seed list through learning
                                                            is feature selection, i.e. what input will be given to a
or other relationships. For example, Hatzivassiloglou
                                                            classification model. The feature selection is applica-
and McKeown [2] expanded a seed list by adding those
                                                            tion dependent - how do we want to classify a set of
words that are linked to seed words through conjunc-
                                                            documents, and what are prominent features from a set
tion such as and, or, but, either-or, or neither-or; while
                                                            of documents that can separate them from each other.
Kim and Hovey made use of WordNet to populate seed
                                                            For the sentiment classification task, it is intuitive that
words through synonym and antonym relationships [4].
                                                            we identify those opinion words from a set of docu-
    In our study, we use the sentiment lexicon devel-
                                                            ments as classification features.
oped by Wiebe et al. [12]. This lexicon list has 8221
                                                                In this study, we simply treated opinion words as
annotated words resulted from manual annotation of a
                                                            tokens and do not apply natural language processing
10,000-sentence corpus of news articles of various top-
                                                            methods such as Part-Of-Speech tagging to analyse
ics. The following is an example of such an annotation:
                                                            the grammatical function of those words. We applied
                                                            Porter stem method to the list and group different forms
          type=strongsubj len=1 word1=admire
                                                            of the same word, and this leaves us 4919 "words".
      pos1=verb       stemmed1=y          priorpolar-
                                                                A closer look at the stemmed opinion words reveals
      ity=positive
                                                            some interesting facts. There are 103 words that are
    The property prior polarity indicates the attitude be-  of contradictory polarities. After we removed these
ing expressed by the word admire and has three values:      words, we had 4816 words with unique sentiment
positive, negative and neutral. The neutral tag are those
                                                               1Avaiable at http://svmlight.jochims.org/svm multiclass.html




                                                         142

polarity. However, there are also some words that have           Category      TREC-2006          TREC-2007
mixed levels of strength. In lieu of this, we created            Negative    3,707 (32.15%)      1,844 (26.34%)
a new level of strength and named it "contextual                  Mixed      3,664 (31.78%)      2,196 (31.37%)
strength"; there are a total of 194 in this category. The        Positive    4,159 (36.07%)      2,960 (42.29%)
distribution of opinion words in term of polarity and              Total          11,530              7,000
strength is summarised in Table 1.
                                                            Table 2: Distribution of document categories in TREC-
                Positive    Negative    Neutral    Total
                                                            2006 and TREC-2007
  Strong        954         2061        107        3192
  Contextual    81          98          14         194
                                                            of 38.6GB, which are the blogs, Permalink documents
  Weak          544         783         163        1490
                                                            of 88.8GB, which are the blog posts with associated
  Total         1579        2942        284        4816
                                                            comments, and HTML homepages of 28.8GB, which
                                                            are the entries to blogs. The permalink documents are
         Table 1: Distribution of opinion words
                                                            the unit for the opinion finding task and polarity tasks.
                                                                The content of a blog post is defined as the content
                                                            of the blog post itself and the contents of all comments
4.3 Opinion Word Vectors
                                                            to the post. A blog post is considered having subjective
In information retrieval, each document is represented      content if "it contains an explicit expression of opin-
by all word tokens from a collection. However, for          ion or sentiment about the target, showing a personal
the purpose of opinion classification, we represent a       attitude of the writer" [7]. Fifty topics were selected
document as a vector of opinion word tokens and ignore      by NIST from a collection of queries of a commercial
those words that do not express any sentiment. As in        search engine for the opinion retrieval task. For a topic,
retrieval models, we weight each feature (an opinion        permalink documents are tagged with NIST relevance
word) of the document vector. The tf × idf weight of        judgement, with the following categories (or scales) [7]:
an opinion word f in a document d is:                       not judged(-1), not relevant(0), relevant(1), negative(2),
                                                            mixed(3) and positive(4).
                     = tffd × log |Df |
                                    |D|
                w                                               The Blog06 collection was used for both TREC-
                  fd
                                                            2006 and TREC-2007 Blog Track. Fifty (different)
                                                            topics were used for each conference.           For each
where tffd is the frequency of f in d. |D|/|Df| is
                                                            topic, we selected documents with NIST assessor
inverse document frequency of f -- |D| is the number
                                                            relevance judgement scale of 2 (negative), 3 (mixed
of documents in the collection, and |Df| is the number
                                                            - both positive and negative) and 4 (positive) for our
of documents containing f. We expect that this model
                                                            study. Table 2 shows the distribution of documents in
is general enough to be applied to opinion classification.
                                                            different categories in TREC-2006 and TREC-2007
                                                            respectively.
5 Evaluation
                                                                Zettair search engine was used to index documents
                                                                                      2

5.1 Topic-independent               versus       Topic-     with the sentiment lexicon. Each document was con-
       dependent Classification                             verted into a vector of opinion words with the weighting
                                                            scheme as described in Section 4.3.
Opinion classification is usually applied to a set of doc-
uments that are of same genre or about a similar topic      5.3 Topic-independent Opinion Classifi-
such as movie reviews and product reviews. With a                  cation
huge number of opinionated documents on the Web and
                                                            To train the topic-independent opinion classification
the nature of inexact match of a Web search engine, it is
                                                            model, we pooled and indexed all documents from 50
unlikely that we can always get a set of documents from
                                                            topics in TREC-2006. SVM model was then trained
the same genre to be classified. As a sentiment lexicon
                                                            on the converted opinion-word vectors with judgement
is independent of semantic topic of a document, we
                                                            scale >=2.       Ten-fold cross validation experiment
then investigate if there exists any difference between
                                                            was conducted on all 10,737 documents of 50 topics.
classification of documents that are about mixed topics
                                                            It showed an overall accuracy of 52.90±3%, that
and documents about a topic; we call these two types of
                                                            is 52.9% of documents correctly classified, with a
document classification topic-independent (or global)
                                                            standard deviation of 3%.
classification and topic-dependent (or local) classifica-
tion respectively.
                                                            5.4 Topic-dependent Opinion Classifica-
                                                                   tion
5.2 Experiment Set-up
                                                            To examine the interactions between topics and opinion
The TREC Blog track 2006 collection Blog06 [7] is a
                                                            classification accuracy, topics of TREC-2006 that con-
sample of the blogosphere crawled from 6 December
                                                            tain at least 10 documents from each opinion category
2005 to 21 February 2006. The collection is 148GB
in total, and comprises three components: XML feeds            2http:www.seg.rmit.edu.au/zettair/




                                                         141

                                                                       6 Conclusion

                                                                       In this paper we have described our research on opinion
                                                                       classification of blogs. We have investigated the differ-
      0.7                                                              ence of global classification of documents from mixed
                                                                       topics and local classification of documents from the
         0.6
                                                                       same topic. Our experiment on the TREC Blog collec-
                                                                       tions has shown that the local classification is signifi-
            0.5
                                                                       cantly more accurate than the global classification. This
               0.4                                                     might be because that documents from the same topic
                                                                       tended to have a similar set of sentiment words. Our
                  0.3
                                                                       future research will concentrate on developing topic-
                                                                       specific opinion classification models, especially it is
                     0.2

                            Topic-independent    Topic-dependent       anticipated that the annotation of opinion words tensity
                                                                       can be used to further improve such models.

                                                                       Acknowledgements The authors would like to thank
                                                                       Steven Garcia for his various help with using Zettair.
Figure 1: Classification accuracy: Topic-independent
vs. topic-independent
                                                                       References
(recall that there are 3 categories positive, negative or
                                                                        [1] K. Dave, S. Lawrence and D. M. Pennock. Mining
mixed) were extracted, this resulted in 36 topics and
                                                                            the peanut gallery: opinion extraction and semantic
9,771 documents in total.
                                                                            classification of product review.     In Proceedings of
    To evaluate the accuracy of topic-dependent opinion
                                                                            the 12th international conference on World Wide Web,
classification, we individually indexed documents from                      pages 519­528, 2003.
the same topic, and applied ten-fold cross validation
                                                                        [2] V. Hatzivassiloglou and K.R.McKeown. Predicting the
experiment to each topic collection accordingly. On av-
                                                                            semantic orientation of adjectives. In Proceedings of 8th
erage, the topic-dependent model achieved an accuracy                       Conference on European chapter of the association for
of 63±13%, significantly higher than that achieved by                       computational linguistica, pages 174­181, 1997.
the topic-independent model.
                                                                        [3] T. Joachims. Text categorisation with support vector
                                                                            machines: Learning with many relevant features. In
5.5 Blind Test of the Classification Model
                                                                            Proceedings of ECML-98, 1998.
The topic-independent classification model trained on                   [4] S. M. Kim and E. Hovy. Determining the sentiment of
documents with TREC-2006 judgments were blind                               opinions. In Proc. of the Coling Conference, 2004.
tested on documents with TREC-2007 judgements.                          [5] M. Koppel and J. Schler. The importance of neutral
27 topics that contain at least 10 documents in each                        examples for learning sentiment. Computational Intel-
category were used in our study. The model showed                           ligence, Volume 22, Number 2, 2006.
an accuracy of 42% on the whole collection. The drop
                                                                        [6] C. MacDonald, I. Ounis and I. Soboroff. Overview of
in performance compared to that of 10-fold croass                           TREC-2007 blog track. In Proceedings of TREC-2007,
validation (52.9±3%) may be attributed to the change                        Gaithersburg, USA, 2008.
of topics between the two collections, which in turn
                                                                        [7] I. Ounis, M. de Rijke, C. MacDonald and I. Soboroff.
suggests that there is strong correlation between topics                    Overview of trec-2006 blog track. In Proceedings of
and opinion words.                                                          TREC-2006, Gaithersburg, USA, 2007.
    On the other hand, in the 10-fold cross validation
                                                                        [8] B. Pang, L. Lee and S. Vaithyanathan. Thumbs up? sen-
experiment on the TREC-2007 collection, the topic-                          timent classification using machine learning techniques.
dependent model achieved an average accuracy of                             In Proceedings of EMNLP, pages 79­86, 2002.
55%. We extracted individual topic's accuracy for the
                                                                        [9] P. Subasic and A. Huettner. Affect analysis of text using
topic-independent model, and used a paired Wilcoxon
                                                                            fuzzy semantic typing. IEEE Transactions on Fuzzy
test to compare the difference in classification accuracy                   systems, Volume 9, pages 483­496, 2001.
between the topic-independent model and the topic-
                                                                       [10] I. Tsochantaridis, T. Hofmann, T. Joachims and Y. Al-
dependent model. The improvement in classification
                                                                            tun. Support vector learning for interdependent and
accuracy of the topic-dependent model over that of                          structured output spaces. In Proc.: of ICML-2004, 2004.
the topic-independent model is statistically significant
                                                                       [11] P. Turney. Thumbs up or thumbs down? semantic orien-
(p < 0.001). Figure 1 shows the summary of two
                                                                            tation applied to unsupervised classification of reviews.
models. As we can see that the topic-dependent model                        In Proceedings of the ACL, 2002.
achieved higher accuracy than the topic-independent
                                                                       [12] J. Wiebe, T. Wilson and C. Cardie. Annotating expres-
model.
                                                                            sions of opinions and emotions in language. Language
                                                                            Resources and Evaluation (formerly Computers and the
                                                                            Humanities), Volume 1, Number 2, 2005.




                                                                    142

