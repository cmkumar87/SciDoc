Mining Large-scale Comparable Corpora from Chinese-English 
                     News Collections 
                      
Degen Huang1 	Lian Zhao2 	Lishuang Li 3 	Haitao Yu4 
Department of Computer Science and Technology 
       Dalian University of Technology 
       
    1huangdg@dlut.edu.cn 
2zhaolian@mail.dlut.edu.cn 



            Abstract 
           
In this paper, we explore a CLIR-based approach to construct large-scale Chi- nese-English comparable corpora, which is valuable for translation knowledge mining. The initial source and target document sets are crawled from news website and standardized uniformly. Keywords are extracted from the source document firstly, and then the extracted keywords are translated and combined as query words through certain criteria to retrieve against the index created using target document set. Meanwhile, the mapping correlations between source and target documents are developed accord- ing to the value of similarity calculated by the retrieval tool. Two methods are evaluated to filter the comparable docu- ment pairs so as to ensure the quality of the comparable corpora. Experimental re- sults indicate that our approach is effec- tive on the construction of Chinese- English comparable corpora. 
   
3lils@dlut.edu.cn 
       4gengshenspirit@163.com 
     
     
bates or legal texts, parallel corpora remain a scarce resource, despite the proposition of auto- mated methods to collect parallel corpora from the Web. Researches on comparable corpora are motivated by the scarcity of parallel corpora. Compared with parallel corpora, comparable corpora are more abundant, up-to-date and ac- cessible. 
  Comparable corpora are defined as pairs of monolingual corpora selected according to the same set of criteria, but in different languages or language varieties. When creating comparable corpora, the key process is to align the source document with relevant target documents. Early work by Braschler and Scäuble (1998) employed content descriptors and publication dates to align German and Italian news stories. Resnik (1999) mined comparable corpora on the assumption that the pages which are comparable of each other share a similar structure (headers, para- graphs, etc.) when text is presented in many lan- guages in the Web. Tao and Zhai (2005) acquired comparable bilingual text corpora based on the observation that terms that are translations of each other or share the same topic tend to co- occur in the comparable corpora at the same/similar time periods. Recently, Talvensaari 
  
1

Introduction 

et al. (2007) introduced a CLIR-based approach 

Parallel corpora are key resource for statistical 
machine translation, in which machine learning techniques are used to learn translation knowl- edge. Sufficient data is necessary for the data- driven approaches to estimate the model parame- ters reliably. However, as Munteanu (2006) stated, beyond a few resource-rich language pairs such as English-Chinese or English-French and a small number of contexts like parliamentary de- 


This work was supported by Microsoft Research Asia. 

to align two document collections with different languages. All the target documents were in- dexed with Lemur. Then appropriate keywords were extracted from the source language docu- ments and translated into the target language as query words to retrieve similar target documents. 
  As we know, the problems may vary with the language of documents when using CLIR-based approach to construct comparable corpora, such as keyword extraction, out-of-vocabulary key- word translation and so on. This paper is a fur- ther endeavor to CLIR-based approach for com- 
  
  
  
  
  
              472 
Coling 2010: Poster Volume, pages 472-480, 
          Beijing, August 2010 
          
             Figure 1. The general architecture of comparable corpora construction 
parable corpora construction. We focus on the of a document. In this paper, keywords are 
construction of Chinese-English comparable cor- viewed as basic units of search indexes in order pora, explore and address the issues during the to retrieve closely related documents. Generally, construction. Experimental results show that our phrases can capture the main idea of a document method is better through a rough comparison more effectively, inasmuch as they have more with Talvensaari et al. (2007) and it also outper- information than single words (an independent forms our reconstruction of Tao and Zhai (2005) linguistic unit after word segmentation for Chi- 
in respect to the quality of comparable corpora. 	nese). 
  This paper is organized as follows. In section 2, 	Existing approaches for keyword extraction 
the general architecture of our system is de- could be distinguished into two main categories: scribed, and each module is illuminated in detail. supervised or unsupervised methods. Supervised Section 3 reports and analyzes the experimental machine learning algorithms were widely used in 

results followed by conclusions in section 4. 
2 System Architecture 

keyword extraction such as Naïve Bayes (Frank et al., 1999; Witten et al., 1999), SVM (Zhang et al., 2006), CRF (Zhang et al., 2008), etc. These 


Figure 1 shows the general architecture of our apapsrodaicfhiesulhafdorexceltlo ntosntatbiulitty.a How-everu,giht e

comparable corpora construction system. It con- golden annotated corpus co trsarn ca goobdigclenoifier, 

w

fc t 

us 

sists of two components: component I and com- especially for news wetb paiges. Unsupassised ponent II. Component I is mainly composed by a methods hinged on evaluating various feateurrves to web crawler, which is used to harvest source and select keywords, such as word frequency (Luhn, target documents from selected web sites. We 1957), word co-occurrence (Matsuo and Ishizuka, can get the final source and target document sets 2004), and TF*IDF (Li et al., 2007). The inher- through content extraction and noise filtering. ent problem in these methods was that most of The core of the system is component II, which their work came in the judgment whether a can- aligns a source document with target documents didate was a keyword or not, but they had not having comparable contents. It implements on paid sufficient attention to the identification of the two document sets generated by component I. phrase candidates. Wan and Xiao (2008) pro- Component II is composed of three modules: posed a method for keyphrase extraction from keyword extraction, keyword translation, and single document. However, it simply combined retrieval & filtering. The methods for three mod- the adjacent candidate words to a multi-word 

ules are detailed respectively. 
2.1 Keyword Extraction 

phrase. 
  Based on the above observation, our approach for keyword extraction focuses more on the con- 
  
A keyword is described as a meaningful and sig- struction of phrasal candidates. It is mainly based nificant expression containing one or more words. on MWE (Multi-Word Expression) extraction Appropriate keywords briefly describe the theme together with relevant word ranking method. 




473 

MWE is a special lexical unit including com- identify MWEs from the document with modi- pound terms, idioms and collocations, etc. The fied segmentation. Consequently, new collection process of keyword extraction in this paper of MWEs is acquired. 
mainly depends on the following stages. 	Additionally, some simple rules are defined Stage 1: The generation of phrasal candidates 	according to language features to filter MWEs. 
(1) The extraction of MWEs from the preproc- In this paper, our method is tailored to extract 
essed document 	keywords from news web pages which contain 
  Document preprocessing is a procedure of some special symmetric marks like "?, ?". The 
morphological analysis including segmentation words in a specially marked area are usually im- 
and part of speech tagging for Chinese. The portant to the document. So we extract words method based on the marginal probabilities de- within each paired marks and view them as a tailed in (Luo and Huang, 2009) is adopted in MWE on the condition that it contains two or 
this part. 	more than two words. All of the MWEs are 
  We extract MWEs using LocalMaxs selection viewed as phrasal candidates and filtered by algorithm together with a relevance measure cal- stopword list. 
culation method (FSCP) proposed by Silva et al. Stage 2: The generation of single words candi- 
(1999). Suffix arrays and related structures in dates 
(Aires et al., 2008) are used to compute the FSCP 	Our method also generates single word candi- 
value so as to raise efficiency. And the initial col- dates with the account that both phrase and sin- lection of MWEs named G for the document is gle word can be served as a keyword. The proc- 
generated after filtered by stopword list. 	ess of single word selection is independent of 
(2) The acquisition of new MWEs through the MWE extraction. The candidate words are re- 
modification for segmentation 	stricted to nouns, verbs, strings (like WTO) and 
  As a matter of fact, the results of segmentation merged words as discussed in the previous stage. for the document usually have some errors espe- But the word will be removed if it only appears cially for out-of-vocabulary (OOV) words which once in the document or is contained in the are segmented to single Chinese characters in stopword list. 
most cases. Inaccurate segmentation leads to Stage 3: Keyword selection based on candidates 
some faults for keyword extraction. As stated in ranking 
(Liu et al., 2007), OOV words can be identified 	As for MWE candidates, we calculate the 
by the method of MWEs extraction mentioned weight for them using Formula 1 which refers to above. Therefore, we modify the segmentation the formula used to sort NP phrases in (Brace- like this: any MWE in G is merged to one word well et al., 2008). But the weight of len is re- if it only consists of single Chinese characters duced. 

and its frequency > freq. The changes before and 
after merging are shown in Table 1. Because the 

Weight(MWE)?? log( len?? fMWE???

method of MWE extraction is based on statistical 

1?? len tf (w )) ?i?1 i 

(1) 

techniques, so low frequency of MWE will result Where len is thlenlength of MWE (in number of e

in poor performance. But large value for freq words); f 

is the frequency of the MWE within 

means that very few MEWs can satisfy the fre- in the doMWE ent; tf(w ) is the frequency of word 

quency restriction. In our experiments, we set w . The fculmwing ruleis are used to rank MWEs: 

freq=2. The extraction process is called again to 

i

ol o 

MWE 
 
Segmentation 
before merging 

Segmentation 
after merging 
     
Po s 
before merging 
    
Po s 
after merging 

                                    ?/n ?/n ?/n ?/n ?/n ?/n ??/oov 
?/ ?/ ?/ ?/ ?/ ?/ ?/ ??/ ?/ 

?? 

?/vl ??/n ?/us ?/vl ??/n ?/us 

??/ ?/ ?/ 

??/ ?/ ?/ 

?/ a 

?/ a 

? /jb ? /n ? ? /b ??/oov ??/b 

?? 

?/ ?/ ??/ ?/ 

??/ ??/ ?/ 

?/ n 

?/ n 

Table 1. Changes before and after merging 





474 

(a) more frequent MWEs are ranked higher; (b) 	(A) Only extracts single words as keywords MWEs with larger weight are ranked higher. In 	while just MWEs with (B). (C) The method pre- order to avoid redundancy, we remove the re- 	sented in this paper which makes a proper com- dundant MWEs with lower rank. 	bination of MWEs and single words. 
Single word candidates are ranked as follows: 	P	R	F

(a) the single word w with larger TF*IDF value 

A (single words) 24.2% 28.5% 26.4% 

is ranked higher; (b) the pos score for w in de- 
scending order is: named entity, merged words, 

B (MWEs) 
C (A+B) 

1 8 .1 % 2 3 .0 % 2 0 .6 % 3 4 .2 % 4 3 .6 % 3 8 .9 % 

nouns, strings, verbs. In the end, top-a MWEs 
and top-b single words are chosen to form the keyword set of the document. 



2 .2 

Table 2. Keyword extraction results 
Keyword Translation 

Stage 4: Parameters evaluation and experimental 
results 
  The max number of keywords extracted from each document is limited to ten (a+b=10) and we run our approach on the dataset which include one hundred Chinese documents from the corpus of NTCIR-5 since they are also news articles. For evaluation of the results, the keywords ex- tracted by our method are compared with the manually extracted keywords (at most ten key- words are assigned to each document). The F- measure is used as evaluation metric. It is de- 
fined like this: F=(P+R)/2; P=nummatch/numsystem; 
R=nummatch/nummanual. Where nummatch is the 
count of keywords extracted by our method 
matching with manually extracted keywords; 
numsystem is the count of keywords extracted by 
our method; nummanual is the count of keywords 
assigned by human. 
  Figure 2 shows the performance curves for our extraction method. In this figure, a ranges from 0 to 10 while b is 10 to 0. It performs best when a = 4 and b = 6. So the two values are adopted in this paper. 
  
  
  
  
  
  
  
  
  
  
 Figure 2. F-measure varies with the value of a 
  We test our approach on another dataset which 
also contains one hundred documents. In the ex- periments, the max number of keywords is set to ten. Table 2 shows the results of keyword extrac- tion under three different conditions respectively. 

As for keyword translation, there are three main 
approaches: translation based on dictionary, par- allel corpora and machine translation. Dictionary based approach is adopted in our system by tak- ing the acquisition of translation resource into account. 
  Word Sense Disambiguation (WSD) and OOV problem are the main difficulties in CLIR (Cross Language Information Retrieval) task. A typical bilingual dictionary will provide a set of alterna- tive translations for a given keyword, so how to choose the optimal translation is called Word Sense Disambiguation. Actually some keywords can not be found and translated due to the cover- age limitation of a bilingual dictionary, which is called OOV problem. 
  In this paper, the keyword is given up if its size of translations gained from the bilingual dic- tionary is larger than two for the convenience of WSD. Additionally, both of the translations are treated as synonyms and equal weight is assigned to them when retrieval. 
  To address the OOV problem, researchers pro- posed methods using snippets returned by a search engine. For example, Wang et al. (2004) introduced a statistics-based approach called SCPCD to mine translations from the returned snippets. Different from (Wang et al., 2004), Zhou et al. (2007) used a pattern-based approach to analyze the mixed-languages snippets. 
  Leveraging on previous work, we analyze the co-occurrence mode of the OOV term and the corresponding translation in the returned snippets. Table 3 shows the typical co-occurrence modes collected during experiments, where the English words in bold are the corresponding translations of the underlined Chinese OOV terms. From Ta- ble 3, we can see the translations in number 1, 2 and 3 are included in the symmetric symbols, like bracket, quotation marks. However, the 
  
 
  
  
  
475 

 Serial 
number 
1
2
                  
Segments extracted from the returned snippets 
?????????:????«The Bridges of Madison County»??:??? 
?????:????(The Bridges of Madison County)-52 ???... 

3	????????"????" cowboy diplomacy)???"????"??? (
4	...???????????? Bayesian Network for Data Mining-??:?... 
5	??«????????» The End of Cowboy Diplomacy)???????? (
6	?????????. The Bridges of Madison County. Forrest Gump ????... 
       Table 3. Chinese OOV and the corresponding translation in returned snippets 
translations in number 4, 5, and 6 are embedded ject1. On the basis of Lemur, it combines infer- 
in the partial sentence while there are noise Eng- ence networks with language modeling. And it's lish words. In order to get the correct translation, widely adopted by institution for scientific re- the partial sentence needs to be segmented. By search since it is effective, flexible, usable and above analysis, we integrate the SCPCD method powerful. So it is employed by us to retrieve re- and the pattern-based method so as to extract lated documents. A query for each source docu- more correct translations. The SCPCD method ment is formed by the translated keywords with can be used to determine the boundaries for Indri query language and then run against the OOVs like number 4, 5, and 6; while pattern- target collection. 
based method makes use of the symmetric sym- 	The essential of alignment is to compare the 
bols like number 1, 2 and 3. Table 4 shows the similarity between source and target document experimental results for OOV translation meth- pairs. In order to reduce the workload of compar- ods. The average top-n inclusion rate is adopted ing, Pooling method is applied to assist the com- as a metric. For a set of test OOV terms, its top-n paring process. We choose the top r documents inclusion rate is defined as the percentage of the returned by Indri retrieval system to build the OOVs whose translations can be found in the related document pool. And g (g<=r) documents 

first n extracted translations. 
         Pattern SCPCD Pattern + SCPCD 

in the pool are selected to form the alignment document pairs together with the source docu- 

Top-1 40.0% 49.2% Top-3 41.5% 55.4% 

6 8 .1 % 7 0 .2 % 

ment. In our experiments, we set r=10 and g=1. 
  In the process of alignment, three features are 
  
Table 4. The performance comparison of differ- used to filter the alignment pairs for the sake of 

ent OOV translation methods 

pruning the low relevant pairs. The first is publi- 

The test dataset used is the Chinese topic csatisoin dlatretycontainedtein dboycumenits.bTtheesecotnd 

terms in CLIR task of NTCIR-5. The search en- iquerymiaadi thecatlacruleat documeIndrwhee weernevhe. d

gine is Google. The bilingual dictionary used by The last is KSD (Keyword snitmilariny rbtetiwealn 

n

g

us is LDC_CE_DICT 2.0. And we only adapt the document pairs) which is defined by t ur systee . 

pattern with symmetric symbols, which has the In this paper, we propose two methoods to filmr te 

highest precision proposed by Cao et al. (2007). 

the alignment pairs by using various features. 

2 .3 	Retrieval and Filtering 	(1) DSF filtering 
                                              This method depends on two features: date 
                                              
The process of retrieval is to construct the align- 
ment relationship between source and target document pairs. It is a core module in our system since the quality of comparable corpora is greatly influenced by alignment level which depends on the relevance between document pairs. Our in- tention here is to retrieve high relevant target documents for the source documents. Open- 

and similarity. At first, we give a priority to the target documents that have the closest date to the source document during the top-r documents searching. A date-window size d is defined to measure the date difference. We set d=1 in this paper. That is to say, the target documents with 

source toolkit Indri is introduced to assist the 

1 Lemur 

toolkit is developed by Carnegie Mellon University 

retrieval process. Indri is a part of the Lemur pro- 	and University of Massachusetts. The open source code is 
                                                      available at http://www.lemurproject.org. 
                                              
                                      
                                      
                                      
476 

exactly the same date as the source document, and one day earlier or later are considered to be closest. Then, we select g documents with larger similarity from the related document pool. Fi- nally, we rank all of the alignment pairs with the score of similarity and set a similarity threshold s to filter further. It should be noted that there are 
n?· g alignment pairs, where n is the number of 

FIS which are dealt with Formula 3. Actually, the two filtering methods differ principally in the last step. DSKF sorts all of the alignment pairs ac- cording to the KSD score while it is similarity in DSF. We also set a KSD threshold k for DSKF method to filter further. The values for s and k will be investigated in the following experiments. 

source documents having non-empty related 	3	Experiments 

document pool. 
(2) DSKF filtering 
  This method utilizes all of the features: date, similarity and KSD. As for KSD, it integrates two factors. One is NTK, namely the number of 

In this section, we first introduce how to acquire 
the source and target document sets. Then our system is tested on the two sets. The experimen- tal results are reported and analyzed finally. 

translated keywords appeared in the target 	3 .1 	Experiment Setup 
document, since the target document is more 

similar to the source document as increasing of NTK. The other is FIS, namely frequency infor- mation score. Inspired by paper (Tao and Zhai, 2005), we use the score of FIS to measure the correlations between the keywords in source document and translated keywords in target document which represent the matching for 
source and target document pair. We define ds as 
the source document, dt as the target document, ks as the set of keywords extracted from ds, kts as 
the set of translated keywords. Formula 2 is used 
to compute the score of FIS: 

ScoreFIS????ki t?s1Len (BM 25(xi , ds )?· IDF (xi )?· (2) 
BM 25( yi , dt )?· IDF ( yi ) / norm(Dif (xi , yi ))) 
Where, ktsLen is the size of kts, yi is an element 
in kts, xi is the element in ks while yi is the trans- lation of xi. Moreover, BM25(w, d) is the normal- 
ized frequency of word w in document d. It has 
been considered as one of the most effective 

To test the effectiveness of the proposed system, 
large-scale of Chinese and English news web pages are crawled respectively from XinHuaNet and used as the document resource. The reasons 
for choosing news pages are: 
  (1) Many websites, like portal website, news agency, government and so on, provide large- scale news reports. At the same time, a large pro- portion of the reports can be crawled politely, so document acquisition is relatively easy. 
  (2) The news pages include various contents, such as politics, economy, sports, so the corpora made up of news pages can avoid the limitations of domain-specific corpora. 
  All the news pages are processed uniformly. The core content of each web page crawled is extracted and several tags describing the headline and publication date are added. Meanwhile, the original contents are kept with no change. Table 5 shows the basic information of document sets. 
  
matching functions for retrieval. IDF stands for 	o	o

Inverse Document Frequency which is also 

Year Number mfesosurce Numobceurmfntasrget 

commonly used in information retrieval. Dif(x, y) 
is defined as the difference between BM25(x, ds) 
and BM25(y, dt). Formula 2 penalizes large dif- 
ference due to the conditions like this: any key- 
word in source document appears many times while its translation appears rarely in target document. The process of its normalization is run by Formula 3 which makes the score less sensi- 

2003 2004 2005 2006 2007 2008 2009 Total 

doc u nt 
23747 25660 47333 28572 25036 14021 
 7476 
 171845 

d
   
et 
3390 2943 
11578 25320 25247 24292 10887 
103657 

tive to the absolute value: 

Table 5. The composition of source document set 

norm(score)???? 1,score,scoree?l1e ?

(3) 

and target document set 

?

s

Furthermore, the final KSD score is got by 	3 .2 	Results and Discussion 
simply adding the normalized scores of NTK and 	The quality of comparable corpora highly de- 





477 

pends on the alignment level between source and target document pairs. Braschler and Scäuble (1998) used five levels of relevance to assess the 
alignments as follows: 
  (1) Same story. The two documents deal with the same event. 
  (2) Related story. The two documents deal with the same event or topic from a slightly dif- ferent viewpoint. Alternatively, the other docu- ment may concern the same event or topic, but the topic is only a part of a broader story or the article is comprised of multiple stories. 
  (3) Shared aspect. The documents deal with related events. They may share locations or per- s ons . 
  (4) Common terminology. The events or topics are not directly related, but the documents share a considerable amount of terminology. 
  (5) Unrelated. The similarities between the documents are slight or nonexistent. 
  We randomly select 500 source documents published in 2009 as the test dataset. Experi- ments with different parameters are constructed based on this dataset. The quality of each align- ment pair is manually assessed using the five- level relevance as discussed above. What should to be pointed out is that parameter s and k are not absolute values, but percentile rank level in our work. For instance, k = 10 means that we only choose the alignment pairs whose KSD score 
rank in top ten percent among all of the results. 
  
Table 6 shows the results filtered by DSF 
method with different values of s (s1 < s2 < s3 < 
s4 ). Table 7 shows the results filtered by DSKF method with various values of k (k1 < k2 < k3 < k4). In order to evaluate the results conveniently, 
two standards are established: (a) the number of 
high relevant pairs created, which is the count of document pairs in Level 1 and 2; (b) the quality of the whole alignments, that is to say the per- centage of alignment pairs with Level 1 and 2. Seen from Table 6 and 7, DSKF is better than DSF by considering the two standards. Com- pared with DSF, more high relevant pairs are left filtered by DSKF when they have the same total number of pairs. In other words, the DSKF method is more powerful to make high relevant pairs in higher rank so as to reduce alignment pairs which are rarely relevant. Therefore, DSKF is adopted in our system. Taking the first crite- 
rion into account, we give up the parameter k1, k2. 
Parameter k4 is not the best considering the sec- ond criterion. Ultimately, k3 is chosen as the final 
value for k. At this point, the number of align- 
ment pairs in Level 1 and 2 is close to the maxi- mum. Meanwhile, the percentage of high align- ments reaches 68.5%. 
  Among the surveyed related work, Talvensaari et al. (2007) created Swedish-English compara- ble corpora based on CLIR techniques and its framework of construction is similar to ours. However, the two systems are different in the 
following aspects: 


Level 
     
s 1 = 10 
Number 



%
     
s 2 = 30 
Number 



%
     
s 3 = 50 
Number 



%
     
s 4 = 70 
Number 



%

Leve1 1 	23 	4 6 .9 % 	54 	3 6 .5 % 	83 	3 3 .5 % 	96 	2 7 .7 % Level 2 	18 	3 6 .7 % 	43 	2 9 .1 % 	62 	2 5 .0 % 	81 	2 3 .3 % Level 3 	4	8 .2 % 	21 	1 4 .2 % 	40 	1 6 .1 % 	57 	1 6 .4 % Level 4 	4	8 .2 % 	19 	1 2 .8 % 	41 	1 6 .5 % 	60 	1 7 .3 % Level 5 	0	0 .0 % 	11 	7 .4 % 	22 	8 .9 % 	53 	1 5 .3 % 
 Total 	49 	100% 	148 	100% 	248 	100% 	347 	100% 
Table 6. The distribution results filtered by DSF with different s parameters 


Level 
     
k 1 = 10 
Number 



%
     
k 2 = 30 
Number 



%
     
k 3 = 50 
Number 



%
     
k 4 = 70 
Number 



%

Level 1 	33 	6 7 .3 % 	78 	5 2 .7 % 	93 	3 7 .5 % 	98 	2 8 .2 % Level 2 	15 	3 0 .6 % 	52 	3 5 .1 % 	77 	3 1 .0 % 	89 	2 5 .6 % Level 3 	1	2 .0 % 	9	6 .1 % 	37 	1 4 .9 % 	62 	1 7 .9 % Level 4 	0	0 .0 % 	9	6 .1 % 	34 	1 3 .7 % 	60 	1 7 .3 % Level 5 	0	0 .0 % 	0	0 .0 % 	7	2 .8 % 	38 	1 1 .0 % 
 Total 	49 	100% 	148 	100% 	248 	100% 	347 	100% 
Table 7. The distribution results filtered by DSKF with different k parameters 





478 

  (1) The language is different. We focus on son with our work. We align each source docu- building comparable corpora of Chinese-English ment with one target document through the 
while they were Swedish-English. 	BM25Corr model in (Tao and Zhai, 2005). The 
  (2) A series of sub problems are different due alignment pairs are ranked according to mapping to language difference. As for keyword extrac- scores calculated by the BM25Corr model. And tion, we propose a method to select both key we select the top N (N = 248) alignment pairs for phrases and single words, while they used RATF the benefit of comparison. 
(Relative Average Term Frequency) method. For 	Table 8 shows the distribution results for the 
OOV problem, we combine the SCPCD method three systems. As illustrated in Table 8, we can with the pattern-based method to extract OOV roughly conclude that our approach creates more translations from snippets returned by a search alignment pairs with the same number of source engine. However, the classified s-gram matching documents when compared with Talvensaari et al. technique was utilized by Talvensaari et al. (2007) (2007). Meanwhile, the percentage of high rele- 
to translate OOV words. 	vant document pairs is larger. 
  (3) Talvensaari et al. (2007) filtered their 	Likewise, our system outperforms BM25Corr 
alignment pairs mainly depending on date and in that it aligns more high relevant documents similarity, while we introduce new feature KSD pairs when they use the same sample of test cor- 
to extend the original feature set. 	pora and create the same total number of pairs. 
  Talvensaari et al. (2007) also randomly chose Obviously, the quality of comparable corpora 
500 source documents and assessed the quality gained by our system is better than BM25Corr. 
of alignments using the same five-level relevance. 	All the experimental results and analysis men- 
  In addition to this, we implement the method tioned above indicate that our method is effective of Tao and Zhai (2005) which is a purely statisti- to create alignment pairs. Up to now, both the cal-based and language independent approach. source and target documents published in 2007- The source and target documents published in 2009 years are used to build comparable corpora 2009 are employed to test the method. The same through our proposed system. It includes 23102 sample as our system including 500 Chinese alignment pairs after filtered by DSKF. 
documents is chosen to make a further compari- 
         Talvensaari et al. 	Our System 	BM25Corr 
Level 	(2007) 	(DSKF filtering) 	(Top N = 248) 
         Number 	%	Number 	%	Number 	%
Level 1 	21 	2 1 .6 % 	93 	3 7 .5 % 	1	0 .4 % 
Level 2 	20 	2 0 .6 % 	77 	3 1 .0 % 	2	0 .8 % 
Level 3 	33 	3 4 .0 % 	37 	1 4 .9 % 	3	1 .2 % 
Level 4 	19 	1 9 .6 % 	34 	1 3 .7 % 	5	2 .0 % 
Level 5 	4	4 .1 % 	7	2 .8 % 	237 	9 5 .6 % 
 Total 	97 	100% 	248 	100% 	248 	100% 
Table 8. The distribution results for Talvensaari et al. (2007), Our System, and BM25Corr 
                                       three features as publication date, similarity 
                                       
4 Conclusions 

score and KSD value are used to filter the 

In this paper, we propose a CLIR-based approach aligned document pairs. Experimental results 
to create large-scale Chinese-English comparable show that our approach is effective to mine Chi- corpora. Firstly, we harvest the original source nese-English document pairs with comparable and target document sets from news website us- contents. In the future, we will optimize the ap- ing open-source crawler. Then the core content proach for every module in the construction of of each document is extracted through discrimi- comparable corpora for the sake of improving nating noise contents. Next, we delve into the the performance of the whole system. What's approaches of problems such as keyword extrac- more, it will be worth consideration to mine tion and OOV translation followed by the proc- mappings between terms which can be served as ess of retrieval to develop mapping correlations a feature for the process of developing mappings between source and target documents. Finally, between document pairs in turn. 




479 

References 
Aires, José, Gabriel Lopes, and Joaquim Ferreira 
Silva. 2008. Efficient Multi-word Expressions Ex- tractor Using Suffix Arrays and Related Structures. In Proceeding of the 2nd ACM workshop on Imp- roving non english web searching, pp. 1-8. 
Bracewell, David B., Fuji Ren, and Shingo Kuroiwa. 
2008. Mining News Sites to Create Special Do- main News Collections. International Journal of Computational Intelligence, 4(1): 56-63. 
Braschler, Martin, and Peter Scäuble. 1998. Multilin- 
gual Information Retrieval Based on Document Alignment Techniques. In Proceedings of the 2nd European Conference on Research and Advanced 
Technology for Digital Libraries, pp. 183-197. 
Cao Guihong, Jianfeng Gao, and Jianyun Nie. A Sys- 
tem to Mine Large-Scale Bilingual Dictionaries from Monolingual Web pages. 2007. In Proceed- ings of Machine Translation Summit XI, pp. 57-64. 
Frank, Eibe, Gordon W. Paynter, and Ian H. Witten. 
1999. Domain-Specific Keyphrase Extraction. In Proceedings of the 16th International Joint Con- 
ference on Artificial Intelligence, pp. 668-673. 
Li Juanzi, Qi'na Fan, and Kuo Zhang. 2007. Keyword 
Extraction Based on tf/idf for Chinese News Document. Wuhan University Journal of Natural Sciences, 12(5): 917-921. 
Liu Tao, Bingquan Liu, Xiaolong Wang, and Minghui 
Li. 2007. The Effectiveness Study of Local Maxi- mum Feature for Chinese Unknown Word Identifi- cation. Journal of Chinese Language and Comput- ing, 17(1): 15-26. 
Luhn, Hans Peter. 1957. A Statistical Approach to 
Mechanized Encoding and Searching of Literary Information. IBM Journal of Research and Devel- opment, 1(4): 309-317. 
Luo Yanyan, and Degen Huang. 2009. Chinese Word 
Segmentation Based on the Marginal Probabilities Generated by CRFs. Journal of Chinese Informa- tion Processing, 23(5): 3-8. 
Matsuo, Yutaka and Mitsuru Ishizuka. 2004. Key- 
word Extraction from a Single Document Using Word Co-occurrence Statistical Information. Inter- national Journal on Artificial Intelligence Tools, 13(1): 157-169. 
Munteanu, Dragos Stefan. 2006. Exploiting Compa- 
rable Corpora. Doctoral Thesis. UMI Order 
No.3257825. University of Southern California. 
Resnik, Philip. 1999. Mining the web for bilingual 
  text. In Proceedings of the 37th Annual Meeting of 

the Association for Computational Linguistics, pp. 527-534. 
Silva, Joaquim Ferreira, Gaël Dias, Sylvie Guilloré, 
and José Gabriel Pereira Lopes. 1999. Using Lo- calMaxs Algorithm for the Extraction of Contigu- ous and Non-contiguous Multiword Lexical Units. In Proceedings of the 9th Portuguese Conference on Artificial Intelligence, pp. 113-132. 
Talvensaari, Tuomas, Jorma Laurikkala, Kalervo 
Järvelin, Martti Juhola and Heikki Keskustalo. 2007. Creating and Exploiting a Comparable Cor- pus in Cross-Language Information Retrieval. ACM Transactions on Information Systems, 25(1):1-21. 
Tao Tao, and Chengxiang Zhai. 2005. Mining Com- 
parable Bilingual Text Corpora for Cross- Language Information Integration. In Proceedings of the 11th ACM SIGKDD international conference on Knowledge discovery in data mining, pp. 691- 696. 
Wan Xiaojun, and Jianguo Xiao. 2008. CollabRank: 
Towards a Collaborative Approach to Single- Document Keyphrase Extraction. In Proceeding of the 22nd International Conference on Computa- tional Linguistics, pp. 969-976. 
Wang Jenq Haur, Jie Wen Teng, Pu Jen Cheng, Wen 
Hsiang Lu, and Lee Feng Chien. 2004. Translating Unknown Cross-Lingual Queries in Digital Librar- ies using a Web-based Approach. In Proceedings of the 4th ACM/IEEE-CS joint Conference on Digi- tal Libraries, pp. 108-116. 
Witten, Ian H., Gordon W. Paynter, Eibe Frank, Carl 
Gutwin, and Craig G. Nevill-Manning. 1999. KEA: Practical automatic keyphrase extraction. In Pro- ceedings of the 4th ACM Conference on Digital Li- braries, pp. 254-255. 
Zhang Chengzhi, Huilin Wang, Yao Liu, Dan Wu, Yi 
Liao, and Bo Wang. 2008. Automatic Keyword Extraction from Documents Using Conditional Random Fields. Journal of Computational Infor- mation Systems, 4(3): 1169-1180. 
Zhang Kuo, Hui Xu, Jie Tang, and Juanzi Li. 2006. 
Keyword Extraction Using Support Vector Ma- chines. In Proceedings of the 7th International Conference on Web-Age Information Management, pp. 85-96. 
Zhou Dong, Mark Truran, Tim Brailsford, and Helen 
Ashman. 2007. NTCIR-6 Experiments Using Pat- tern Matched Translation Extraction. In Proceed- ings of 6th NTCIR Workshop Meeting, pp. 145-151. 





480 
