          Re-examining Automatic Keyphrase Extraction Approaches
                                          in Scientific Articles

                      Su Nam Kim                                          Min-Yen Kan
                        CSSE dept.                             Department of Computer Science
                University of Melbourne                        National University of Singapore
         snkim@csse.unimelb.edu.au                              kanmy@comp.nus.edu.sg



                     Abstract                               2006; Nguyen and Kan, 2007). In Section 2, we
                                                            give a more comprehensive overview of previous
    We tackle two major issues in automatic                 attempts.
    keyphrase extraction using scientific arti-                Current keyphrase technology still has much
    cles: candidate selection and feature engi-             room for improvement. First of all, although sev-
    neering. To develop an efficient candidate              eral candidate selection methods have been pro-
    selection method, we analyze the nature                 posed for automatic keyphrase extraction in the
    and variation of keyphrases and then se-                past (e.g. (Frank et al., 1999; Park et al., 2004;
    lect candidates using regular expressions.              Nguyen and Kan, 2007)), most of them do not ef-
    Secondly, we re-examine the existing fea-               fectively deal with various keyphrase forms which
    tures broadly used for the supervised ap-               results in the ignorance of some keyphrases as can-
    proach, exploring different ways to en-                 didates. Moreover, no studies thus far have done
    hance their performance.         While most             a detailed investigation of the nature and varia-
    other approaches are supervised, we also                tion of manually-provided keyphrases. As a con-
    study the optimal features for unsuper-                 sequence, the community lacks a standardized list
    vised keyphrase extraction. Our research                of candidate forms, which leads to difficulties in
    has shown that effective candidate selec-               direct comparison across techniques during evalu-
    tion leads to better performance as evalua-             ation and hinders re-usability.
    tion accounts for candidate coverage. Our                  Secondly, previous studies have shown the ef-
    work also attests that many of existing fea-            fectiveness of their own features but not many
    tures are also usable in unsupervised ex-               compared their features with other existing fea-
    traction.                                               tures. That leads to a redundancy in studies and
                                                            hinders direct comparison. In addition, existing
1 Introduction
                                                            features are specifically designed for supervised
Keyphrases are simplex nouns or noun phrases                approaches with few exceptions. However, this
(NPs) that represent the key ideas of the document.         approach involves a large amount of manual labor,
Keyphrases can serve as a representative summary            thus reducing its utility for real-world application.
of the document and also serve as high quality in-          Hence, unsupervised approach is inevitable in or-
dex terms. It is thus no surprise that keyphrases           der to minimize manual tasks and to encourage
have been utilized to acquire critical information          utilization. It is a worthy study to attest the re-
as well as to improve the quality of natural lan-           liability and re-usability for the unsupervised ap-
guage processing (NLP) applications such as doc-            proach in order to set up the tentative guideline for
ument summarizer(Davanzo and Magnini, 2005),´               applications.
information retrieval (IR)(Gutwin et al., 1999) and            This paper targets to resolve these issues of
document clustering(Hammouda et al., 2005).                 candidate selection and feature engineering. In
  In the past, various attempts have been made to           our work on candidate selection, we analyze the
boost automatic keyphrase extraction performance            nature and variation of keyphrases with the pur-
based primarily on statistics(Frank et al., 1999;           pose of proposing a candidate selection method
Turney, 2003; Park et al., 2004; Wan and Xiao,              which improves the coverage of candidates that
2008) and a rich set of heuristic features(Barker           occur in various forms. Our second contribution
and Corrnacchia, 2000; Medelyan and Witten,                 re-examines existing keyphrase extraction features


                                                         9

             Proceedings of the 2009 Workshop on Multiword Expressions, ACL-IJCNLP 2009, pages 9­16,
                             Suntec, Singapore, 6 August 2009. 2009 ACL and AFNLPc

reported in the literature, in terms of their effec-  Textract (Park et al., 2004) also ranks the can-
tiveness and re-usability. We test and compare        didate keyphrases by its judgment of keyphrases'
the usefulness of each feature for further improve-   degree of domain specificity based on subject-
ment. In addition, we assess how well these fea-      specific collocations(Damerau, 1993), in addi-
tures can be applied in an unsupervised approach.     tion to term cohesion using Dice coefficient(Dice,
   In the remaining sections, we describe an          1945). Recently, Wan and Xiao (2008) extracts
overview of related work in Section 2, our propos-    automatic keyphrases from single documents, uti-
als on candidate selection and feature engineering    lizing document clustering information. The as-
in Section 4 and 5, our system architecture and       sumption behind this work is that the documents
data in Section 6. Then, we evaluate our propos-      with the same or similar topics interact with each
als, discuss outcomes and conclude our work in        other in terms of salience of words. The authors
Section 7, 8 and 9, respectively.                     first clustered the documents then used the graph-
                                                      based ranking algorithm to rank the candidates in
2 Related Work                                        a document by making use of mutual influences of
                                                      other documents in the same cluster.
The majority of related work has been carried
out using statistical approaches, a rich set of       3 Keyphrase Analysis
symbolic resources and linguistically-motivated
heuristics(Frank et al., 1999; Turney, 1999; Barker   In previous study, KEA employed the index-
and Corrnacchia, 2000; Matsuo and Ishizuka,           ing words as candidates whereas others such as
2004; Nguyen and Kan, 2007). Features used can        (Park et al., 2004; Nguyen and Kan, 2007) gen-
be categorized into three broad groups: (1) docu-     erated handcrafted regular expression rules. How-
ment cohesion features (i.e. relationship between     ever, none carefully undertook the analysis of
document and keyphrases)(Frank et al., 1999;          keyphrases. We believe there is more to be learned
Matsuo and Ishizuka, 2004; Medelyan and Wit-          from the reference keyphrases themselves by do-
ten, 2006; Nguyen and Kan, 2007), and to lesser,      ing a fine-grained, careful analysis of their form
(2) keyphrase cohesion features (i.e. relationship    and composition. Note that we used the articles
among keyphrases)(Turney, 2003) and (3) term          collected from ACM digital library for both ana-
cohesion features (i.e. relationship among compo-     lyzing keyphrases as well as evaluating methods.
nents in a keyphrase)(Park et al., 2004).             See Section 6 for data in detail.
   The simplest system is KEA (Frank et al.,             Syntactically, keyphrases can be formed by ei-
1999; Witten et al., 1999) that uses TF*IDF (i.e.     ther simplex nouns (e.g. algorithm, keyphrase,
term frequency * inverse document frequency) and      multi-agent) or noun phrases (NPs) which can be a
first occurrence in the document. TF*IDF mea-         sequence of nouns and their auxiliary words such
sures the document cohesion and the first occur-      as adjectives and adverbs (e.g. mobile network,
rence implies the importance of the abstract or       fast computing, partially observable Markov de-
introduction which indicates the keyphrases have      cision process) despite few incidences. They can
a locality.   Turney (2003) added the notion of       also incorporate a prepositional phrase (PP) (e.g.
keyphrase cohesion to KEA features and Nguyen         quality of service, policy of distributed caching).
and Kan (2007) added linguistic features such         When keyphrases take the form of an NP with an
as section information and suffix sequence. The       attached PP (i.e. NPs in of-PP form), the preposi-
GenEx system(Turney, 1999) employed an inven-         tion of is most common, but others such as for, in,
tory of nine syntactic features, such as length in    via also occur (e.g. incentive for cooperation, in-
words and frequency of stemming phrase as a           equality in welfare, agent security via approximate
set of parametrized heuristic rules. Barker and       policy, trade in financial instrument based on log-
Corrnacchia (2000) introduced a method based          ical formula). The patterns above correlate well
on head noun heuristics that took three features:     to part-of-speech (POS) patterns used in modern
length of candidate, frequency and head noun fre-     keyphrase extraction systems.
quency. To take advantage of domain knowledge,           However, our analysis uncovered additional lin-
Hulth et al. (2001) used a hierarchically-organized   guistic patterns and alternations which other stud-
domain-specific thesaurus from Swedish Parlia-        ies may have overlooked. In our study we also
ment as a secondary knowledge source.          The    found that keyphrases also occur as a simple con-


                                                    10

   Criteria    Rules
  Frequency    (Rule1) Frequency heuristic i.e. frequency  2 for simplex words vs. frequency  1 for NPs
   Length      (Rule2) Length heuristic i.e. up to length 3 for NPs in non-of-PP form vs. up to length 4 for NPs in of-PP form
               (e.g. synchronous concurrent program vs. model of multiagent interaction)
 Alternation   (Rule3) of-PP form alternation
               (e.g. number of sensor = sensor number, history of past encounter = past encounter history)
               (Rule4) Possessive alternation
               (e.g. agent's goal = goal of agent, security's value = value of security)
  Extraction   (Rule5) Noun Phrase = (NN|NNS|NNP|NNPS|JJ|JJR|JJS) (NN|NNS|NNP|NNPS)      

               (e.g. complexity, effective algorithm, grid computing, distributed web-service discovery architecture)
               (Rule6) Simplex Word/NP IN Simplex Word/NP
               (e.g. quality of service, sensitivity of VOIP traffic (VOIP traffic extracted),
               simplified instantiation of zebroid (simplified instantiation extracted))


                                          Table 1: Candidate Selection Rules

junctions (e.g. search and rescue, propagation and                  this section, before we present our method, we first
delivery), and much more rarely, as conjunctions                    describe the detail of keyphrase analysis.
of more complex NPs (e.g. history of past en-                          In our keyphrase analysis, we observed that
counter and transitivity). Some keyphrases appear                   most of author assigned keyphrase and/or reader
to be more complex (e.g. pervasive document edit                    assigned keyphrase are syntactically more of-
and management system, task and resource allo-                      ten simplex words and less often NPs.              When
cation in agent system). Similarly, abbreviations                   keyphrases take an NP form, they tend to be a sim-
and possessive forms figure as common patterns                      ple form of NPs. i.e. either without a PP or with
(e.g. belief desire intention = BDI, inverse docu-                  only a PP or with a conjunction, but few appear as
ment frequency = (IDF); Bayes' theorem, agent's                     a mixture of such forms. We also noticed that the
dominant strategy).                                                 components of NPs are normally nouns and adjec-
   A critical insight of our work is that keyphrases                tives but rarely, are adverbs and verbs. As a re-
can be morphologically and semantically altered.                    sult, we decided to ignore NPs containing adverbs
Keyphrases that incorporate a PP or have an un-                     and verbs in this study as our candidates since they
derlying genitive composition are often easily var-                 tend to produce more errors and to require more
ied by word order alternation. Previous studies                     complexity.
have used the altered keyphrases when forming in                       Another observation is that keyphrases contain-
of-PP form. For example, quality of service can                     ing more than three words are rare (i.e. 6% in our
be altered to service quality, sometimes with lit-                  data set), validating what Paukkeri et al. (2008)
tle semantic difference. Also, as most morpho-                      observed. Hence, we apply a length heuristic. Our
logical variation in English relates to noun num-                   candidate selection rule collects candidates up to
ber and verb inflection, keyphrases are subject to                  length 3, but also of length 4 for NPs in of-PP
these rules as well (e.g. distributed system 6= dis-                form, since they may have a non-genetive alter-
tributing system, dynamical caching 6= dynamical                    nation that reduces its length to 3 (e.g. perfor-
cache). In addition, possessives tend to alternate                  mance of distributed system = distributed system
with of-PP form (e.g. agent's goal = goal of agent,                 performance). In previous studies, words occur-
security's value = value of security).                              ring at least twice are selected as candidates. How-
                                                                    ever, during our acquisition of reader assigned
4 Candidate Selection
                                                                    keyphrase, we observed that readers tend to collect
We now describe our proposed candidate selection                    NPs as keyphrases, regardless of their frequency.
process. Candidate selection is a crucial step for                  Due to this, we apply different frequency thresh-
automatic keyphrase extraction. This step is corre-                 olds for simplex words (>= 2) and NPs (>= 1).
lated to term extraction study since top Nth ranked                 Note that 30% of NPs occurred only once in our
terms become keyphrases in documents. In pre-                       data.
vious study, KEA employed the indexing words                           Finally, we generated regular expression rules
as candidates whereas others such as (Park et al.,                  to extract candidates, as presented in Table 1. Our
2004; Nguyen and Kan, 2007) generated hand-                         candidate extraction rules are based on those in
crafted regular expression rules. However, none                     Nguyen and Kan (2007). However, our Rule6
carefully undertook the analysis of keyphrases. In                  for NPs in of-PP form broadens the coverage of


                                                               11

possible candidates. i.e. with a given NPs in of-       puting algorithm. We also normalize TF with re-
PP form, not only we collect simplex word(s),           spect to candidate types: i.e. we separately treat
but we also extract non-of-PP form of NPs from          simplex words and NPs to compute TF. To make
noun phrases governing the PP and the PP. For           our IDFs broadly representative, we employed the
example, our rule extracts effective algorithm of       Google n-gram counts, that were computed
grid computing as well as effective algorithm and       over terabytes of data. Given this large, generic
grid computing as candidates while the previous         source of word count, IDF can be incorporated
works' rules do not.                                    without corpus-dependent processing, hence such
                                                        features are useful in unsupervised approaches
5 Feature Engineering                                   as well. The following list shows variations of
                                                        TF*IDF, employed as features in our system.
With a wider candidate selection criteria, the onus
of filtering out irrelevant candidates becomes the         · (F1a) TF*IDF
responsibility of careful feature engineering. We
list 25 features that we have found useful in ex-          · (F1b*) TF including counts of substrings
tracting keyphrases, comprising of 9 existing and
                                                           · (F1c*) TF of substring as a separate feature
16 novel and/or modified features that we intro-
duce in our work (marked with ). As one of                 · (F1d*) normalized TF by candidate types
our goals in feature engineering is to assess the            (i.e. simplex words vs. NPs)
suitability of features in the unsupervised setting,
we have also indicated which features are suitable         · (F1e*) normalized TF by candidate types as
only for the supervised setting (S) or applicable to         a separate feature
both (S, U).
                                                           · (F1f*) IDF using Google n-gram

5.1 Document Cohesion
                                                           F2 : First Occurrence (S,U) KEA used the first
Document cohesion indicates how important the           appearance of the word in the document(Frank et
candidates are for the given document. The most         al., 1999; Witten et al., 1999). The main idea
popular feature for this cohesion is TF*IDF but         behind this feature is that keyphrases tend to oc-
some works have also used context words to check        cur in the beginning of documents, especially in
the correlation between candidates and the given        structured reports (e.g., in abstract and introduc-
document. Other features for document cohesion          tion sections) and newswire.
are distance, section information and so on. We            F3 : Section Information (S,U) Nguyen and
note that listed features other than TF*IDF are re-     Kan (2007) used the identity of which specific
lated to locality. That is, the intuition behind these  document section a candidate occurs in. This lo-
features is that keyphrases tend to appear in spe-      cality feature attempts to identify key sections. For
cific area such as the beginning and the end of doc-    example, in their study of scientific papers, the
uments.                                                 authors weighted candidates differently depending
   F1 : TF*IDF (S,U) TF*IDF indicates doc-              on whether they occurred in the abstract, introduc-
ument cohesion by looking at the frequency of           tion, conclusion, section head, title and/or refer-
terms in the documents and is broadly used in pre-      ences.
vious work(Frank et al., 1999; Witten et al., 1999;        F4* : Additional Section Information (S,U)
Nguyen and Kan, 2007). However, a disadvan-             We first added the related work or previous work
tage of the feature is in requiring a large corpus      as one of section information not included in
to compute useful IDF. As an alternative, con-          Nguyen and Kan (2007). We also propose and test
text words(Matsuo and Ishizuka, 2004) can also          a number of variations. We used the substrings
be used to measure document cohesion. From our          that occur in section headers and reference titles
study of keyphrases, we saw that substrings within      as keyphrases. We counted the co-occurrence of
longer candidates need to be properly counted, and      candidates (i.e. the section TF) across all key sec-
as such our method measures TF in substrings as         tions that indicates the correlation among key sec-
well as in exact matches. For example, grid com-        tions. We assign section-specific weights as in-
puting is often a substring of other phrases such as    dividual sections exhibit different propensities for
grid computing algorithm and efficient grid com-        generating keyphrases. For example, introduction


                                                      12

contains the majority of keyphrases while the ti-               original work, a large, external web corpus was
tle or section head contains many fewer due to the              used to obtain the similarity judgments. As we
variation in size.                                              did not have access to the same web corpus and
                                                                all candidates/keyphrases were not found in the
   · (F4a*) section, 'related/previous work'                    Google n-gram corpus, we approximated this fea-
                                                                ture using a similar notion of contextual similarity.
   · (F4b*) counting substring occurring in key
                                                                We simulated a latent 2-dimensional matrix (simi-
      sections
                                                                lar to latent semantic analysis) by listing all candi-
   · (F4c*) section TF across all key sections                  date words in rows and their neighboring words
                                                                (nouns, verbs, and adjectives only) in columns.
   · (F4d*) weighting key sections according to                 The cosine measure is then used to compute the
      the portion of keyphrases found                           similarity among keyphrases.

   F5* : Last Occurrence (S,U) Similar to dis-
                                                                5.3 Term Cohesion
tance in KEA , the position of the last occurrence
of a candidate may also imply the importance of                 Term cohesion further refines the candidacy judg-

keyphrases, as keyphrases tend to appear in the                 ment, by incorporating an internal analysis of the

last part of document such as the conclusion and                candidate's constituent words.      Term cohesion

discussion.                                                     posits that high values for internal word associa-
                                                                tion measures correlates indicates that the candi-
5.2 Keyphrase Cohesion                                          date is a keyphrase (Church and Hanks, 1989).
The intuition behind using keyphrase cohesion is                   F9 : Term Cohesion (S,U) Park et al. (2004)
that actual keyphrases are often associated with                used in the Dice coefficient (Dice, 1945)
each other, since they are semantically related to              to measure term cohesion particularly for multi-
topic of the document. Note that this assumption                word terms. In their work, as NPs are longer than
holds only when the document describes a single,                simplex words, they simply discounted simplex
coherent topic ­ a document that represents a col-              word cohesion by 10%. In our work, we vary the
lection may be first need to be segmented into its              measure of TF used in Dice coefficient,
constituent topics.                                             similar to our discussion earlier.
   F6* : Co-occurrence of Another Candidate
in Section (S,U) When candidates co-occur in                       · (F9a) term cohesion by (Park et al., 2004),

several key sections together, then they are more
                                                                   · (F9b*) normalized TF by candidate types
likely keyphrases. Hence, we used the number of
                                                                     (i.e. simplex words vs. NPs),
sections that candidates co-occur.
   F7* : Title overlap (S) In a way, titles also rep-
                                                                   · (F9c*) applying different weight by candi-
resent the topics of their documents. A large col-
                                                                     date types,
lection of titles in the domain can act as a prob-
abilistic prior of what words could stand as con-                  · (F9d*) normalized TF and different weight-
stituent words in keyphrases. In our work, as we                     ing by candidate types
examined scientific papers from computer science,
we used a collection of titles obtained from the                5.4 Other Features
large CiteSeer1 collection to create this feature.
                                                                F10 : Acronym (S) Nguyen and Kan (2007) ac-
   · (F7a*) co-occurrence (Boolean) in title col-               counted for the importance of acronym as a fea-
      location                                                  ture. We found that this feature is heavily depen-
                                                                dent on the data set. Hence, we used it only for
   · (F7b*) co-occurrence (TF) in title collection              N&K to attest our candidate selection method.
                                                                   F11 : POS sequence (S) Hulth and Megyesi
   F8 : Keyphrase Cohesion (S,U) Turney (2003)
                                                                (2006) pointed out that POS sequences of
integrated keyphrase cohesion into his system by
                                                                keyphrases are similar. It showed the distinctive
checking the semantic similarity between top N
                                                                distribution of POS sequences of keyphrases and
ranked candidates against the remainder. In the
                                                                use them as a feature. Like acronym, this is also
   1It contains 1.3M titles from articles, papers and reports.  subject to the data set.


                                                              13

   F12 : Suffix sequence (S) Similar to acronym,                             number of author assigned keyphrase and reader
Nguyen and Kan (2007) also used a candidate's                                assigned keyphrase found in the documents.
suffix sequence as a feature, to capture the propen-
                                                                                           Author           Reader         Combined
sity of English to use certain Latin derivational
                                                                              Total       1252 (53)      3110 (111)       3816 (146)
morphology for technical keyphrases. This fea-                                NPs            904            2537              3027
ture is also a data dependent features, thus used in                          Average    3.85 (4.01)     12.44 (12.88)   15.26 (15.85)
supervised approach only.                                                     Found          769            2509              2864

   F13 : Length of Keyphrases (S,U) Barker and                                        Table 2: Statistics in Keyphrases
Corrnacchia (2000) showed that candidate length
is also a useful feature in extraction as well as in                         7 Evaluation
candidate selection, as the majority of keyphrases                           The baseline system for both the supervised and
are one or two terms in length.                                              unsupervised approaches is modified N&K which
                                                                             uses TF*IDF, distance, section information and
6 System and Data
                                                                             additional section information (i.e. F1-4). Apart
To assess the performance of the proposed candi-                             from baseline , we also implemented basic
date selection rules and features, we implemented                            KEA and N&K to compare. Note that N&K is con-
a keyphrase extraction pipe line. We start with                              sidered a supervised approach, as it utilizes fea-
raw text of computer science articles converted                              tures like acronym, POS sequence, and suffix se-
from PDF by pdftotext.                           Then, we parti-             quence.
tioned the into section such as title and sections                             Table 3 and 4 shows the performance of our can-
via heuristic rules and applied sentence segmenter                           didate selection method and features with respect
2, ParsCit3(Councill et al., 2008) for refer-                                to supervised and unsupervised approaches using
ence collection, part-of-speech tagger4 and lem-                             the current standard evaluation method (i.e. exact
matizer5(Minnen et al., 2001) of the input. Af-                              matching scheme) over top 5th, 10th, 15th candi-
ter preprocessing, we built both supervised and                              dates.
unsupervised classifiers using Naive Bayes from                                BestFeatures includes F1c:TF of substring as
the WEKA machine learning toolkit(Witten and                                 a separate feature, F2:first occurrence, F3:section
Frank, 2005), Maximum Entropy6, and simple                                   information, F4d:weighting key sections, F5:last
weighting.                                                                   occurrence, F6:co-occurrence of another candi-
   In evaluation, we collected 250 papers from                               date in section, F7b:title overlap, F9a:term co-
four different categories7 of the ACM digital li-                            hesion by (Park et al., 2004), F13:length of
brary. Each paper was 6 to 8 pages on average.                               keyphrases. Best-TF*IDF means using all best
In author assigned keyphrase, we found many                                  features but TF*IDF.
were missing or found as substrings. To rem-                                   In Tables 3 and 4, C denotes the classifier tech-
edy this, we collected reader assigned keyphrase                             nique: unsupervised (U) or supervised using Max-
by hiring senior year undergraduates in computer                             imum Entropy (S)8.
science, each whom annotated five of the papers                                In Table 5, the performance of each feature is
with an annotation guideline and on average, took                            measured using N&K system and the target fea-
about 15 minutes to annotate each paper. The fi-                             ture. + indicates an improvement, - indicates a
nal statistics of keyphrases is presented in Table                           performance decline, and ? indicates no effect
2 where Combined represents the total number of                              or unconfirmed due to small changes of perfor-
keyphrases. The numbers in () denotes the num-                               mances.     Again, supervised denotes Maximum
ber of keyphrases in of-PP form. Found means the                             Entropy training and Unsupervised is our unsu-
                                                                             pervised approach.
   2http://www.eng.ritsumei.ac.jp/asao/resources/sentseg/
   3http://wing.comp.nus.edu.sg/parsCit/                                     8 Discussion
   4http://search.cpan.org/dist/Lingua-EN-
Tagger/Tagger.pm                                                             We compared the performances over our candi-
   5
    http://www.informatics.susx.ac.uk/research/groups/nlp/carroll/morph.html
   6                                                                         date selection and feature engineering with sim-
    http://maxent.sourceforge.net/index.html
   7C2.4 (Distributed Systems), H3.3 (Information Search                     ple KEA , N&K and our baseline system. In eval-
and Retrieval), I2.11 (Distributed Artificial Intelligence-                  uating candidate selection, we found that longer
Multiagent Systems) and J4 (Social and Behavioral Sciences-
Economics)                                                                      8Due to the page limits, we present the best performance.



                                                                          14

    Method         Features    C                      Five                                Ten                                Fifteen
                                    Match    Precision    Recall  Fscore  Match  Precising   Recall  Fscore  Match   Precision     Recall  Fscore

     All            KEA        U     0.03     0.64%       0.21%   0.32%    0.09   0.92%      0.60%   0.73%   0.13      0.88%       0.86%   0.87%
  Candidates                   S     0.79     15.84%      5.19%   7.82%    1.39   13.88%     9.09%   10.99%  1.84     12.24%       12.03%  12.13%
                    N&K        S     1.32     26.48%      8.67%   13.06%   2.04   20.36%     13.34%  16.12%  2.54     16.93%       16.64%  16.78%
                 baseline      U     0.92     18.32%      6.00%   9.04%    1.57   15.68%     10.27%  12.41%  2.20     14.64%       14.39%  14.51%
                               S     1.15     23.04%      7.55%   11.37%   1.90   18.96%     12.42%  15.01%  2.44     16.24%       15.96%  16.10%
  Length<=3         KEA        U     0.03     0.64%       0.21%   0.32%    0.09   0.92%      0.60%   0.73%   0.13      0.88%       0.86%   0.87%
  Candidates                   S     0.81     16.16%      5.29%   7.97%    1.40   14.00%     9.17%   11.08%  1.84     12.24%       12.03%  12.13%
                    N&K        S     1.40     27.92%      9.15%   13.78%   2.10   21.04%     13.78%  16.65%  2.62     17.49%       17.19%  17.34%
                 baseline      U     0.92     18.4%       6.03%   9.08%    1.58   15.76%     10.32%  12.47%  2.20     14.64%       14.39%  14.51%
                               S     1.18     23.68%      7.76%   11.69%   1.90   19.00%     12.45%  15.04%  2.40     16.00%       15.72%  15.86%
  Length<=3         KEA        U     0.01     0.24%       0.08%   0.12%    0.05   0.52%      0.34%   0.41%   0.07      0.48%       0.47%   0.47%
  Candidates                   S     0.83     16.64%      5.45%   8.21%    1.42   14.24%     9.33%   11.27%  1.87     12.45%       12.24%  12.34%
 + Alternation      N&K        S     1.53     30.64%      10.04%  15.12%   2.31   23.08%     15.12%  18.27%  2.88     19.20%       18.87%  19.03%
                 baseline      U     0.98     19.68%      6.45%   9.72%    1.72   17.24%     11.29%  13.64%  2.37     15.79%       15.51%  15.65%
                               S     1.33     26.56%      8.70%   13.11%   2.09   20.88%     13.68%  16.53%  2.69     17.92%       17.61%  17.76%



                                    Table 3: Performance on Proposed Candidate Selection

           Features      C                   Five                               Ten                             Fifteen
                              Match     Prec.    Recall    Fscore  Match   Prec.  Recall     Fscore Match   Prec.    Recall     Fscore
             Best        U     1.14     .228      .747       .113    1.92   .192   .126      .152    2.61   .174      .171       .173
                          S    1.56     .312      .102       .154    2.50   .250   .164      .198    3.15   .210      .206       .208
             Best        U     1.14     .228       .74       .113    1.92   .192   .126      .152    2.61   .174      .171       .173
        w/o TF*IDF        S    1.56     .311      .102       .154    2.46   .246   .161      .194    3.12   .208      .204       .206


                                          Table 4: Performance on Feature Engineering


      A      Method     Feature                                            tion, keyphrase co-occurrence with selected sec-
      +        S        F1a,F2,F3,F4a,F4d,F9a
               U        F1a,F1c,F2,F3,F4a,F4d,F5,F7b,F9a                   tions was proposed in our work and found empiri-
       -       S        F1b,F1c,F1d,F1f,F4b,F4c,F7a,F7b,F9b-d,F13          cally useful. Term cohesion (Park et al., 2004) is a
               U        F1d,F1e,F1f,F4b,F4c,F6,F7a,F9b-d
      ?        S        F1e,F10,F11,F12                                    useful feature although it has a heuristic factor that
               U        F1b
                                                                           reduce the weight by 10% for simplex words. Nor-
        Table 5: Performance on Each Feature                               mally, term cohesion is subject to NPs only, hence
                                                                           it needs to be extended to work with multi-word
                                                                           NPs as well. Table 5 summarizes the reflections
length candidates play a role to be noises so de-                          on each feature.
creased the overall performance. We also con-
firmed that candidate alternation offered the flexi-                           As unsupervised methods have the appeal of not
bility of keyphrases leading higher candidate cov-                         needing to be trained on expensive hand-annotated
erage as well as better performance.                                       data, we also compared the performance of super-
   To re-examine features, we analyzed the impact                          vised and unsupervised methods. Given the fea-
of existing and new features and their variations.                         tures initially introduced for supervised learning,
First of all, unlike previous studies, we found that                       unsupervised performance is surprisingly high.
the performance with and without TF*IDF did not                            While supervised classifier produced a matching
lead to a large difference which indicates the im-                         count of 3.15, the unsupervised classifier obtains a
pact of TF*IDF was minor, as long as other fea-                            count of 2.61. We feel this indicates that the exist-
tures are incorporated. Secondly, counting sub-                            ing features for supervised methods are also suit-
strings for TF improved performance, while ap-                             able for use in unsupervised methods, with slightly
plying term weighting for TF and/or IDF did not                            reduced performance. In general, we observed that
impact on the performance.                    We estimated the             the best features in both supervised and unsuper-
cause that many of keyphrases are substrings of                            vised methods are the same ­ section information
candidates and vice versa. Thirdly, section in-                            and candidate length. In our analysis of the im-
formation was also validated to improve perfor-                            pact of individual features, we observed that most
mance, as in Nguyen and Kan (2007). Extend-                                features affect performance in the same way for
ing this logic, modeling additional section infor-                         both supervised and unsupervised approaches, as
mation (related work) and weighting sections both                          shown in Table 5. These findings indicate that al-
turned out to be useful features. Other locality                           though these features may be been originally de-
features were also validated as helpful: both first                        signed for use in a supervised approach, they are
occurrence and last occurrence are helpful as it                           stable and can be expected to perform similar in
implies the locality of the key ideas. In addi-                            unsupervised approaches.


                                                                         15

9 Conclusion                                                    Annette Hulth and Jussi Karlgren and Anna Jonsson and
                                                                   Henrik Bostrm and Lars Asker. Automatic Keyword Ex-
We have identified and tackled two core issues                     traction using Domain Knowledge. In Proceedings of CI-
                                                                   CLing. 2001.
in automatic keyphrase extraction: candidate se-
lection and feature engineering. In the area of                 Annette Hulth and Beata Megyesi. A study on automatically
                                                                   extracted keywords in text categorization. In Proceedings
candidate selection, we observe variations and al-                 of ACL/COLING. 2006, 537­544.
ternations that were previously unaccounted for.                Mario Jarmasz and Caroline Barriere. Using semantic sim-
Our selection rules expand the scope of possible                   ilarity over tera-byte corpus, compute the performance of
                                                                   keyphrase extraction. In Proceedings of CLINE. 2004.
keyphrase coverage, while not overly expanding
                                                                Dawn Lawrie and W. Bruce Croft and Arnold Rosenberg.
the total number candidates to consider. In our                    Finding Topic Words for Hierarchical Summarization. In
re-examination of feature engineering, we com-                     Proceedings of SIGIR. 2001, pp. 349­357.
piled a comprehensive feature list from previous                Y. Matsuo and M. Ishizuka. Keyword Extraction from a Sin-
works while exploring the use of substrings in de-                 gle Document using Word Co-occurrence Statistical Infor-
                                                                   mation. In International Journal on Artificial Intelligence
vising new features. Moreover, we also attested to                 Tools. 2004, 13(1), pp. 157­169.
each feature's fitness for use in unsupervised ap-              Olena Medelyan and Ian Witten. Thesaurus based automatic
proaches, in order to utilize them in real-world ap-               keyphrase indexing. In Proceedings of ACM/IEED-CS
                                                                   joint conference on Digital libraries. 2006, pp.296­297.
plications with minimal cost.
                                                                Guido Minnen and John Carroll and Darren Pearce. Applied
10 Acknowledgement                                                 morphological processing of English. NLE. 2001, 7(3),
                                                                   pp.207­223.

This work was partially supported by a National Research        Thuy Dung Nguyen and Min-Yen Kan. Key phrase Extrac-
Foundation grant, Interactive Media Search (grant # R 252          tion in Scientific Publications. In Proceeding of ICADL.
000 325 279), while the first author was a postdoctoral fellow     2007, pp.317-326.
at the National University of Singapore.                        Youngja Park and Roy Byrd and Branimir Boguraev. Auto-
                                                                   matic Glossary Extraction Beyond Terminology Identifi-
                                                                   cation. In Proceedings of COLING. 2004, pp.48­55.
References
                                                                Mari-Sanna Paukkeri and Ilari Nieminen and Matti Polla
 Ken Barker and Nadia Corrnacchia. Using noun phrase               and Timo Honkela. A Language-Independent Approach
   heads to extract document keyphrases. In Proceedings of         to Keyphrase Extraction and Evaluation. In Proceedings
   the 13th Biennial Conference of the Canadian Society on         of COLING. 2008.
   Computational Studies of Intelligence: Advances in Arti-     Peter Turney. Learning to Extract Keyphrases from Text.
   ficial Intelligence. 2000.                                      In National Research Council, Institute for Information
Regina Barzilay and Michael Elhadad. Using lexical chains          Technology, Technical Report ERB-1057. 1999.
   for text summarization. In Proceedings of the ACL/EACL       Peter Turney. Coherent keyphrase extraction via Web min-
   1997 Workshop on Intelligent Scalable Text Summariza-           ing. In Proceedings of IJCAI. 2003, pp. 434­439.
   tion. 1997, pp. 10­17.
                                                                Xiaojun Wan and Jianguo Xiao. CollabRank: towards a col-
 Kenneth Church and Patrick Hanks.          Word association       laborative approach to single-document keyphrase extrac-
   norms, mutual information and lexicography. In Proceed-         tion. In Proceedings of COLING. 2008.
   ings of ACL. 1989, 76­83.
                                                                Ian Witten and Gordon Paynter and Eibe Frank and Car
Isaac Councill and C. Lee Giles and Min-Yen Kan. ParsCit:          Gutwin and Graig Nevill-Manning. KEA:Practical Au-
   An open-source CRF reference string parsing package. In         tomatic Key phrase Extraction. In Proceedings of ACM
   Proceedings of LREC. 2008, 28­30.                               DL. 1999, pp.254­256.
 Ernesto DAvanzo and Bernado Magnini.
             ´                                 A Keyphrase-     Ian Witten and Eibe Frank. Data Mining: Practical Ma-
   Based Approach to Summarization:the LAKE System at              chine Learning Tools and Techniques.       Morgan Kauf-
   DUC-2005. In Proceedings of DUC. 2005.                          mann, 2005.
 F. Damerau. Generating and evaluating domain-oriented          Yongzheng Zhang and Nur Zinchir-Heywood and Evange-
   multi-word terms from texts. Information Processing and         los Milios. Term based Clustering and Summarization of
   Management. 1993, 29, pp.43­447.                                Web Page Collections. In Proceedings of Conference of
Lee Dice. Measures of the amount of ecologic associations          the Canadian Society for Computational Studies of Intel-
   between species. Journal of Ecology. 1945, 2.                   ligence. 2004.

 Eibe Frank and Gordon Paynter and Ian Witten and Carl
   Gutwin and Craig Nevill-manning.         Domain Specific
   Keyphrase Extraction. In Proceedings of IJCAI. 1999,
   pp.668­673.

Carl Gutwin and Gordon Paynter and Ian Witten and Craig
   Nevill-Manning and Eibe Frank. Improving browsing in
   digital libraries with keyphrase indexes. Journal of Deci-
   sion Support Systems. 1999, 27, pp.81­104.

Khaled Hammouda and Diego Matute and Mohamed Kamel.
   CorePhrase: keyphrase extraction for document cluster-
   ing. In Proceedings of MLDM. 2005.



                                                              16

